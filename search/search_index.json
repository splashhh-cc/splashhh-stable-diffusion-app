{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"InvokeAI is an implementation of Stable Diffusion, the open source text-to-image and image-to-image generator. It provides a streamlined process with various new features and options to aid the image generation process. It runs on Windows, Mac and Linux machines, and runs on GPU cards with as little as 4 GB or RAM. Quick links : [ Discord Server ] [ Code and Downloads ] [ Bug Reports ] [ Discussion, Ideas & Q&A ] Note This fork is rapidly evolving. Please use the Issues tab to report bugs and make feature requests. Be sure to use the provided templates. They will help aid diagnose issues faster. Hardware Requirements # System # You wil need one of the following: An NVIDIA-based graphics card with 4 GB or more VRAM memory. An AMD-based graphics card with 4 GB or more VRAM memory (Linux only) An Apple computer with an M1 chip. We do not recommend the following video cards due to issues with their running in half-precision mode and having insufficient VRAM to render 512x512 images in full-precision mode: NVIDIA 10xx series cards such as the 1080ti GTX 1650 series cards GTX 1660 series cards Memory and Disk # At least 12 GB Main Memory RAM. At least 18 GB of free disk space for the machine learning model, Python, and all its dependencies. Installation # This fork is supported across Linux, Windows and Macintosh. Linux users can use either an Nvidia-based card (with CUDA support) or an AMD card (using the ROCm driver). Installation Getting Started Guide # Automated Installer # This method is recommended for 1 st time users Manual Installation # This method is recommended for experienced users and developers Docker Installation # This method is recommended for those familiar with running Docker containers Other Installation Guides # PyPatchMatch XFormers CUDA and ROCm Drivers Installing New Models InvokeAI Features # The InvokeAI Web Interface # WebUI overview WebUI hotkey reference guide WebUI Unified Canvas for Img2Img, inpainting and outpainting The InvokeAI Command Line Interface # Command Line Interace Reference Guide Image Management # Image2Image Inpainting Outpainting Adding custom styles and subjects Upscaling and Face Reconstruction Embiggen upscaling Other Features Model Management # Installing Model Merging Style/Subject Concepts and Embeddings Textual Inversion Not Safe for Work (NSFW) Checker Prompt Engineering # Prompt Syntax Generating Variations Latest Changes # v2.3.0 (9 February 2023) # Migration to Stable Diffusion diffusers models # Previous versions of InvokeAI supported the original model file format introduced with Stable Diffusion 1.4. In the original format, known variously as \"checkpoint\", or \"legacy\" format, there is a single large weights file ending with .ckpt or .safetensors . Though this format has served the community well, it has a number of disadvantages, including file size, slow loading times, and a variety of non-standard variants that require special-case code to handle. In addition, because checkpoint files are actually a bundle of multiple machine learning sub-models, it is hard to swap different sub-models in and out, or to share common sub-models. A new format, introduced by the StabilityAI company in collaboration with HuggingFace, is called diffusers and consists of a directory of individual models. The most immediate benefit of diffusers is that they load from disk very quickly. A longer term benefit is that in the near future diffusers models will be able to share common sub-models, dramatically reducing disk space when you have multiple fine-tune models derived from the same base. When you perform a new install of version 2.3.0, you will be offered the option to install the diffusers versions of a number of popular SD models, including Stable Diffusion versions 1.5 and 2.1 (including the 768x768 pixel version of 2.1). These will act and work just like the checkpoint versions. Do not be concerned if you already have a lot of \".ckpt\" or \".safetensors\" models on disk! InvokeAI 2.3.0 can still load these and generate images from them without any extra intervention on your part. To take advantage of the optimized loading times of diffusers models, InvokeAI offers options to convert legacy checkpoint models into optimized diffusers models. If you use the invokeai command line interface, the relevant commands are: !convert_model -- Take the path to a local checkpoint file or a URL that is pointing to one, convert it into a diffusers model, and import it into InvokeAI's models registry file. !optimize_model -- If you already have a checkpoint model in your InvokeAI models file, this command will accept its short name and convert it into a like-named diffusers model, optionally deleting the original checkpoint file. !import_model -- Take the local path of either a checkpoint file or a diffusers model directory and import it into InvokeAI's registry file. You may also provide the ID of any diffusers model that has been published on the HuggingFace models repository and it will be downloaded and installed automatically. The WebGUI offers similar functionality for model management. For advanced users, new command-line options provide additional functionality. Launching invokeai with the argument --autoconvert <path to directory> takes the path to a directory of checkpoint files, automatically converts them into diffusers models and imports them. Each time the script is launched, the directory will be scanned for new checkpoint files to be loaded. Alternatively, the --ckpt_convert argument will cause any checkpoint or safetensors model that is already registered with InvokeAI to be converted into a diffusers model on the fly, allowing you to take advantage of future diffusers-only features without explicitly converting the model and saving it to disk. Please see INSTALLING MODELS for more information on model management in both the command-line and Web interfaces. Support for the XFormers Memory-Efficient Crossattention Package # On CUDA (Nvidia) systems, version 2.3.0 supports the XFormers library. Once installed, the xformers package dramatically reduces the memory footprint of loaded Stable Diffusion models files and modestly increases image generation speed. xformers will be installed and activated automatically if you specify a CUDA system at install time. The caveat with using xformers is that it introduces slightly non-deterministic behavior, and images generated using the same seed and other settings will be subtly different between invocations. Generally the changes are unnoticeable unless you rapidly shift back and forth between images, but to disable xformers and restore fully deterministic behavior, you may launch InvokeAI using the --no-xformers option. This is most conveniently done by opening the file invokeai/invokeai.init with a text editor, and adding the line --no-xformers at the bottom. A Negative Prompt Box in the WebUI # There is now a separate text input box for negative prompts in the WebUI. This is convenient for stashing frequently-used negative prompts (\"mangled limbs, bad anatomy\"). The [negative prompt] syntax continues to work in the main prompt box as well. To see exactly how your prompts are being parsed, launch invokeai with the --log_tokenization option. The console window will then display the tokenization process for both positive and negative prompts. Model Merging # Version 2.3.0 offers an intuitive user interface for merging up to three Stable Diffusion models using an intuitive user interface. Model merging allows you to mix the behavior of models to achieve very interesting effects. To use this, each of the models must already be imported into InvokeAI and saved in diffusers format, then launch the merger using a new menu item in the InvokeAI launcher script ( invoke.sh , invoke.bat ) or directly from the command line with invokeai-merge --gui . You will be prompted to select the models to merge, the proportions in which to mix them, and the mixing algorithm. The script will create a new merged diffusers model and import it into InvokeAI for your use. See MODEL MERGING for more details. Textual Inversion Training # Textual Inversion (TI) is a technique for training a Stable Diffusion model to emit a particular subject or style when triggered by a keyword phrase. You can perform TI training by placing a small number of images of the subject or style in a directory, and choosing a distinctive trigger phrase, such as \"pointillist-style\". After successful training, The subject or style will be activated by including <pointillist-style> in your prompt. Previous versions of InvokeAI were able to perform TI, but it required using a command-line script with dozens of obscure command-line arguments. Version 2.3.0 features an intuitive TI frontend that will build a TI model on top of any diffusers model. To access training you can launch from a new item in the launcher script or from the command line using invokeai-ti --gui . See TEXTUAL INVERSION for further details. A New Installer Experience # The InvokeAI installer has been upgraded in order to provide a smoother and hopefully more glitch-free experience. In addition, InvokeAI is now packaged as a PyPi project, allowing developers and power-users to install InvokeAI with the command pip install InvokeAI --use-pep517 . Please see Installation for details. Developers should be aware that the pip installation procedure has been simplified and that the conda method is no longer supported at all. Accordingly, the environments_and_requirements directory has been deleted from the repository. Command-line name changes # All of InvokeAI's functionality, including the WebUI, command-line interface, textual inversion training and model merging, can all be accessed from the invoke.sh and invoke.bat launcher scripts. The menu of options has been expanded to add the new functionality. For the convenience of developers and power users, we have normalized the names of the InvokeAI command-line scripts: invokeai -- Command-line client invokeai --web -- Web GUI invokeai-merge --gui -- Model merging script with graphical front end invokeai-ti --gui -- Textual inversion script with graphical front end invokeai-configure -- Configuration tool for initializing the invokeai directory and selecting popular starter models. For backward compatibility, the old command names are also recognized, including invoke.py and configure-invokeai.py . However, these are deprecated and will eventually be removed. Developers should be aware that the locations of the script's source code has been moved. The new locations are: * invokeai => ldm/invoke/CLI.py * invokeai-configure => ldm/invoke/config/configure_invokeai.py * invokeai-ti => ldm/invoke/training/textual_inversion.py * invokeai-merge => ldm/invoke/merge_diffusers Developers are strongly encouraged to perform an \"editable\" install of InvokeAI using pip install -e . --use-pep517 in the Git repository, and then to call the scripts using their 2.3.0 names, rather than executing the scripts directly. Developers should also be aware that the several important data files have been relocated into a new directory named invokeai . This includes the WebGUI's frontend and backend directories, and the INITIAL_MODELS.yaml files used by the installer to select starter models. Eventually all InvokeAI modules will be in subdirectories of invokeai . Please see 2.3.0 Release Notes for further details. For older changelogs, please visit the CHANGELOG . Troubleshooting # Please check out our Troubleshooting Guide to get solutions for common installation problems and other issues. Contributing # Anyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so. If you are unfamiliar with how to contribute to GitHub projects, here is a Getting Started Guide . A full set of contribution guidelines, along with templates, are in progress, but for now the most important thing is to make your pull request against the \"development\" branch , and not against \"main\". This will help keep public breakage to a minimum and will allow you to propose more radical changes. Contributors # This fork is a combined effort of various people from across the world. Check out the list of all these amazing people . We thank them for their time, hard work and effort. Support # For support, please use this repository's GitHub Issues tracking service. Feel free to send me an email if you use and like the script. Original portions of the software are Copyright \u00a9 2022-23 by The InvokeAI Team . Further Reading # Please see the original README for more information on this software and underlying algorithm, located in the file README-CompViz.md .","title":"Home"},{"location":"#hardware-requirements","text":"","title":" Hardware Requirements"},{"location":"#system","text":"You wil need one of the following: An NVIDIA-based graphics card with 4 GB or more VRAM memory. An AMD-based graphics card with 4 GB or more VRAM memory (Linux only) An Apple computer with an M1 chip. We do not recommend the following video cards due to issues with their running in half-precision mode and having insufficient VRAM to render 512x512 images in full-precision mode: NVIDIA 10xx series cards such as the 1080ti GTX 1650 series cards GTX 1660 series cards","title":" System"},{"location":"#memory-and-disk","text":"At least 12 GB Main Memory RAM. At least 18 GB of free disk space for the machine learning model, Python, and all its dependencies.","title":" Memory and Disk"},{"location":"#installation","text":"This fork is supported across Linux, Windows and Macintosh. Linux users can use either an Nvidia-based card (with CUDA support) or an AMD card (using the ROCm driver).","title":" Installation"},{"location":"#installation-getting-started-guide","text":"","title":"Installation Getting Started Guide"},{"location":"#automated-installer","text":"This method is recommended for 1 st time users","title":"Automated Installer"},{"location":"#manual-installation","text":"This method is recommended for experienced users and developers","title":"Manual Installation"},{"location":"#docker-installation","text":"This method is recommended for those familiar with running Docker containers","title":"Docker Installation"},{"location":"#other-installation-guides","text":"PyPatchMatch XFormers CUDA and ROCm Drivers Installing New Models","title":"Other Installation Guides"},{"location":"#invokeai-features","text":"","title":" InvokeAI Features"},{"location":"#the-invokeai-web-interface","text":"WebUI overview WebUI hotkey reference guide WebUI Unified Canvas for Img2Img, inpainting and outpainting","title":"The InvokeAI Web Interface"},{"location":"#the-invokeai-command-line-interface","text":"Command Line Interace Reference Guide","title":"The InvokeAI Command Line Interface"},{"location":"#image-management","text":"Image2Image Inpainting Outpainting Adding custom styles and subjects Upscaling and Face Reconstruction Embiggen upscaling Other Features","title":"Image Management"},{"location":"#model-management","text":"Installing Model Merging Style/Subject Concepts and Embeddings Textual Inversion Not Safe for Work (NSFW) Checker","title":"Model Management"},{"location":"#prompt-engineering","text":"Prompt Syntax Generating Variations","title":"Prompt Engineering"},{"location":"#latest-changes","text":"","title":" Latest Changes"},{"location":"#v230-9-february-2023","text":"","title":"v2.3.0 (9 February 2023)"},{"location":"#migration-to-stable-diffusion-diffusers-models","text":"Previous versions of InvokeAI supported the original model file format introduced with Stable Diffusion 1.4. In the original format, known variously as \"checkpoint\", or \"legacy\" format, there is a single large weights file ending with .ckpt or .safetensors . Though this format has served the community well, it has a number of disadvantages, including file size, slow loading times, and a variety of non-standard variants that require special-case code to handle. In addition, because checkpoint files are actually a bundle of multiple machine learning sub-models, it is hard to swap different sub-models in and out, or to share common sub-models. A new format, introduced by the StabilityAI company in collaboration with HuggingFace, is called diffusers and consists of a directory of individual models. The most immediate benefit of diffusers is that they load from disk very quickly. A longer term benefit is that in the near future diffusers models will be able to share common sub-models, dramatically reducing disk space when you have multiple fine-tune models derived from the same base. When you perform a new install of version 2.3.0, you will be offered the option to install the diffusers versions of a number of popular SD models, including Stable Diffusion versions 1.5 and 2.1 (including the 768x768 pixel version of 2.1). These will act and work just like the checkpoint versions. Do not be concerned if you already have a lot of \".ckpt\" or \".safetensors\" models on disk! InvokeAI 2.3.0 can still load these and generate images from them without any extra intervention on your part. To take advantage of the optimized loading times of diffusers models, InvokeAI offers options to convert legacy checkpoint models into optimized diffusers models. If you use the invokeai command line interface, the relevant commands are: !convert_model -- Take the path to a local checkpoint file or a URL that is pointing to one, convert it into a diffusers model, and import it into InvokeAI's models registry file. !optimize_model -- If you already have a checkpoint model in your InvokeAI models file, this command will accept its short name and convert it into a like-named diffusers model, optionally deleting the original checkpoint file. !import_model -- Take the local path of either a checkpoint file or a diffusers model directory and import it into InvokeAI's registry file. You may also provide the ID of any diffusers model that has been published on the HuggingFace models repository and it will be downloaded and installed automatically. The WebGUI offers similar functionality for model management. For advanced users, new command-line options provide additional functionality. Launching invokeai with the argument --autoconvert <path to directory> takes the path to a directory of checkpoint files, automatically converts them into diffusers models and imports them. Each time the script is launched, the directory will be scanned for new checkpoint files to be loaded. Alternatively, the --ckpt_convert argument will cause any checkpoint or safetensors model that is already registered with InvokeAI to be converted into a diffusers model on the fly, allowing you to take advantage of future diffusers-only features without explicitly converting the model and saving it to disk. Please see INSTALLING MODELS for more information on model management in both the command-line and Web interfaces.","title":"Migration to Stable Diffusion diffusers models"},{"location":"#support-for-the-xformers-memory-efficient-crossattention-package","text":"On CUDA (Nvidia) systems, version 2.3.0 supports the XFormers library. Once installed, the xformers package dramatically reduces the memory footprint of loaded Stable Diffusion models files and modestly increases image generation speed. xformers will be installed and activated automatically if you specify a CUDA system at install time. The caveat with using xformers is that it introduces slightly non-deterministic behavior, and images generated using the same seed and other settings will be subtly different between invocations. Generally the changes are unnoticeable unless you rapidly shift back and forth between images, but to disable xformers and restore fully deterministic behavior, you may launch InvokeAI using the --no-xformers option. This is most conveniently done by opening the file invokeai/invokeai.init with a text editor, and adding the line --no-xformers at the bottom.","title":"Support for the XFormers Memory-Efficient Crossattention Package"},{"location":"#a-negative-prompt-box-in-the-webui","text":"There is now a separate text input box for negative prompts in the WebUI. This is convenient for stashing frequently-used negative prompts (\"mangled limbs, bad anatomy\"). The [negative prompt] syntax continues to work in the main prompt box as well. To see exactly how your prompts are being parsed, launch invokeai with the --log_tokenization option. The console window will then display the tokenization process for both positive and negative prompts.","title":"A Negative Prompt Box in the WebUI"},{"location":"#model-merging","text":"Version 2.3.0 offers an intuitive user interface for merging up to three Stable Diffusion models using an intuitive user interface. Model merging allows you to mix the behavior of models to achieve very interesting effects. To use this, each of the models must already be imported into InvokeAI and saved in diffusers format, then launch the merger using a new menu item in the InvokeAI launcher script ( invoke.sh , invoke.bat ) or directly from the command line with invokeai-merge --gui . You will be prompted to select the models to merge, the proportions in which to mix them, and the mixing algorithm. The script will create a new merged diffusers model and import it into InvokeAI for your use. See MODEL MERGING for more details.","title":"Model Merging"},{"location":"#textual-inversion-training","text":"Textual Inversion (TI) is a technique for training a Stable Diffusion model to emit a particular subject or style when triggered by a keyword phrase. You can perform TI training by placing a small number of images of the subject or style in a directory, and choosing a distinctive trigger phrase, such as \"pointillist-style\". After successful training, The subject or style will be activated by including <pointillist-style> in your prompt. Previous versions of InvokeAI were able to perform TI, but it required using a command-line script with dozens of obscure command-line arguments. Version 2.3.0 features an intuitive TI frontend that will build a TI model on top of any diffusers model. To access training you can launch from a new item in the launcher script or from the command line using invokeai-ti --gui . See TEXTUAL INVERSION for further details.","title":"Textual Inversion Training"},{"location":"#a-new-installer-experience","text":"The InvokeAI installer has been upgraded in order to provide a smoother and hopefully more glitch-free experience. In addition, InvokeAI is now packaged as a PyPi project, allowing developers and power-users to install InvokeAI with the command pip install InvokeAI --use-pep517 . Please see Installation for details. Developers should be aware that the pip installation procedure has been simplified and that the conda method is no longer supported at all. Accordingly, the environments_and_requirements directory has been deleted from the repository.","title":"A New Installer Experience"},{"location":"#command-line-name-changes","text":"All of InvokeAI's functionality, including the WebUI, command-line interface, textual inversion training and model merging, can all be accessed from the invoke.sh and invoke.bat launcher scripts. The menu of options has been expanded to add the new functionality. For the convenience of developers and power users, we have normalized the names of the InvokeAI command-line scripts: invokeai -- Command-line client invokeai --web -- Web GUI invokeai-merge --gui -- Model merging script with graphical front end invokeai-ti --gui -- Textual inversion script with graphical front end invokeai-configure -- Configuration tool for initializing the invokeai directory and selecting popular starter models. For backward compatibility, the old command names are also recognized, including invoke.py and configure-invokeai.py . However, these are deprecated and will eventually be removed. Developers should be aware that the locations of the script's source code has been moved. The new locations are: * invokeai => ldm/invoke/CLI.py * invokeai-configure => ldm/invoke/config/configure_invokeai.py * invokeai-ti => ldm/invoke/training/textual_inversion.py * invokeai-merge => ldm/invoke/merge_diffusers Developers are strongly encouraged to perform an \"editable\" install of InvokeAI using pip install -e . --use-pep517 in the Git repository, and then to call the scripts using their 2.3.0 names, rather than executing the scripts directly. Developers should also be aware that the several important data files have been relocated into a new directory named invokeai . This includes the WebGUI's frontend and backend directories, and the INITIAL_MODELS.yaml files used by the installer to select starter models. Eventually all InvokeAI modules will be in subdirectories of invokeai . Please see 2.3.0 Release Notes for further details. For older changelogs, please visit the CHANGELOG .","title":"Command-line name changes"},{"location":"#troubleshooting","text":"Please check out our Troubleshooting Guide to get solutions for common installation problems and other issues.","title":" Troubleshooting"},{"location":"#contributing","text":"Anyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so. If you are unfamiliar with how to contribute to GitHub projects, here is a Getting Started Guide . A full set of contribution guidelines, along with templates, are in progress, but for now the most important thing is to make your pull request against the \"development\" branch , and not against \"main\". This will help keep public breakage to a minimum and will allow you to propose more radical changes.","title":" Contributing"},{"location":"#contributors","text":"This fork is a combined effort of various people from across the world. Check out the list of all these amazing people . We thank them for their time, hard work and effort.","title":" Contributors"},{"location":"#support","text":"For support, please use this repository's GitHub Issues tracking service. Feel free to send me an email if you use and like the script. Original portions of the software are Copyright \u00a9 2022-23 by The InvokeAI Team .","title":" Support"},{"location":"#further-reading","text":"Please see the original README for more information on this software and underlying algorithm, located in the file README-CompViz.md .","title":" Further Reading"},{"location":"CHANGELOG/","text":"Changelog # v2.3.0 (15 January 2023) # **Transition to diffusers Version 2.3 provides support for both the traditional .ckpt weight checkpoint files as well as the HuggingFace diffusers format. This introduces several changes you should know about. The models.yaml format has been updated. There are now two different type of configuration stanza. The traditional ckpt one will look like this, with a format of ckpt and a weights field that points to the absolute or ROOTDIR-relative location of the ckpt file. inpainting-1.5: description: RunwayML SD 1.5 model optimized for inpainting (4.27 GB) repo_id: runwayml/stable-diffusion-inpainting format: ckpt width: 512 height: 512 weights: models/ldm/stable-diffusion-v1/sd-v1-5-inpainting.ckpt config: configs/stable-diffusion/v1-inpainting-inference.yaml vae: models/ldm/stable-diffusion-v1/vae-ft-mse-840000-ema-pruned.ckpt A configuration stanza for a diffusers model hosted at HuggingFace will look like this, with a format of diffusers and a repo_id that points to the repository ID of the model on HuggingFace: stable-diffusion-2.1: description: Stable Diffusion version 2.1 diffusers model (5.21 GB) repo_id: stabilityai/stable-diffusion-2-1 format: diffusers A configuration stanza for a diffuers model stored locally should look like this, with a format of diffusers , but a path field that points at the directory that contains model_index.json : waifu-diffusion: description: Latest waifu diffusion 1.4 format: diffusers path: models/diffusers/hakurei-haifu-diffusion-1.4 In order of precedence, InvokeAI will now use HF_HOME, then XDG_CACHE_HOME, then finally default to ROOTDIR/models to store HuggingFace diffusers models. Consequently, the format of the models directory has changed to mimic the HuggingFace cache directory. When HF_HOME and XDG_HOME are not set, diffusers models are now automatically downloaded and retrieved from the directory ROOTDIR/models/diffusers , while other models are stored in the directory ROOTDIR/models/hub . This organization is the same as that used by HuggingFace for its cache management. This allows you to share diffusers and ckpt model files easily with other machine learning applications that use the HuggingFace libraries. To do this, set the environment variable HF_HOME before starting up InvokeAI to tell it what directory to cache models in. To tell InvokeAI to use the standard HuggingFace cache directory, you would set HF_HOME like this (Linux/Mac): export HF_HOME=~/.cache/huggingface Both HuggingFace and InvokeAI will fall back to the XDG_CACHE_HOME environment variable if HF_HOME is not set; this path takes precedence over ROOTDIR/models to allow for the same sharing with other machine learning applications that use HuggingFace libraries. If you upgrade to InvokeAI 2.3.* from an earlier version, there will be a one-time migration from the old models directory format to the new one. You will see a message about this the first time you start invoke.py . Both the front end back ends of the model manager have been rewritten to accommodate diffusers. You can import models using their local file path, using their URLs, or their HuggingFace repo_ids. On the command line, all these syntaxes work: !import_model stabilityai/stable-diffusion-2-1-base !import_model /opt/sd-models/sd-1.4.ckpt !import_model https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model/blob/main/PaperCut_v1.ckpt **KNOWN BUGS (15 January 2023) On CUDA systems, the 768 pixel stable-diffusion-2.0 and stable-diffusion-2.1 models can only be run as diffusers models when the xformer library is installed and configured. Without xformers , InvokeAI returns black images. Inpainting and outpainting have regressed in quality. Both these issues are being actively worked on. v2.2.4 (11 December 2022) # the invokeai directory Previously there were two directories to worry about, the directory that contained the InvokeAI source code and the launcher scripts, and the invokeai directory that contained the models files, embeddings, configuration and outputs. With the 2.2.4 release, this dual system is done away with, and everything, including the invoke.bat and invoke.sh launcher scripts, now live in a directory named invokeai . By default this directory is located in your home directory (e.g. \\Users\\yourname on Windows), but you can select where it goes at install time. After installation, you can delete the install directory (the one that the zip file creates when it unpacks). Do not delete or move the invokeai directory! Initialization file invokeai/invokeai.init You can place frequently-used startup options in this file, such as the default number of steps or your preferred sampler. To keep everything in one place, this file has now been moved into the invokeai directory and is named invokeai.init . To update from Version 2.2.3 The easiest route is to download and unpack one of the 2.2.4 installer files. When it asks you for the location of the invokeai runtime directory, respond with the path to the directory that contains your 2.2.3 invokeai . That is, if invokeai lives at C:\\Users\\fred\\invokeai , then answer with C:\\Users\\fred and answer \"Y\" when asked if you want to reuse the directory. The update.sh ( update.bat ) script that came with the 2.2.3 source installer does not know about the new directory layout and won't be fully functional. To update to 2.2.5 (and beyond) there's now an update path As they become available, you can update to more recent versions of InvokeAI using an update.sh ( update.bat ) script located in the invokeai directory. Running it without any arguments will install the most recent version of InvokeAI. Alternatively, you can get set releases by running the update.sh script with an argument in the command shell. This syntax accepts the path to the desired release's zip file, which you can find by clicking on the green \"Code\" button on this repository's home page. Other 2.2.4 Improvements Fix InvokeAI GUI initialization by @addianto in #1687 fix link in documentation by @lstein in #1728 Fix broken link by @ShawnZhong in #1736 Remove reference to binary installer by @lstein in #1731 documentation fixes for 2.2.3 by @lstein in #1740 Modify installer links to point closer to the source installer by @ebr in #1745 add documentation warning about 1650/60 cards by @lstein in #1753 Fix Linux source URL in installation docs by @andybearman in #1756 Make install instructions discoverable in readme by @damian0815 in #1752 typo fix by @ofirkris in #1755 Non-interactive model download (support HUGGINGFACE_TOKEN) by @ebr in #1578 fix(srcinstall): shell installer - cp scripts instead of linking by @tildebyte in #1765 stability and usage improvements to binary & source installers by @lstein in #1760 fix off-by-one bug in cross-attention-control by @damian0815 in #1774 Eventually update APP_VERSION to 2.2.3 by @spezialspezial in #1768 invoke script cds to its location before running by @lstein in #1805 Make PaperCut and VoxelArt models load again by @lstein in #1730 Fix --embedding_directory / --embedding_path not working by @blessedcoolant in #1817 Clean up readme by @hipsterusername in #1820 Optimized Docker build with support for external working directory by @ebr in #1544 disable pushing the cloud container by @mauwii in #1831 Fix docker push github action and expand with additional metadata by @ebr in #1837 Fix Broken Link To Notebook by @VedantMadane in #1821 Account for flat models by @spezialspezial in #1766 Update invoke.bat.in isolate environment variables by @lynnewu in #1833 Arch Linux Specific PatchMatch Instructions & fixing conda install on linux by @SammCheese in #1848 Make force free GPU memory work in img2img by @addianto in #1844 New installer by @lstein v2.2.3 (2 December 2022) # Note This point release removes references to the binary installer from the installation guide. The binary installer is not stable at the current time. First time users are encouraged to use the \"source\" installer as described in Installing InvokeAI with the Source Installer With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation. Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 & M2). You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas . This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI. v2.2.2 (30 November 2022) # Note The binary installer is not ready for prime time. First time users are recommended to install via the \"source\" installer accessible through the links at the bottom of this page.**** With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation. Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 & M2). You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas . This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI. v2.2.0 (2 December 2022) # With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation. Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 & M2). You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas . This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI. v2.1.3 (13 November 2022) # A choice of installer scripts that automate installation and configuration. See Installation . A streamlined manual installation process that works for both Conda and PIP-only installs. See Manual Installation . The ability to save frequently-used startup options (model to load, steps, sampler, etc) in a .invokeai file. See Client Support for AMD GPU cards (non-CUDA) on Linux machines. Multiple bugs and edge cases squashed. v2.1.0 (2 November 2022) # update mac instructions to use invokeai for env name by @willwillems in #1030 Update .gitignore by @blessedcoolant in #1040 reintroduce fix for m1 from #579 missing after merge by @skurovec in #1056 Update Stable_Diffusion_AI_Notebook.ipynb (Take 2) by @ChloeL19 in #1060 Print out the device type which is used by @manzke in #1073 Hires Addition by @hipsterusername in #1063 fix for \"1 leaked semaphore objects to clean up at shutdown\" on M1 by @skurovec in #1081 Forward dream.py to invoke.py using the same interpreter, add deprecation warning by @db3000 in #1077 fix noisy images at high step counts by @lstein in #1086 Generalize facetool strength argument by @db3000 in #1078 Enable fast switching among models at the invoke> command line by @lstein in #1066 Fix Typo, committed changing ldm environment to invokeai by @jdries3 in #1095 Update generate.py by @unreleased in #1109 Update 'ldm' env to 'invokeai' in troubleshooting steps by @19wolf in #1125 Fixed documentation typos and resolved merge conflicts by @rupeshs in #1123 Fix broken doc links, fix malaprop in the project subtitle by @majick in #1131 Only output facetool parameters if enhancing faces by @db3000 in #1119 Update gitignore to ignore codeformer weights at new location by @spezialspezial in #1136 fix links to point to invoke-ai.github.io #1117 by @mauwii in #1143 Rework-mkdocs by @mauwii in #1144 add option to CLI and pngwriter that allows user to set PNG compression level by @lstein in #1127 Fix img2img DDIM index out of bound by @wfng92 in #1137 Fix gh actions by @mauwii in #1128 update mac instructions to use invokeai for env name by @willwillems in #1030 Update .gitignore by @blessedcoolant in #1040 reintroduce fix for m1 from #579 missing after merge by @skurovec in #1056 Update Stable_Diffusion_AI_Notebook.ipynb (Take 2) by @ChloeL19 in #1060 Print out the device type which is used by @manzke in #1073 Hires Addition by @hipsterusername in #1063 fix for \"1 leaked semaphore objects to clean up at shutdown\" on M1 by @skurovec in #1081 Forward dream.py to invoke.py using the same interpreter, add deprecation warning by @db3000 in #1077 fix noisy images at high step counts by @lstein in #1086 Generalize facetool strength argument by @db3000 in #1078 Enable fast switching among models at the invoke> command line by @lstein in #1066 Fix Typo, committed changing ldm environment to invokeai by @jdries3 in #1095 Fixed documentation typos and resolved merge conflicts by @rupeshs in #1123 Only output facetool parameters if enhancing faces by @db3000 in #1119 add option to CLI and pngwriter that allows user to set PNG compression level by @lstein in #1127 Fix img2img DDIM index out of bound by @wfng92 in #1137 Add text prompt to inpaint mask support by @lstein in #1133 Respect http[s] protocol when making socket.io middleware by @damian0815 in #976 WebUI: Adds Codeformer support by @psychedelicious in #1151 Skips normalizing prompts for web UI metadata by @psychedelicious in #1165 Add Asymmetric Tiling by @carson-katri in #1132 Web UI: Increases max CFG Scale to 200 by @psychedelicious in #1172 Corrects color channels in face restoration; Fixes #1167 by @psychedelicious in #1175 Flips channels using array slicing instead of using OpenCV by @psychedelicious in #1178 Fix typo in docs: s/Formally/Formerly by @noodlebox in #1176 fix clipseg loading problems by @lstein in #1177 Correct color channels in upscale using array slicing by @wfng92 in #1181 Web UI: Filters existing images when adding new images; Fixes #1085 by @psychedelicious in #1171 fix a number of bugs in textual inversion by @lstein in #1190 Improve !fetch, add !replay command by @ArDiouscuros in #882 Fix generation of image with s>1000 by @holstvoogd in #951 Web UI: Gallery improvements by @psychedelicious in #1198 Update CLI.md by @krummrey in #1211 outcropping improvements by @lstein in #1207 add support for loading VAE autoencoders by @lstein in #1216 remove duplicate fix_func for MPS by @wfng92 in #1210 Metadata storage and retrieval fixes by @lstein in #1204 nix: add shell.nix file by @Cloudef in #1170 Web UI: Changes vite dist asset paths to relative by @psychedelicious in #1185 Web UI: Removes isDisabled from PromptInput by @psychedelicious in #1187 Allow user to generate images with initial noise as on M1 / mps system by @ArDiouscuros in #981 feat: adding filename format template by @plucked in #968 Web UI: Fixes broken bundle by @psychedelicious in #1242 Support runwayML custom inpainting model by @lstein in #1243 Update IMG2IMG.md by @talitore in #1262 New dockerfile - including a build- and a run- script as well as a GH-Action by @mauwii in #1233 cut over from karras to model noise schedule for higher steps by @lstein in #1222 Prompt tweaks by @lstein in #1268 Outpainting implementation by @Kyle0654 in #1251 fixing aspect ratio on hires by @tjennings in #1249 Fix-build-container-action by @mauwii in #1274 handle all unicode characters by @damian0815 in #1276 adds models.user.yml to .gitignore by @JakeHL in #1281 remove debug branch, set fail-fast to false by @mauwii in #1284 Protect-secrets-on-pr by @mauwii in #1285 Web UI: Adds initial inpainting implementation by @psychedelicious in #1225 fix environment-mac.yml - tested on x64 and arm64 by @mauwii in #1289 Use proper authentication to download model by @mauwii in #1287 Prevent indexing error for mode RGB by @spezialspezial in #1294 Integrate sd-v1-5 model into test matrix (easily expandable), remove unecesarry caches by @mauwii in #1293 add --no-interactive to configure_invokeai step by @mauwii in #1302 1-click installer and updater. Uses micromamba to install git and conda into a contained environment (if necessary) before running the normal installation script by @cmdr2 in #1253 configure_invokeai.py script downloads the weight files by @lstein in #1290 v2.0.1 (13 October 2022) # fix noisy images at high step count when using k* samplers dream.py script now calls invoke.py module directly rather than via a new python process (which could break the environment) v2.0.0 (9 October 2022) # dream.py script renamed invoke.py . A dream.py script wrapper remains for backward compatibility. Completely new WebGUI - launch with python3 scripts/invoke.py --web Support for inpainting and outpainting img2img runs on all k* samplers Support for negative prompts Support for CodeFormer face reconstruction Support for Textual Inversion on Macintoshes Support in both WebGUI and CLI for post-processing of previously-generated images using facial reconstruction, ESRGAN upscaling, outcropping (similar to DALL-E infinite canvas), and \"embiggen\" upscaling. See the !fix command. New --hires option on invoke> line allows larger images to be created without duplicating elements , at the cost of some performance. New --perlin and --threshold options allow you to add and control variation during image generation (see Thresholding and Perlin Noise Initialization ) Extensive metadata now written into PNG files, allowing reliable regeneration of images and tweaking of previous settings. Command-line completion in invoke.py now works on Windows, Linux and Mac platforms. Improved command-line completion behavior New commands added: List command-line history with !history Search command-line history with !search Clear history with !clear Deprecated --full_precision / -F . Simply omit it and invoke.py will auto configure. To switch away from auto use the new flag like --precision=float32 . v1.14 (11 September 2022) # Memory optimizations for small-RAM cards. 512x512 now possible on 4 GB GPUs. Full support for Apple hardware with M1 or M2 chips. Add \"seamless mode\" for circular tiling of image. Generates beautiful effects. ( prixt ). Inpainting support. Improved web server GUI. Lots of code and documentation cleanups. v1.13 (3 September 2022) # Support image variations (see VARIATIONS ( Kevin Gibbons and many contributors and reviewers) Supports a Google Colab notebook for a standalone server running on Google hardware Arturo Mendivil WebUI supports GFPGAN/ESRGAN facial reconstruction and upscaling Kevin Gibbons WebUI supports incremental display of in-progress images during generation Kevin Gibbons A new configuration file scheme that allows new models (including upcoming stable-diffusion-v1.5) to be added without altering the code. ( David Wager ) Can specify --grid on invoke.py command line as the default. Miscellaneous internal bug and stability fixes. Works on M1 Apple hardware. Multiple bug fixes. v1.12 (28 August 2022) # Improved file handling, including ability to read prompts from standard input. (kudos to Yunsaki The web server is now integrated with the invoke.py script. Invoke by adding --web to the invoke.py command arguments. Face restoration and upscaling via GFPGAN and Real-ESGAN are now automatically enabled if the GFPGAN directory is located as a sibling to Stable Diffusion. VRAM requirements are modestly reduced. Thanks to both Blessedcoolant and Oceanswave for their work on this. You can now swap samplers on the invoke> command line. Blessedcoolant v1.11 (26 August 2022) # NEW FEATURE: Support upscaling and face enhancement using the GFPGAN module. (kudos to Oceanswave You now can specify a seed of -1 to use the previous image's seed, -2 to use the seed for the image generated before that, etc. Seed memory only extends back to the previous command, but will work on all images generated with the -n# switch. Variant generation support temporarily disabled pending more general solution. Created a feature branch named yunsaki-morphing-invoke which adds experimental support for iteratively modifying the prompt and its parameters. Please see Pull Request #86 for a synopsis of how this works. Note that when this feature is eventually added to the main branch, it will may be modified significantly. v1.10 (25 August 2022) # A barebones but fully functional interactive web server for online generation of txt2img and img2img. v1.09 (24 August 2022) # A new -v option allows you to generate multiple variants of an initial image in img2img mode. (kudos to Oceanswave . See this discussion in the PR for examples and details on use ) Added ability to personalize text to image generation (kudos to Oceanswave and nicolai256 ) Enabled all of the samplers from k_diffusion v1.08 (24 August 2022) # Escape single quotes on the invoke> command before trying to parse. This avoids parse errors. Removed instruction to get Python3.8 as first step in Windows install. Anaconda3 does it for you. Added bounds checks for numeric arguments that could cause crashes. Cleaned up the copyright and license agreement files. v1.07 (23 August 2022) # Image filenames will now never fill gaps in the sequence, but will be assigned the next higher name in the chosen directory. This ensures that the alphabetic and chronological sort orders are the same. v1.06 (23 August 2022) # Added weighted prompt support contributed by xraxra Example of using weighted prompts to tweak a demonic figure contributed by bmaltais v1.05 (22 August 2022 - after the drop) # Filenames now use the following formats: 000010.95183149.png -- Two files produced by the same command (e.g. -n2), 000010.26742632.png -- distinguished by a different seed. 000011.455191342.01.png -- Two files produced by the same command using 000011.455191342.02.png -- a batch size>1 (e.g. -b2). They have the same seed. 000011.4160627868.grid#1 -4.png -- a grid of four images (-g); the whole grid can be regenerated with the indicated key It should no longer be possible for one image to overwrite another You can use the \"cd\" and \"pwd\" commands at the invoke> prompt to set and retrieve the path of the output directory. v1.04 (22 August 2022 - after the drop) # Updated README to reflect installation of the released weights. Suppressed very noisy and inconsequential warning when loading the frozen CLIP tokenizer. v1.03 (22 August 2022) # The original txt2img and img2img scripts from the CompViz repository have been moved into a subfolder named \"orig_scripts\", to reduce confusion. v1.02 (21 August 2022) # A copy of the prompt and all of its switches and options is now stored in the corresponding image in a tEXt metadata field named \"Dream\". You can read the prompt using scripts/images2prompt.py, or an image editor that allows you to explore the full metadata. Please run \"conda env update\" to load the k_lms dependencies!! v1.01 (21 August 2022) # added k_lms sampling. Please run \"conda env update\" to load the k_lms dependencies!! use half precision arithmetic by default, resulting in faster execution and lower memory requirements Pass argument --full_precision to invoke.py to get slower but more accurate image generation Links # Read Me","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":" Changelog"},{"location":"CHANGELOG/#v230-15-january-2023","text":"**Transition to diffusers Version 2.3 provides support for both the traditional .ckpt weight checkpoint files as well as the HuggingFace diffusers format. This introduces several changes you should know about. The models.yaml format has been updated. There are now two different type of configuration stanza. The traditional ckpt one will look like this, with a format of ckpt and a weights field that points to the absolute or ROOTDIR-relative location of the ckpt file. inpainting-1.5: description: RunwayML SD 1.5 model optimized for inpainting (4.27 GB) repo_id: runwayml/stable-diffusion-inpainting format: ckpt width: 512 height: 512 weights: models/ldm/stable-diffusion-v1/sd-v1-5-inpainting.ckpt config: configs/stable-diffusion/v1-inpainting-inference.yaml vae: models/ldm/stable-diffusion-v1/vae-ft-mse-840000-ema-pruned.ckpt A configuration stanza for a diffusers model hosted at HuggingFace will look like this, with a format of diffusers and a repo_id that points to the repository ID of the model on HuggingFace: stable-diffusion-2.1: description: Stable Diffusion version 2.1 diffusers model (5.21 GB) repo_id: stabilityai/stable-diffusion-2-1 format: diffusers A configuration stanza for a diffuers model stored locally should look like this, with a format of diffusers , but a path field that points at the directory that contains model_index.json : waifu-diffusion: description: Latest waifu diffusion 1.4 format: diffusers path: models/diffusers/hakurei-haifu-diffusion-1.4 In order of precedence, InvokeAI will now use HF_HOME, then XDG_CACHE_HOME, then finally default to ROOTDIR/models to store HuggingFace diffusers models. Consequently, the format of the models directory has changed to mimic the HuggingFace cache directory. When HF_HOME and XDG_HOME are not set, diffusers models are now automatically downloaded and retrieved from the directory ROOTDIR/models/diffusers , while other models are stored in the directory ROOTDIR/models/hub . This organization is the same as that used by HuggingFace for its cache management. This allows you to share diffusers and ckpt model files easily with other machine learning applications that use the HuggingFace libraries. To do this, set the environment variable HF_HOME before starting up InvokeAI to tell it what directory to cache models in. To tell InvokeAI to use the standard HuggingFace cache directory, you would set HF_HOME like this (Linux/Mac): export HF_HOME=~/.cache/huggingface Both HuggingFace and InvokeAI will fall back to the XDG_CACHE_HOME environment variable if HF_HOME is not set; this path takes precedence over ROOTDIR/models to allow for the same sharing with other machine learning applications that use HuggingFace libraries. If you upgrade to InvokeAI 2.3.* from an earlier version, there will be a one-time migration from the old models directory format to the new one. You will see a message about this the first time you start invoke.py . Both the front end back ends of the model manager have been rewritten to accommodate diffusers. You can import models using their local file path, using their URLs, or their HuggingFace repo_ids. On the command line, all these syntaxes work: !import_model stabilityai/stable-diffusion-2-1-base !import_model /opt/sd-models/sd-1.4.ckpt !import_model https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model/blob/main/PaperCut_v1.ckpt **KNOWN BUGS (15 January 2023) On CUDA systems, the 768 pixel stable-diffusion-2.0 and stable-diffusion-2.1 models can only be run as diffusers models when the xformer library is installed and configured. Without xformers , InvokeAI returns black images. Inpainting and outpainting have regressed in quality. Both these issues are being actively worked on.","title":"v2.3.0 (15 January 2023)"},{"location":"CHANGELOG/#v224-11-december-2022","text":"the invokeai directory Previously there were two directories to worry about, the directory that contained the InvokeAI source code and the launcher scripts, and the invokeai directory that contained the models files, embeddings, configuration and outputs. With the 2.2.4 release, this dual system is done away with, and everything, including the invoke.bat and invoke.sh launcher scripts, now live in a directory named invokeai . By default this directory is located in your home directory (e.g. \\Users\\yourname on Windows), but you can select where it goes at install time. After installation, you can delete the install directory (the one that the zip file creates when it unpacks). Do not delete or move the invokeai directory! Initialization file invokeai/invokeai.init You can place frequently-used startup options in this file, such as the default number of steps or your preferred sampler. To keep everything in one place, this file has now been moved into the invokeai directory and is named invokeai.init . To update from Version 2.2.3 The easiest route is to download and unpack one of the 2.2.4 installer files. When it asks you for the location of the invokeai runtime directory, respond with the path to the directory that contains your 2.2.3 invokeai . That is, if invokeai lives at C:\\Users\\fred\\invokeai , then answer with C:\\Users\\fred and answer \"Y\" when asked if you want to reuse the directory. The update.sh ( update.bat ) script that came with the 2.2.3 source installer does not know about the new directory layout and won't be fully functional. To update to 2.2.5 (and beyond) there's now an update path As they become available, you can update to more recent versions of InvokeAI using an update.sh ( update.bat ) script located in the invokeai directory. Running it without any arguments will install the most recent version of InvokeAI. Alternatively, you can get set releases by running the update.sh script with an argument in the command shell. This syntax accepts the path to the desired release's zip file, which you can find by clicking on the green \"Code\" button on this repository's home page. Other 2.2.4 Improvements Fix InvokeAI GUI initialization by @addianto in #1687 fix link in documentation by @lstein in #1728 Fix broken link by @ShawnZhong in #1736 Remove reference to binary installer by @lstein in #1731 documentation fixes for 2.2.3 by @lstein in #1740 Modify installer links to point closer to the source installer by @ebr in #1745 add documentation warning about 1650/60 cards by @lstein in #1753 Fix Linux source URL in installation docs by @andybearman in #1756 Make install instructions discoverable in readme by @damian0815 in #1752 typo fix by @ofirkris in #1755 Non-interactive model download (support HUGGINGFACE_TOKEN) by @ebr in #1578 fix(srcinstall): shell installer - cp scripts instead of linking by @tildebyte in #1765 stability and usage improvements to binary & source installers by @lstein in #1760 fix off-by-one bug in cross-attention-control by @damian0815 in #1774 Eventually update APP_VERSION to 2.2.3 by @spezialspezial in #1768 invoke script cds to its location before running by @lstein in #1805 Make PaperCut and VoxelArt models load again by @lstein in #1730 Fix --embedding_directory / --embedding_path not working by @blessedcoolant in #1817 Clean up readme by @hipsterusername in #1820 Optimized Docker build with support for external working directory by @ebr in #1544 disable pushing the cloud container by @mauwii in #1831 Fix docker push github action and expand with additional metadata by @ebr in #1837 Fix Broken Link To Notebook by @VedantMadane in #1821 Account for flat models by @spezialspezial in #1766 Update invoke.bat.in isolate environment variables by @lynnewu in #1833 Arch Linux Specific PatchMatch Instructions & fixing conda install on linux by @SammCheese in #1848 Make force free GPU memory work in img2img by @addianto in #1844 New installer by @lstein","title":"v2.2.4 (11 December 2022)"},{"location":"CHANGELOG/#v223-2-december-2022","text":"Note This point release removes references to the binary installer from the installation guide. The binary installer is not stable at the current time. First time users are encouraged to use the \"source\" installer as described in Installing InvokeAI with the Source Installer With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation. Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 & M2). You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas . This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI.","title":"v2.2.3 (2 December 2022)"},{"location":"CHANGELOG/#v222-30-november-2022","text":"Note The binary installer is not ready for prime time. First time users are recommended to install via the \"source\" installer accessible through the links at the bottom of this page.**** With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation. Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 & M2). You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas . This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI.","title":"v2.2.2 (30 November 2022)"},{"location":"CHANGELOG/#v220-2-december-2022","text":"With InvokeAI 2.2, this project now provides enthusiasts and professionals a robust workflow solution for creating AI-generated and human facilitated compositions. Additional enhancements have been made as well, improving safety, ease of use, and installation. Optimized for efficiency, InvokeAI needs only ~3.5GB of VRAM to generate a 512x768 image (and less for smaller images), and is compatible with Windows/Linux/Mac (M1 & M2). You can see the release video here, which introduces the main WebUI enhancement for version 2.2 - The Unified Canvas . This new workflow is the biggest enhancement added to the WebUI to date, and unlocks a stunning amount of potential for users to create and iterate on their creations. The following sections describe what's new for InvokeAI.","title":"v2.2.0 (2 December 2022)"},{"location":"CHANGELOG/#v213-13-november-2022","text":"A choice of installer scripts that automate installation and configuration. See Installation . A streamlined manual installation process that works for both Conda and PIP-only installs. See Manual Installation . The ability to save frequently-used startup options (model to load, steps, sampler, etc) in a .invokeai file. See Client Support for AMD GPU cards (non-CUDA) on Linux machines. Multiple bugs and edge cases squashed.","title":"v2.1.3 (13 November 2022)"},{"location":"CHANGELOG/#v210-2-november-2022","text":"update mac instructions to use invokeai for env name by @willwillems in #1030 Update .gitignore by @blessedcoolant in #1040 reintroduce fix for m1 from #579 missing after merge by @skurovec in #1056 Update Stable_Diffusion_AI_Notebook.ipynb (Take 2) by @ChloeL19 in #1060 Print out the device type which is used by @manzke in #1073 Hires Addition by @hipsterusername in #1063 fix for \"1 leaked semaphore objects to clean up at shutdown\" on M1 by @skurovec in #1081 Forward dream.py to invoke.py using the same interpreter, add deprecation warning by @db3000 in #1077 fix noisy images at high step counts by @lstein in #1086 Generalize facetool strength argument by @db3000 in #1078 Enable fast switching among models at the invoke> command line by @lstein in #1066 Fix Typo, committed changing ldm environment to invokeai by @jdries3 in #1095 Update generate.py by @unreleased in #1109 Update 'ldm' env to 'invokeai' in troubleshooting steps by @19wolf in #1125 Fixed documentation typos and resolved merge conflicts by @rupeshs in #1123 Fix broken doc links, fix malaprop in the project subtitle by @majick in #1131 Only output facetool parameters if enhancing faces by @db3000 in #1119 Update gitignore to ignore codeformer weights at new location by @spezialspezial in #1136 fix links to point to invoke-ai.github.io #1117 by @mauwii in #1143 Rework-mkdocs by @mauwii in #1144 add option to CLI and pngwriter that allows user to set PNG compression level by @lstein in #1127 Fix img2img DDIM index out of bound by @wfng92 in #1137 Fix gh actions by @mauwii in #1128 update mac instructions to use invokeai for env name by @willwillems in #1030 Update .gitignore by @blessedcoolant in #1040 reintroduce fix for m1 from #579 missing after merge by @skurovec in #1056 Update Stable_Diffusion_AI_Notebook.ipynb (Take 2) by @ChloeL19 in #1060 Print out the device type which is used by @manzke in #1073 Hires Addition by @hipsterusername in #1063 fix for \"1 leaked semaphore objects to clean up at shutdown\" on M1 by @skurovec in #1081 Forward dream.py to invoke.py using the same interpreter, add deprecation warning by @db3000 in #1077 fix noisy images at high step counts by @lstein in #1086 Generalize facetool strength argument by @db3000 in #1078 Enable fast switching among models at the invoke> command line by @lstein in #1066 Fix Typo, committed changing ldm environment to invokeai by @jdries3 in #1095 Fixed documentation typos and resolved merge conflicts by @rupeshs in #1123 Only output facetool parameters if enhancing faces by @db3000 in #1119 add option to CLI and pngwriter that allows user to set PNG compression level by @lstein in #1127 Fix img2img DDIM index out of bound by @wfng92 in #1137 Add text prompt to inpaint mask support by @lstein in #1133 Respect http[s] protocol when making socket.io middleware by @damian0815 in #976 WebUI: Adds Codeformer support by @psychedelicious in #1151 Skips normalizing prompts for web UI metadata by @psychedelicious in #1165 Add Asymmetric Tiling by @carson-katri in #1132 Web UI: Increases max CFG Scale to 200 by @psychedelicious in #1172 Corrects color channels in face restoration; Fixes #1167 by @psychedelicious in #1175 Flips channels using array slicing instead of using OpenCV by @psychedelicious in #1178 Fix typo in docs: s/Formally/Formerly by @noodlebox in #1176 fix clipseg loading problems by @lstein in #1177 Correct color channels in upscale using array slicing by @wfng92 in #1181 Web UI: Filters existing images when adding new images; Fixes #1085 by @psychedelicious in #1171 fix a number of bugs in textual inversion by @lstein in #1190 Improve !fetch, add !replay command by @ArDiouscuros in #882 Fix generation of image with s>1000 by @holstvoogd in #951 Web UI: Gallery improvements by @psychedelicious in #1198 Update CLI.md by @krummrey in #1211 outcropping improvements by @lstein in #1207 add support for loading VAE autoencoders by @lstein in #1216 remove duplicate fix_func for MPS by @wfng92 in #1210 Metadata storage and retrieval fixes by @lstein in #1204 nix: add shell.nix file by @Cloudef in #1170 Web UI: Changes vite dist asset paths to relative by @psychedelicious in #1185 Web UI: Removes isDisabled from PromptInput by @psychedelicious in #1187 Allow user to generate images with initial noise as on M1 / mps system by @ArDiouscuros in #981 feat: adding filename format template by @plucked in #968 Web UI: Fixes broken bundle by @psychedelicious in #1242 Support runwayML custom inpainting model by @lstein in #1243 Update IMG2IMG.md by @talitore in #1262 New dockerfile - including a build- and a run- script as well as a GH-Action by @mauwii in #1233 cut over from karras to model noise schedule for higher steps by @lstein in #1222 Prompt tweaks by @lstein in #1268 Outpainting implementation by @Kyle0654 in #1251 fixing aspect ratio on hires by @tjennings in #1249 Fix-build-container-action by @mauwii in #1274 handle all unicode characters by @damian0815 in #1276 adds models.user.yml to .gitignore by @JakeHL in #1281 remove debug branch, set fail-fast to false by @mauwii in #1284 Protect-secrets-on-pr by @mauwii in #1285 Web UI: Adds initial inpainting implementation by @psychedelicious in #1225 fix environment-mac.yml - tested on x64 and arm64 by @mauwii in #1289 Use proper authentication to download model by @mauwii in #1287 Prevent indexing error for mode RGB by @spezialspezial in #1294 Integrate sd-v1-5 model into test matrix (easily expandable), remove unecesarry caches by @mauwii in #1293 add --no-interactive to configure_invokeai step by @mauwii in #1302 1-click installer and updater. Uses micromamba to install git and conda into a contained environment (if necessary) before running the normal installation script by @cmdr2 in #1253 configure_invokeai.py script downloads the weight files by @lstein in #1290","title":"v2.1.0 (2 November 2022)"},{"location":"CHANGELOG/#v201-13-october-2022","text":"fix noisy images at high step count when using k* samplers dream.py script now calls invoke.py module directly rather than via a new python process (which could break the environment)","title":"v2.0.1 (13 October 2022)"},{"location":"CHANGELOG/#v200-9-october-2022","text":"dream.py script renamed invoke.py . A dream.py script wrapper remains for backward compatibility. Completely new WebGUI - launch with python3 scripts/invoke.py --web Support for inpainting and outpainting img2img runs on all k* samplers Support for negative prompts Support for CodeFormer face reconstruction Support for Textual Inversion on Macintoshes Support in both WebGUI and CLI for post-processing of previously-generated images using facial reconstruction, ESRGAN upscaling, outcropping (similar to DALL-E infinite canvas), and \"embiggen\" upscaling. See the !fix command. New --hires option on invoke> line allows larger images to be created without duplicating elements , at the cost of some performance. New --perlin and --threshold options allow you to add and control variation during image generation (see Thresholding and Perlin Noise Initialization ) Extensive metadata now written into PNG files, allowing reliable regeneration of images and tweaking of previous settings. Command-line completion in invoke.py now works on Windows, Linux and Mac platforms. Improved command-line completion behavior New commands added: List command-line history with !history Search command-line history with !search Clear history with !clear Deprecated --full_precision / -F . Simply omit it and invoke.py will auto configure. To switch away from auto use the new flag like --precision=float32 .","title":"v2.0.0 (9 October 2022)"},{"location":"CHANGELOG/#v114-11-september-2022","text":"Memory optimizations for small-RAM cards. 512x512 now possible on 4 GB GPUs. Full support for Apple hardware with M1 or M2 chips. Add \"seamless mode\" for circular tiling of image. Generates beautiful effects. ( prixt ). Inpainting support. Improved web server GUI. Lots of code and documentation cleanups.","title":"v1.14 (11 September 2022)"},{"location":"CHANGELOG/#v113-3-september-2022","text":"Support image variations (see VARIATIONS ( Kevin Gibbons and many contributors and reviewers) Supports a Google Colab notebook for a standalone server running on Google hardware Arturo Mendivil WebUI supports GFPGAN/ESRGAN facial reconstruction and upscaling Kevin Gibbons WebUI supports incremental display of in-progress images during generation Kevin Gibbons A new configuration file scheme that allows new models (including upcoming stable-diffusion-v1.5) to be added without altering the code. ( David Wager ) Can specify --grid on invoke.py command line as the default. Miscellaneous internal bug and stability fixes. Works on M1 Apple hardware. Multiple bug fixes.","title":"v1.13 (3 September 2022)"},{"location":"CHANGELOG/#v112-28-august-2022","text":"Improved file handling, including ability to read prompts from standard input. (kudos to Yunsaki The web server is now integrated with the invoke.py script. Invoke by adding --web to the invoke.py command arguments. Face restoration and upscaling via GFPGAN and Real-ESGAN are now automatically enabled if the GFPGAN directory is located as a sibling to Stable Diffusion. VRAM requirements are modestly reduced. Thanks to both Blessedcoolant and Oceanswave for their work on this. You can now swap samplers on the invoke> command line. Blessedcoolant","title":"v1.12 (28 August 2022)"},{"location":"CHANGELOG/#v111-26-august-2022","text":"NEW FEATURE: Support upscaling and face enhancement using the GFPGAN module. (kudos to Oceanswave You now can specify a seed of -1 to use the previous image's seed, -2 to use the seed for the image generated before that, etc. Seed memory only extends back to the previous command, but will work on all images generated with the -n# switch. Variant generation support temporarily disabled pending more general solution. Created a feature branch named yunsaki-morphing-invoke which adds experimental support for iteratively modifying the prompt and its parameters. Please see Pull Request #86 for a synopsis of how this works. Note that when this feature is eventually added to the main branch, it will may be modified significantly.","title":"v1.11 (26 August 2022)"},{"location":"CHANGELOG/#v110-25-august-2022","text":"A barebones but fully functional interactive web server for online generation of txt2img and img2img.","title":"v1.10 (25 August 2022)"},{"location":"CHANGELOG/#v109-24-august-2022","text":"A new -v option allows you to generate multiple variants of an initial image in img2img mode. (kudos to Oceanswave . See this discussion in the PR for examples and details on use ) Added ability to personalize text to image generation (kudos to Oceanswave and nicolai256 ) Enabled all of the samplers from k_diffusion","title":"v1.09 (24 August 2022)"},{"location":"CHANGELOG/#v108-24-august-2022","text":"Escape single quotes on the invoke> command before trying to parse. This avoids parse errors. Removed instruction to get Python3.8 as first step in Windows install. Anaconda3 does it for you. Added bounds checks for numeric arguments that could cause crashes. Cleaned up the copyright and license agreement files.","title":"v1.08 (24 August 2022)"},{"location":"CHANGELOG/#v107-23-august-2022","text":"Image filenames will now never fill gaps in the sequence, but will be assigned the next higher name in the chosen directory. This ensures that the alphabetic and chronological sort orders are the same.","title":"v1.07 (23 August 2022)"},{"location":"CHANGELOG/#v106-23-august-2022","text":"Added weighted prompt support contributed by xraxra Example of using weighted prompts to tweak a demonic figure contributed by bmaltais","title":"v1.06 (23 August 2022)"},{"location":"CHANGELOG/#v105-22-august-2022-after-the-drop","text":"Filenames now use the following formats: 000010.95183149.png -- Two files produced by the same command (e.g. -n2), 000010.26742632.png -- distinguished by a different seed. 000011.455191342.01.png -- Two files produced by the same command using 000011.455191342.02.png -- a batch size>1 (e.g. -b2). They have the same seed. 000011.4160627868.grid#1 -4.png -- a grid of four images (-g); the whole grid can be regenerated with the indicated key It should no longer be possible for one image to overwrite another You can use the \"cd\" and \"pwd\" commands at the invoke> prompt to set and retrieve the path of the output directory.","title":"v1.05 (22 August 2022 - after the drop)"},{"location":"CHANGELOG/#v104-22-august-2022-after-the-drop","text":"Updated README to reflect installation of the released weights. Suppressed very noisy and inconsequential warning when loading the frozen CLIP tokenizer.","title":"v1.04 (22 August 2022 - after the drop)"},{"location":"CHANGELOG/#v103-22-august-2022","text":"The original txt2img and img2img scripts from the CompViz repository have been moved into a subfolder named \"orig_scripts\", to reduce confusion.","title":"v1.03 (22 August 2022)"},{"location":"CHANGELOG/#v102-21-august-2022","text":"A copy of the prompt and all of its switches and options is now stored in the corresponding image in a tEXt metadata field named \"Dream\". You can read the prompt using scripts/images2prompt.py, or an image editor that allows you to explore the full metadata. Please run \"conda env update\" to load the k_lms dependencies!!","title":"v1.02 (21 August 2022)"},{"location":"CHANGELOG/#v101-21-august-2022","text":"added k_lms sampling. Please run \"conda env update\" to load the k_lms dependencies!! use half precision arithmetic by default, resulting in faster execution and lower memory requirements Pass argument --full_precision to invoke.py to get slower but more accurate image generation","title":"v1.01 (21 August 2022)"},{"location":"CHANGELOG/#links","text":"Read Me","title":"Links"},{"location":"features/","text":"Here you can find the documentation for InvokeAI's various features. The Basics # * The Web User Interface # Guide to the Web interface. Also see the WebUI Hotkeys Reference Guide * The Unified Canvas # Build complex scenes by combine and modifying multiple images in a stepwise fashion. This feature combines img2img, inpainting and outpainting in a single convenient digital artist-optimized user interface. * The Command Line Interface (CLI) # Scriptable access to InvokeAI's features. Image Generation # * Prompt Engineering # Get the images you want with the InvokeAI prompt engineering language. * Post-Processing # Restore mangled faces and make images larger with upscaling. Also see the Embiggen Upscaling Guide . * The Concepts Library # Add custom subjects and styles using HuggingFace's repository of embeddings. * Image-to-Image Guide for the CLI # Use a seed image to build new creations in the CLI. * Inpainting Guide for the CLI # Selectively erase and replace portions of an existing image in the CLI. * Outpainting Guide for the CLI # Extend the borders of the image with an \"outcrop\" function within the CLI. * Generating Variations # Have an image you like and want to generate many more like it? Variations are the ticket. Model Management # * Model Installation # Learn how to import third-party models and switch among them. This guide also covers optimizing models to load quickly. * Merging Models # Teach an old model new tricks. Merge 2-3 models together to create a new model that combines characteristics of the originals. * Textual Inversion # Personalize models by adding your own style or subjects. Other Features # * The NSFW Checker # Prevent InvokeAI from displaying unwanted racy images. * Miscellaneous # Run InvokeAI on Google Colab, generate images with repeating patterns, batch process a file of prompts, increase the \"creativity\" of image generation by adding initial noise, and more!","title":"Overview"},{"location":"features/#the-basics","text":"","title":"The Basics"},{"location":"features/#the-web-user-interface","text":"Guide to the Web interface. Also see the WebUI Hotkeys Reference Guide","title":"* The Web User Interface"},{"location":"features/#the-unified-canvas","text":"Build complex scenes by combine and modifying multiple images in a stepwise fashion. This feature combines img2img, inpainting and outpainting in a single convenient digital artist-optimized user interface.","title":"* The Unified Canvas"},{"location":"features/#the-command-line-interface-cli","text":"Scriptable access to InvokeAI's features.","title":"* The Command Line Interface (CLI)"},{"location":"features/#image-generation","text":"","title":"Image Generation"},{"location":"features/#prompt-engineering","text":"Get the images you want with the InvokeAI prompt engineering language.","title":"* Prompt Engineering"},{"location":"features/#post-processing","text":"Restore mangled faces and make images larger with upscaling. Also see the Embiggen Upscaling Guide .","title":"* Post-Processing"},{"location":"features/#the-concepts-library","text":"Add custom subjects and styles using HuggingFace's repository of embeddings.","title":"* The Concepts Library"},{"location":"features/#image-to-image-guide-for-the-cli","text":"Use a seed image to build new creations in the CLI.","title":"* Image-to-Image Guide for the CLI"},{"location":"features/#inpainting-guide-for-the-cli","text":"Selectively erase and replace portions of an existing image in the CLI.","title":"* Inpainting Guide for the CLI"},{"location":"features/#outpainting-guide-for-the-cli","text":"Extend the borders of the image with an \"outcrop\" function within the CLI.","title":"* Outpainting Guide for the CLI"},{"location":"features/#generating-variations","text":"Have an image you like and want to generate many more like it? Variations are the ticket.","title":"* Generating Variations"},{"location":"features/#model-management","text":"","title":"Model Management"},{"location":"features/#model-installation","text":"Learn how to import third-party models and switch among them. This guide also covers optimizing models to load quickly.","title":"* Model Installation"},{"location":"features/#merging-models","text":"Teach an old model new tricks. Merge 2-3 models together to create a new model that combines characteristics of the originals.","title":"* Merging Models"},{"location":"features/#textual-inversion","text":"Personalize models by adding your own style or subjects.","title":"* Textual Inversion"},{"location":"features/#other-features","text":"","title":"Other Features"},{"location":"features/#the-nsfw-checker","text":"Prevent InvokeAI from displaying unwanted racy images.","title":"* The NSFW Checker"},{"location":"features/#miscellaneous","text":"Run InvokeAI on Google Colab, generate images with repeating patterns, batch process a file of prompts, increase the \"creativity\" of image generation by adding initial noise, and more!","title":"* Miscellaneous"},{"location":"features/CLI/","text":"CLI # Interactive Command Line Interface # The InvokeAI command line interface (CLI) provides scriptable access to InvokeAI's features.Some advanced features are only available through the CLI, though they eventually find their way into the WebUI. The CLI is accessible from the invoke.sh / invoke.bat launcher by selecting option (1). Alternatively, it can be launched directly from the command line by activating the InvokeAI environment and giving the command: invokeai After some startup messages, you will be presented with the invoke> prompt. Here you can type prompts to generate images and issue other commands to load and manipulate generative models. The CLI has a large number of command-line options that control its behavior. To get a concise summary of the options, call invokeai with the --help argument: invokeai --help The script uses the readline library to allow for in-line editing, command history ( Up and Down ), autocompletion, and more. To help keep track of which prompts generated which images, the script writes a log file of image names and prompts to the selected output directory. Here is a typical session PS1:C: \\U sers \\f red> invokeai * Initializing, be patient... * Initializing, be patient... >> Initialization file /home/lstein/invokeai/invokeai.init found. Loading... >> Internet connectivity is True >> InvokeAI, version 2 .3.0-rc5 >> InvokeAI runtime directory is \"/home/lstein/invokeai\" >> GFPGAN Initialized >> CodeFormer Initialized >> ESRGAN Initialized >> Using device_type cuda >> xformers memory-efficient attention is available and enabled ( ...more initialization messages... ) * Initialization done ! Awaiting your command ( -h for help, 'q' to quit ) invoke> ashley judd riding a camel -n2 -s150 Outputs: outputs/img-samples/00009.png: \"ashley judd riding a camel\" -n2 -s150 -S 416354203 outputs/img-samples/00010.png: \"ashley judd riding a camel\" -n2 -s150 -S 1362479620 invoke> \"there's a fly in my soup\" -n6 -g outputs/img-samples/00011.png: \"there's a fly in my soup\" -n6 -g -S 2685670268 seeds for individual rows: [ 2685670268 , 1216708065 , 2335773498 , 822223658 , 714542046 , 3395302430 ] invoke> q Arguments # The script recognizes a series of command-line switches that will change important global defaults, such as the directory for image outputs and the location of the model weight files. List of arguments recognized at the command line # These command-line arguments can be passed to invoke.py when you first run it from the Windows, Mac or Linux command line. Some set defaults that can be overridden on a per-prompt basis (see List of prompt arguments . Others Argument Shortcut Default Description --help -h Print a concise help message. --outdir <path> -o<path> outputs/img_samples Location for generated images. --prompt_as_dir -p False Name output directories using the prompt text. --from_file <path> None Read list of prompts from a file. Use - to read from standard input --model <modelname> stable-diffusion-1.5 Loads the initial model specified in configs/models.yaml. --ckpt_convert False If provided both .ckpt and .safetensors files will be auto-converted into diffusers format in memory --autoconvert <path> None On startup, scan the indicated directory for new .ckpt/.safetensor files and automatically convert and import them --precision fp16 Provide fp32 for full precision mode, fp16 for half-precision. fp32 needed for Macintoshes and some NVidia cards. --png_compression <0-9> -z<0-9> 6 Select level of compression for output files, from 0 (no compression) to 9 (max compression) --safety-checker False Activate safety checker for NSFW and other potentially disturbing imagery --patchmatch , --no-patchmatch --patchmatch Load/Don't load the PatchMatch inpainting extension --xformers , --no-xformers --xformers Load/Don't load the Xformers memory-efficient attention module (CUDA only) --web False Start in web server mode --host <ip addr> localhost Which network interface web server should listen on. Set to 0.0.0.0 to listen on any. --port <port> 9090 Which port web server should listen for requests on. --config <path> configs/models.yaml Configuration file for models and their weights. --iterations <int> -n<int> 1 How many images to generate per prompt. --width <int> -W<int> 512 Width of generated image --height <int> -H<int> 512 Height of generated image --strength <float> -s<float> 0.75 For img2img: how hard to try to match the prompt to the initial image. Ranges from 0.0-0.99, with higher values replacing the initial image completely. --fit -F False For img2img: scale the init image to fit into the specified -H and -W dimensions --grid -g False Save all image series as a grid rather than individually. --sampler <sampler> -A<sampler> k_lms Sampler to use. Use -h to get list of available samplers. --seamless False Create interesting effects by tiling elements of the image. --embedding_path <path> None Path to pre-trained embedding manager checkpoints, for custom models --gfpgan_model_path experiments/pretrained_models/GFPGANv1.4.pth Path to GFPGAN model file. --free_gpu_mem False Free GPU memory after sampling, to allow image decoding and saving in low VRAM conditions --precision auto Set model precision, default is selected by device. Options: auto, float32, float16, autocast These arguments are deprecated but still work Argument Shortcut Default Description --full_precision False Same as --precision=fp32 --weights <path> None Path to weights file; use --model stable-diffusion-1.4 instead --laion400m -l False Use older LAION400m weights; use --model=laion400m instead Tip On Windows systems, you may run into problems when passing the invoke script standard backslashed path names because the Python interpreter treats \"\\\" as an escape. You can either double your slashes (ick): C:\\\\path\\\\to\\\\my\\\\file , or use Linux/Mac style forward slashes (better): C:/path/to/my/file . The .invokeai initialization file # To start up invoke.py with your preferred settings, place your desired startup options in a file in your home directory named .invokeai The file should contain the startup options as you would type them on the command line ( --steps=10 --grid ), one argument per line, or a mixture of both using any of the accepted command switch formats: my unmodified initialization file ~/.invokeai 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # InvokeAI initialization file # This is the InvokeAI initialization file, which contains command-line default values. # Feel free to edit. If anything goes wrong, you can re-initialize this file by deleting # or renaming it and then running invokeai-configure again. # The --root option below points to the folder in which InvokeAI stores its models, configs and outputs. --root = \"/Users/mauwii/invokeai\" # the --outdir option controls the default location of image files. --outdir = \"/Users/mauwii/invokeai/outputs\" # You may place other frequently-used startup commands here, one or more per line. # Examples: # --web --host=0.0.0.0 # --steps=20 # -Ak_euler_a -C10.0 Note The initialization file only accepts the command line arguments. There are additional arguments that you can provide on the invoke> command line (such as -n or --iterations ) that cannot be entered into this file. Also be alert for empty blank lines at the end of the file, which will cause an arguments error at startup time. List of prompt arguments # After the invoke.py script initializes, it will present you with a invoke> prompt. Here you can enter information to generate images from text ( txt2img ), to embellish an existing image or sketch ( img2img ), or to selectively alter chosen regions of the image ( inpainting ). txt2img # invoke> waterfall and rainbow -W640 -H480 This will create the requested image with the dimensions 640 (width) and 480 (height). Here are the invoke> command that apply to txt2img: Argument Shortcut Default Description \"my prompt\" Text prompt to use. The quotation marks are optional. --width <int> -W<int> 512 Width of generated image --height <int> -H<int> 512 Height of generated image --iterations <int> -n<int> 1 How many images to generate from this prompt --steps <int> -s<int> 50 How many steps of refinement to apply --cfg_scale <float> -C<float> 7.5 How hard to try to match the prompt to the generated image; any number greater than 1.0 works, but the useful range is roughly 5.0 to 20.0 --seed <int> -S<int> None Set the random seed for the next series of images. This can be used to recreate an image generated previously. --sampler <sampler> -A<sampler> k_lms Sampler to use. Use -h to get list of available samplers. --karras_max <int> 29 When using k_* samplers, set the maximum number of steps before shifting from using the Karras noise schedule (good for low step counts) to the LatentDiffusion noise schedule (good for high step counts) This value is sticky. [29] --hires_fix Larger images often have duplication artefacts. This option suppresses duplicates by generating the image at low res, and then using img2img to increase the resolution --png_compression <0-9> -z<0-9> 6 Select level of compression for output files, from 0 (no compression) to 9 (max compression) --grid -g False Turn on grid mode to return a single image combining all the images generated by this prompt --individual -i True Turn off grid mode (deprecated; leave off --grid instead) --outdir <path> -o<path> outputs/img_samples Temporarily change the location of these images --seamless False Activate seamless tiling for interesting effects --seamless_axes x,y Specify which axes to use circular convolution on. --log_tokenization -t False Display a color-coded list of the parsed tokens derived from the prompt --skip_normalization -x False Weighted subprompts will not be normalized. See Weighted Prompts --upscale <int> <float> -U <int> <float> -U 1 0.75 Upscale image by magnification factor (2, 4), and set strength of upscaling (0.0-1.0). If strength not set, will default to 0.75. --facetool_strength <float> -G <float> -G0 Fix faces (defaults to using the GFPGAN algorithm); argument indicates how hard the algorithm should try (0.0-1.0) --facetool <name> -ft <name> -ft gfpgan Select face restoration algorithm to use: gfpgan, codeformer --codeformer_fidelity -cf <float> 0.75 Used along with CodeFormer. Takes values between 0 and 1. 0 produces high quality but low accuracy. 1 produces high accuracy but low quality --save_original -save_orig False When upscaling or fixing faces, this will cause the original image to be saved rather than replaced. --variation <float> -v<float> 0.0 Add a bit of noise (0.0=none, 1.0=high) to the image in order to generate a series of variations. Usually used in combination with -S<seed> and -n<int> to generate a series a riffs on a starting image. See Variations . --with_variations <pattern> None Combine two or more variations. See Variations for now to use this. --save_intermediates <n> None Save the image from every nth step into an \"intermediates\" folder inside the output directory Note the width and height of the image must be multiples of 64. You can provide different values, but they will be rounded down to the nearest multiple of 64. This is a example of img2img invoke> waterfall and rainbow -I./vacation-photo.png -W640 -H480 --fit This will modify the indicated vacation photograph by making it more like the prompt. Results will vary greatly depending on what is in the image. We also ask to --fit the image into a box no bigger than 640x480. Otherwise the image size will be identical to the provided photo and you may run out of memory if it is large. In addition to the command-line options recognized by txt2img, img2img accepts additional options: Argument Shortcut Default Description --init_img <path> -I<path> None Path to the initialization image --fit -F False Scale the image to fit into the specified -H and -W dimensions --strength <float> -s<float> 0.75 How hard to try to match the prompt to the initial image. Ranges from 0.0-0.99, with higher values replacing the initial image completely. inpainting # invoke> waterfall and rainbow -I./vacation-photo.png -M./vacation-mask.png -W640 -H480 --fit This will do the same thing as img2img, but image alterations will only occur within transparent areas defined by the mask file specified by -M . You may also supply just a single initial image with the areas to overpaint made transparent, but you must be careful not to destroy the pixels underneath when you create the transparent areas. See Inpainting for details. inpainting accepts all the arguments used for txt2img and img2img, as well as the --mask (-M) and --text_mask (-tm) arguments: Argument Shortcut Default Description --init_mask <path> -M<path> None Path to an image the same size as the initial_image, with areas for inpainting made transparent. --invert_mask False If true, invert the mask so that transparent areas are opaque and vice versa. --text_mask <prompt> [<float>] -tm <prompt> [<float>] Create a mask from a text prompt describing part of the image The mask may either be an image with transparent areas, in which case the inpainting will occur in the transparent areas only, or a black and white image, in which case all black areas will be painted into. --text_mask (short form -tm ) is a way to generate a mask using a text description of the part of the image to replace. For example, if you have an image of a breakfast plate with a bagel, toast and scrambled eggs, you can selectively mask the bagel and replace it with a piece of cake this way: invoke> a piece of cake -I /path/to/breakfast.png -tm bagel The algorithm uses clipseg to classify different regions of the image. The classifier puts out a confidence score for each region it identifies. Generally regions that score above 0.5 are reliable, but if you are getting too much or too little masking you can adjust the threshold down (to get more mask), or up (to get less). In this example, by passing -tm a higher value, we are insisting on a more stringent classification. invoke> a piece of cake -I /path/to/breakfast.png -tm bagel 0 .6 Custom Styles and Subjects # You can load and use hundreds of community-contributed Textual Inversion models just by typing the appropriate trigger phrase. Please see Concepts Library for more details. Other Commands # The CLI offers a number of commands that begin with \"!\". Postprocessing images # To postprocess a file using face restoration or upscaling, use the !fix command. !fix # This command runs a post-processor on a previously-generated image. It takes a PNG filename or path and applies your choice of the -U , -G , or --embiggen switches in order to fix faces or upscale. If you provide a filename, the script will look for it in the current output directory. Otherwise you can provide a full or partial path to the desired file. Some examples: Upscale to 4X its original size and fix faces using codeformer invoke> !fix 0000045 .4829112.png -G1 -U4 -ft codeformer Use the GFPGAN algorithm to fix faces, then upscale to 3X using --embiggen invoke> !fix 0000045 .4829112.png -G0.8 -ft gfpgan >> fixing outputs/img-samples/0000045.4829112.png >> retrieved seed 4829112 and prompt \"boy enjoying a banana split\" >> GFPGAN - Restoring Faces for image seed:4829112 Outputs: [ 1 ] outputs/img-samples/000017.4829112.gfpgan-00.png: !fix \"outputs/img-samples/0000045.4829112.png\" -s 50 -S -W 512 -H 512 -C 7 .5 -A k_lms -G 0 .8 !mask # This command takes an image, a text prompt, and uses the clipseg algorithm to automatically generate a mask of the area that matches the text prompt. It is useful for debugging the text masking process prior to inpainting with the --text_mask argument. See [INPAINTING.md] for details. Model selection and importation # The CLI allows you to add new models on the fly, as well as to switch among them rapidly without leaving the script. There are several different model formats, each described in the Model Installation Guide . !models # This prints out a list of the models defined in `config/models.yaml'. The active model is bold-faced Example: inpainting-1.5 not loaded Stable Diffusion inpainting model stable-diffusion-1.5 active Stable Diffusion v1.5 waifu-diffusion not loaded Waifu Diffusion v1.4 !switch <model> # This quickly switches from one model to another without leaving the CLI script. invoke.py uses a memory caching system; once a model has been loaded, switching back and forth is quick. The following example shows this in action. Note how the second column of the !models table changes to cached after a model is first loaded, and that the long initialization step is not needed when loading a cached model. !import_model <hugging_face_repo_ID> # This imports and installs a diffusers -style model that is stored on the HuggingFace Web Site . You can look up any Stable Diffusion diffusers model and install it with a command like the following: !import_model prompthero/openjourney !import_model <path/to/diffusers/directory> # If you have a copy of a diffusers -style model saved to disk, you can import it by passing the path to model's top-level directory. !import_model <url> # For a .ckpt or .safetensors file, if you have a direct download URL for the file, you can provide it to !import_model and the file will be downloaded and installed for you. !import_model <path/to/model/weights.ckpt> # This command imports a new model weights file into InvokeAI, makes it available for image generation within the script, and writes out the configuration for the model into config/models.yaml for use in subsequent sessions. Provide !import_model with the path to a weights file ending in .ckpt . If you type a partial path and press tab, the CLI will autocomplete. Although it will also autocomplete to .vae files, these are not currenty supported (but will be soon). When you hit return, the CLI will prompt you to fill in additional information about the model, including the short name you wish to use for it with the !switch command, a brief description of the model, the default image width and height to use with this model, and the model's configuration file. The latter three fields are automatically filled with reasonable defaults. In the example below, the bold-faced text shows what the user typed in with the exception of the width, height and configuration file paths, which were filled in automatically. !import_model <path/to/directory_of_models> # If you provide the path of a directory that contains one or more .ckpt or .safetensors files, the CLI will scan the directory and interactively offer to import the models it finds there. Also see the --autoconvert command-line option. !edit_model <name_of_model> # The !edit_model command can be used to modify a model that is already defined in config/models.yaml . Call it with the short name of the model you wish to modify, and it will allow you to modify the model's description , weights and other fields. Example: invoke> !edit_model waifu-diffusion >> Editing model waifu-diffusion from configuration file ./configs/models.yaml description: Waifu diffusion v1.4beta weights: models/ldm/stable-diffusion-v1/ model-epoch10-float16.ckpt config: configs/stable-diffusion/v1-inference.yaml width: 512 height: 512 >> New configuration: waifu-diffusion: config: configs/stable-diffusion/v1-inference.yaml description: Waifu diffusion v1.4beta weights: models/ldm/stable-diffusion-v1/model-epoch10-float16.ckpt height: 512 width: 512 OK to import [n]? y >> Caching model stable-diffusion-1.4 in system RAM >> Loading waifu-diffusion from models/ldm/stable-diffusion-v1/model-epoch10-float16.ckpt ... History processing # The CLI provides a series of convenient commands for reviewing previous actions, retrieving them, modifying them, and re-running them. !history # The invoke script keeps track of all the commands you issue during a session, allowing you to re-run them. On Mac and Linux systems, it also writes the command-line history out to disk, giving you access to the most recent 1000 commands issued. The !history command will return a numbered list of all the commands issued during the session (Windows), or the most recent 1000 commands (Mac|Linux). You can then repeat a command by using the command !NNN , where \"NNN\" is the history line number. For example: invoke> !history ... [ 14 ] happy woman sitting under tree wearing broad hat and flowing garment [ 15 ] beautiful woman sitting under tree wearing broad hat and flowing garment [ 18 ] beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 [ 20 ] watercolor of beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 -S2878767194 [ 21 ] surrealist painting of beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 -S2878767194 ... invoke> !20 invoke> watercolor of beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 -S2878767194 !fetch # This command retrieves the generation parameters from a previously generated image and either loads them into the command line (Linux|Mac), or prints them out in a comment for copy-and-paste (Windows). You may provide either the name of a file in the current output directory, or a full file path. Specify path to a folder with image png files, and wildcard *.png to retrieve the dream command used to generate the images, and save them to a file commands.txt for further processing. load the generation command for a single png file invoke> !fetch 0000015 .8929913.png # the script returns the next line, ready for editing and running: invoke> a fantastic alien landscape -W 576 -H 512 -s 60 -A plms -C 7 .5 fetch the generation commands from a batch of files and store them into selected.txt invoke> !fetch outputs \\s elected-imgs \\* .png selected.txt !replay # This command replays a text file generated by !fetch or created manually Example invoke> !replay outputs \\s elected-imgs \\s elected.txt Note These commands may behave unexpectedly if given a PNG file that was not generated by InvokeAI. !search <search string> # This is similar to !history but it only returns lines that contain search string . For example: invoke> !search surreal [ 21 ] surrealist painting of beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 -S2878767194 !clear # This clears the search history from memory and disk. Be advised that this operation is irreversible and does not issue any warnings! Command-line editing and completion # The command-line offers convenient history tracking, editing, and command completion. To scroll through previous commands and potentially edit/reuse them, use the Up and Down keys. To edit the current command, use the Left and Right keys to position the cursor, and then Backspace , Del or insert characters. To move to the very beginning of the command, type Ctrl + A (or Cmd + A on the Mac) To move to the end of the command, type Ctrl + E . To cut a section of the command, position the cursor where you want to start cutting and type Ctrl + K To paste a cut section back in, position the cursor where you want to paste, and type Ctrl + Y Windows users can get similar, but more limited, functionality if they launch invoke.py with the winpty program and have the pyreadline3 library installed: > winpty python scripts\\invoke.py On the Mac and Linux platforms, when you exit invoke.py, the last 1000 lines of your command-line history will be saved. When you restart invoke.py , you can access the saved history using the Up key. In addition, limited command-line completion is installed. In various contexts, you can start typing your command and press Tab . A list of potential completions will be presented to you. You can then type a little more, hit Tab again, and eventually autocomplete what you want. When specifying file paths using the one-letter shortcuts, the CLI will attempt to complete pathnames for you. This is most handy for the -I (init image) and -M (init mask) paths. To initiate completion, start the path with a slash ( / ) or ./ . For example: invoke> zebra with a mustache -I./test-pictures<TAB> -I./test-pictures/Lincoln-and-Parrot.png -I./test-pictures/zebra.jpg -I./test-pictures/madonna.png -I./test-pictures/bad-sketch.png -I./test-pictures/man_with_eagle/ You can then type Z , hit Tab again, and it will autofill to zebra.jpg . More text completion features (such as autocompleting seeds) are on their way.","title":"Command-Line Interface"},{"location":"features/CLI/#cli","text":"","title":" CLI"},{"location":"features/CLI/#interactive-command-line-interface","text":"The InvokeAI command line interface (CLI) provides scriptable access to InvokeAI's features.Some advanced features are only available through the CLI, though they eventually find their way into the WebUI. The CLI is accessible from the invoke.sh / invoke.bat launcher by selecting option (1). Alternatively, it can be launched directly from the command line by activating the InvokeAI environment and giving the command: invokeai After some startup messages, you will be presented with the invoke> prompt. Here you can type prompts to generate images and issue other commands to load and manipulate generative models. The CLI has a large number of command-line options that control its behavior. To get a concise summary of the options, call invokeai with the --help argument: invokeai --help The script uses the readline library to allow for in-line editing, command history ( Up and Down ), autocompletion, and more. To help keep track of which prompts generated which images, the script writes a log file of image names and prompts to the selected output directory. Here is a typical session PS1:C: \\U sers \\f red> invokeai * Initializing, be patient... * Initializing, be patient... >> Initialization file /home/lstein/invokeai/invokeai.init found. Loading... >> Internet connectivity is True >> InvokeAI, version 2 .3.0-rc5 >> InvokeAI runtime directory is \"/home/lstein/invokeai\" >> GFPGAN Initialized >> CodeFormer Initialized >> ESRGAN Initialized >> Using device_type cuda >> xformers memory-efficient attention is available and enabled ( ...more initialization messages... ) * Initialization done ! Awaiting your command ( -h for help, 'q' to quit ) invoke> ashley judd riding a camel -n2 -s150 Outputs: outputs/img-samples/00009.png: \"ashley judd riding a camel\" -n2 -s150 -S 416354203 outputs/img-samples/00010.png: \"ashley judd riding a camel\" -n2 -s150 -S 1362479620 invoke> \"there's a fly in my soup\" -n6 -g outputs/img-samples/00011.png: \"there's a fly in my soup\" -n6 -g -S 2685670268 seeds for individual rows: [ 2685670268 , 1216708065 , 2335773498 , 822223658 , 714542046 , 3395302430 ] invoke> q","title":"Interactive Command Line Interface"},{"location":"features/CLI/#arguments","text":"The script recognizes a series of command-line switches that will change important global defaults, such as the directory for image outputs and the location of the model weight files.","title":"Arguments"},{"location":"features/CLI/#list-of-arguments-recognized-at-the-command-line","text":"These command-line arguments can be passed to invoke.py when you first run it from the Windows, Mac or Linux command line. Some set defaults that can be overridden on a per-prompt basis (see List of prompt arguments . Others Argument Shortcut Default Description --help -h Print a concise help message. --outdir <path> -o<path> outputs/img_samples Location for generated images. --prompt_as_dir -p False Name output directories using the prompt text. --from_file <path> None Read list of prompts from a file. Use - to read from standard input --model <modelname> stable-diffusion-1.5 Loads the initial model specified in configs/models.yaml. --ckpt_convert False If provided both .ckpt and .safetensors files will be auto-converted into diffusers format in memory --autoconvert <path> None On startup, scan the indicated directory for new .ckpt/.safetensor files and automatically convert and import them --precision fp16 Provide fp32 for full precision mode, fp16 for half-precision. fp32 needed for Macintoshes and some NVidia cards. --png_compression <0-9> -z<0-9> 6 Select level of compression for output files, from 0 (no compression) to 9 (max compression) --safety-checker False Activate safety checker for NSFW and other potentially disturbing imagery --patchmatch , --no-patchmatch --patchmatch Load/Don't load the PatchMatch inpainting extension --xformers , --no-xformers --xformers Load/Don't load the Xformers memory-efficient attention module (CUDA only) --web False Start in web server mode --host <ip addr> localhost Which network interface web server should listen on. Set to 0.0.0.0 to listen on any. --port <port> 9090 Which port web server should listen for requests on. --config <path> configs/models.yaml Configuration file for models and their weights. --iterations <int> -n<int> 1 How many images to generate per prompt. --width <int> -W<int> 512 Width of generated image --height <int> -H<int> 512 Height of generated image --strength <float> -s<float> 0.75 For img2img: how hard to try to match the prompt to the initial image. Ranges from 0.0-0.99, with higher values replacing the initial image completely. --fit -F False For img2img: scale the init image to fit into the specified -H and -W dimensions --grid -g False Save all image series as a grid rather than individually. --sampler <sampler> -A<sampler> k_lms Sampler to use. Use -h to get list of available samplers. --seamless False Create interesting effects by tiling elements of the image. --embedding_path <path> None Path to pre-trained embedding manager checkpoints, for custom models --gfpgan_model_path experiments/pretrained_models/GFPGANv1.4.pth Path to GFPGAN model file. --free_gpu_mem False Free GPU memory after sampling, to allow image decoding and saving in low VRAM conditions --precision auto Set model precision, default is selected by device. Options: auto, float32, float16, autocast These arguments are deprecated but still work Argument Shortcut Default Description --full_precision False Same as --precision=fp32 --weights <path> None Path to weights file; use --model stable-diffusion-1.4 instead --laion400m -l False Use older LAION400m weights; use --model=laion400m instead Tip On Windows systems, you may run into problems when passing the invoke script standard backslashed path names because the Python interpreter treats \"\\\" as an escape. You can either double your slashes (ick): C:\\\\path\\\\to\\\\my\\\\file , or use Linux/Mac style forward slashes (better): C:/path/to/my/file .","title":"List of arguments recognized at the command line"},{"location":"features/CLI/#the-invokeai-initialization-file","text":"To start up invoke.py with your preferred settings, place your desired startup options in a file in your home directory named .invokeai The file should contain the startup options as you would type them on the command line ( --steps=10 --grid ), one argument per line, or a mixture of both using any of the accepted command switch formats: my unmodified initialization file ~/.invokeai 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # InvokeAI initialization file # This is the InvokeAI initialization file, which contains command-line default values. # Feel free to edit. If anything goes wrong, you can re-initialize this file by deleting # or renaming it and then running invokeai-configure again. # The --root option below points to the folder in which InvokeAI stores its models, configs and outputs. --root = \"/Users/mauwii/invokeai\" # the --outdir option controls the default location of image files. --outdir = \"/Users/mauwii/invokeai/outputs\" # You may place other frequently-used startup commands here, one or more per line. # Examples: # --web --host=0.0.0.0 # --steps=20 # -Ak_euler_a -C10.0 Note The initialization file only accepts the command line arguments. There are additional arguments that you can provide on the invoke> command line (such as -n or --iterations ) that cannot be entered into this file. Also be alert for empty blank lines at the end of the file, which will cause an arguments error at startup time.","title":"The .invokeai initialization file"},{"location":"features/CLI/#list-of-prompt-arguments","text":"After the invoke.py script initializes, it will present you with a invoke> prompt. Here you can enter information to generate images from text ( txt2img ), to embellish an existing image or sketch ( img2img ), or to selectively alter chosen regions of the image ( inpainting ).","title":"List of prompt arguments"},{"location":"features/CLI/#txt2img","text":"invoke> waterfall and rainbow -W640 -H480 This will create the requested image with the dimensions 640 (width) and 480 (height). Here are the invoke> command that apply to txt2img: Argument Shortcut Default Description \"my prompt\" Text prompt to use. The quotation marks are optional. --width <int> -W<int> 512 Width of generated image --height <int> -H<int> 512 Height of generated image --iterations <int> -n<int> 1 How many images to generate from this prompt --steps <int> -s<int> 50 How many steps of refinement to apply --cfg_scale <float> -C<float> 7.5 How hard to try to match the prompt to the generated image; any number greater than 1.0 works, but the useful range is roughly 5.0 to 20.0 --seed <int> -S<int> None Set the random seed for the next series of images. This can be used to recreate an image generated previously. --sampler <sampler> -A<sampler> k_lms Sampler to use. Use -h to get list of available samplers. --karras_max <int> 29 When using k_* samplers, set the maximum number of steps before shifting from using the Karras noise schedule (good for low step counts) to the LatentDiffusion noise schedule (good for high step counts) This value is sticky. [29] --hires_fix Larger images often have duplication artefacts. This option suppresses duplicates by generating the image at low res, and then using img2img to increase the resolution --png_compression <0-9> -z<0-9> 6 Select level of compression for output files, from 0 (no compression) to 9 (max compression) --grid -g False Turn on grid mode to return a single image combining all the images generated by this prompt --individual -i True Turn off grid mode (deprecated; leave off --grid instead) --outdir <path> -o<path> outputs/img_samples Temporarily change the location of these images --seamless False Activate seamless tiling for interesting effects --seamless_axes x,y Specify which axes to use circular convolution on. --log_tokenization -t False Display a color-coded list of the parsed tokens derived from the prompt --skip_normalization -x False Weighted subprompts will not be normalized. See Weighted Prompts --upscale <int> <float> -U <int> <float> -U 1 0.75 Upscale image by magnification factor (2, 4), and set strength of upscaling (0.0-1.0). If strength not set, will default to 0.75. --facetool_strength <float> -G <float> -G0 Fix faces (defaults to using the GFPGAN algorithm); argument indicates how hard the algorithm should try (0.0-1.0) --facetool <name> -ft <name> -ft gfpgan Select face restoration algorithm to use: gfpgan, codeformer --codeformer_fidelity -cf <float> 0.75 Used along with CodeFormer. Takes values between 0 and 1. 0 produces high quality but low accuracy. 1 produces high accuracy but low quality --save_original -save_orig False When upscaling or fixing faces, this will cause the original image to be saved rather than replaced. --variation <float> -v<float> 0.0 Add a bit of noise (0.0=none, 1.0=high) to the image in order to generate a series of variations. Usually used in combination with -S<seed> and -n<int> to generate a series a riffs on a starting image. See Variations . --with_variations <pattern> None Combine two or more variations. See Variations for now to use this. --save_intermediates <n> None Save the image from every nth step into an \"intermediates\" folder inside the output directory Note the width and height of the image must be multiples of 64. You can provide different values, but they will be rounded down to the nearest multiple of 64. This is a example of img2img invoke> waterfall and rainbow -I./vacation-photo.png -W640 -H480 --fit This will modify the indicated vacation photograph by making it more like the prompt. Results will vary greatly depending on what is in the image. We also ask to --fit the image into a box no bigger than 640x480. Otherwise the image size will be identical to the provided photo and you may run out of memory if it is large. In addition to the command-line options recognized by txt2img, img2img accepts additional options: Argument Shortcut Default Description --init_img <path> -I<path> None Path to the initialization image --fit -F False Scale the image to fit into the specified -H and -W dimensions --strength <float> -s<float> 0.75 How hard to try to match the prompt to the initial image. Ranges from 0.0-0.99, with higher values replacing the initial image completely.","title":"txt2img"},{"location":"features/CLI/#inpainting","text":"invoke> waterfall and rainbow -I./vacation-photo.png -M./vacation-mask.png -W640 -H480 --fit This will do the same thing as img2img, but image alterations will only occur within transparent areas defined by the mask file specified by -M . You may also supply just a single initial image with the areas to overpaint made transparent, but you must be careful not to destroy the pixels underneath when you create the transparent areas. See Inpainting for details. inpainting accepts all the arguments used for txt2img and img2img, as well as the --mask (-M) and --text_mask (-tm) arguments: Argument Shortcut Default Description --init_mask <path> -M<path> None Path to an image the same size as the initial_image, with areas for inpainting made transparent. --invert_mask False If true, invert the mask so that transparent areas are opaque and vice versa. --text_mask <prompt> [<float>] -tm <prompt> [<float>] Create a mask from a text prompt describing part of the image The mask may either be an image with transparent areas, in which case the inpainting will occur in the transparent areas only, or a black and white image, in which case all black areas will be painted into. --text_mask (short form -tm ) is a way to generate a mask using a text description of the part of the image to replace. For example, if you have an image of a breakfast plate with a bagel, toast and scrambled eggs, you can selectively mask the bagel and replace it with a piece of cake this way: invoke> a piece of cake -I /path/to/breakfast.png -tm bagel The algorithm uses clipseg to classify different regions of the image. The classifier puts out a confidence score for each region it identifies. Generally regions that score above 0.5 are reliable, but if you are getting too much or too little masking you can adjust the threshold down (to get more mask), or up (to get less). In this example, by passing -tm a higher value, we are insisting on a more stringent classification. invoke> a piece of cake -I /path/to/breakfast.png -tm bagel 0 .6","title":"inpainting"},{"location":"features/CLI/#custom-styles-and-subjects","text":"You can load and use hundreds of community-contributed Textual Inversion models just by typing the appropriate trigger phrase. Please see Concepts Library for more details.","title":"Custom Styles and Subjects"},{"location":"features/CLI/#other-commands","text":"The CLI offers a number of commands that begin with \"!\".","title":"Other Commands"},{"location":"features/CLI/#postprocessing-images","text":"To postprocess a file using face restoration or upscaling, use the !fix command.","title":"Postprocessing images"},{"location":"features/CLI/#fix","text":"This command runs a post-processor on a previously-generated image. It takes a PNG filename or path and applies your choice of the -U , -G , or --embiggen switches in order to fix faces or upscale. If you provide a filename, the script will look for it in the current output directory. Otherwise you can provide a full or partial path to the desired file. Some examples: Upscale to 4X its original size and fix faces using codeformer invoke> !fix 0000045 .4829112.png -G1 -U4 -ft codeformer Use the GFPGAN algorithm to fix faces, then upscale to 3X using --embiggen invoke> !fix 0000045 .4829112.png -G0.8 -ft gfpgan >> fixing outputs/img-samples/0000045.4829112.png >> retrieved seed 4829112 and prompt \"boy enjoying a banana split\" >> GFPGAN - Restoring Faces for image seed:4829112 Outputs: [ 1 ] outputs/img-samples/000017.4829112.gfpgan-00.png: !fix \"outputs/img-samples/0000045.4829112.png\" -s 50 -S -W 512 -H 512 -C 7 .5 -A k_lms -G 0 .8","title":"!fix"},{"location":"features/CLI/#mask","text":"This command takes an image, a text prompt, and uses the clipseg algorithm to automatically generate a mask of the area that matches the text prompt. It is useful for debugging the text masking process prior to inpainting with the --text_mask argument. See [INPAINTING.md] for details.","title":"!mask"},{"location":"features/CLI/#model-selection-and-importation","text":"The CLI allows you to add new models on the fly, as well as to switch among them rapidly without leaving the script. There are several different model formats, each described in the Model Installation Guide .","title":"Model selection and importation"},{"location":"features/CLI/#models","text":"This prints out a list of the models defined in `config/models.yaml'. The active model is bold-faced Example: inpainting-1.5 not loaded Stable Diffusion inpainting model stable-diffusion-1.5 active Stable Diffusion v1.5 waifu-diffusion not loaded Waifu Diffusion v1.4","title":"!models"},{"location":"features/CLI/#switch-model","text":"This quickly switches from one model to another without leaving the CLI script. invoke.py uses a memory caching system; once a model has been loaded, switching back and forth is quick. The following example shows this in action. Note how the second column of the !models table changes to cached after a model is first loaded, and that the long initialization step is not needed when loading a cached model.","title":"!switch &lt;model&gt;"},{"location":"features/CLI/#import_model-hugging_face_repo_id","text":"This imports and installs a diffusers -style model that is stored on the HuggingFace Web Site . You can look up any Stable Diffusion diffusers model and install it with a command like the following: !import_model prompthero/openjourney","title":"!import_model &lt;hugging_face_repo_ID&gt;"},{"location":"features/CLI/#import_model-pathtodiffusersdirectory","text":"If you have a copy of a diffusers -style model saved to disk, you can import it by passing the path to model's top-level directory.","title":"!import_model &lt;path/to/diffusers/directory&gt;"},{"location":"features/CLI/#import_model-url","text":"For a .ckpt or .safetensors file, if you have a direct download URL for the file, you can provide it to !import_model and the file will be downloaded and installed for you.","title":"!import_model &lt;url&gt;"},{"location":"features/CLI/#import_model-pathtomodelweightsckpt","text":"This command imports a new model weights file into InvokeAI, makes it available for image generation within the script, and writes out the configuration for the model into config/models.yaml for use in subsequent sessions. Provide !import_model with the path to a weights file ending in .ckpt . If you type a partial path and press tab, the CLI will autocomplete. Although it will also autocomplete to .vae files, these are not currenty supported (but will be soon). When you hit return, the CLI will prompt you to fill in additional information about the model, including the short name you wish to use for it with the !switch command, a brief description of the model, the default image width and height to use with this model, and the model's configuration file. The latter three fields are automatically filled with reasonable defaults. In the example below, the bold-faced text shows what the user typed in with the exception of the width, height and configuration file paths, which were filled in automatically.","title":"!import_model &lt;path/to/model/weights.ckpt&gt;"},{"location":"features/CLI/#import_model-pathtodirectory_of_models","text":"If you provide the path of a directory that contains one or more .ckpt or .safetensors files, the CLI will scan the directory and interactively offer to import the models it finds there. Also see the --autoconvert command-line option.","title":"!import_model &lt;path/to/directory_of_models&gt;"},{"location":"features/CLI/#edit_model-name_of_model","text":"The !edit_model command can be used to modify a model that is already defined in config/models.yaml . Call it with the short name of the model you wish to modify, and it will allow you to modify the model's description , weights and other fields. Example: invoke> !edit_model waifu-diffusion >> Editing model waifu-diffusion from configuration file ./configs/models.yaml description: Waifu diffusion v1.4beta weights: models/ldm/stable-diffusion-v1/ model-epoch10-float16.ckpt config: configs/stable-diffusion/v1-inference.yaml width: 512 height: 512 >> New configuration: waifu-diffusion: config: configs/stable-diffusion/v1-inference.yaml description: Waifu diffusion v1.4beta weights: models/ldm/stable-diffusion-v1/model-epoch10-float16.ckpt height: 512 width: 512 OK to import [n]? y >> Caching model stable-diffusion-1.4 in system RAM >> Loading waifu-diffusion from models/ldm/stable-diffusion-v1/model-epoch10-float16.ckpt ...","title":"!edit_model &lt;name_of_model&gt;"},{"location":"features/CLI/#history-processing","text":"The CLI provides a series of convenient commands for reviewing previous actions, retrieving them, modifying them, and re-running them.","title":"History processing"},{"location":"features/CLI/#history","text":"The invoke script keeps track of all the commands you issue during a session, allowing you to re-run them. On Mac and Linux systems, it also writes the command-line history out to disk, giving you access to the most recent 1000 commands issued. The !history command will return a numbered list of all the commands issued during the session (Windows), or the most recent 1000 commands (Mac|Linux). You can then repeat a command by using the command !NNN , where \"NNN\" is the history line number. For example: invoke> !history ... [ 14 ] happy woman sitting under tree wearing broad hat and flowing garment [ 15 ] beautiful woman sitting under tree wearing broad hat and flowing garment [ 18 ] beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 [ 20 ] watercolor of beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 -S2878767194 [ 21 ] surrealist painting of beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 -S2878767194 ... invoke> !20 invoke> watercolor of beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 -S2878767194","title":"!history"},{"location":"features/CLI/#fetch","text":"This command retrieves the generation parameters from a previously generated image and either loads them into the command line (Linux|Mac), or prints them out in a comment for copy-and-paste (Windows). You may provide either the name of a file in the current output directory, or a full file path. Specify path to a folder with image png files, and wildcard *.png to retrieve the dream command used to generate the images, and save them to a file commands.txt for further processing. load the generation command for a single png file invoke> !fetch 0000015 .8929913.png # the script returns the next line, ready for editing and running: invoke> a fantastic alien landscape -W 576 -H 512 -s 60 -A plms -C 7 .5 fetch the generation commands from a batch of files and store them into selected.txt invoke> !fetch outputs \\s elected-imgs \\* .png selected.txt","title":"!fetch"},{"location":"features/CLI/#replay","text":"This command replays a text file generated by !fetch or created manually Example invoke> !replay outputs \\s elected-imgs \\s elected.txt Note These commands may behave unexpectedly if given a PNG file that was not generated by InvokeAI.","title":"!replay"},{"location":"features/CLI/#search-search-string","text":"This is similar to !history but it only returns lines that contain search string . For example: invoke> !search surreal [ 21 ] surrealist painting of beautiful woman sitting under tree wearing broad hat and flowing garment -v0.2 -n6 -S2878767194","title":"!search &lt;search string&gt;"},{"location":"features/CLI/#clear","text":"This clears the search history from memory and disk. Be advised that this operation is irreversible and does not issue any warnings!","title":"!clear"},{"location":"features/CLI/#command-line-editing-and-completion","text":"The command-line offers convenient history tracking, editing, and command completion. To scroll through previous commands and potentially edit/reuse them, use the Up and Down keys. To edit the current command, use the Left and Right keys to position the cursor, and then Backspace , Del or insert characters. To move to the very beginning of the command, type Ctrl + A (or Cmd + A on the Mac) To move to the end of the command, type Ctrl + E . To cut a section of the command, position the cursor where you want to start cutting and type Ctrl + K To paste a cut section back in, position the cursor where you want to paste, and type Ctrl + Y Windows users can get similar, but more limited, functionality if they launch invoke.py with the winpty program and have the pyreadline3 library installed: > winpty python scripts\\invoke.py On the Mac and Linux platforms, when you exit invoke.py, the last 1000 lines of your command-line history will be saved. When you restart invoke.py , you can access the saved history using the Up key. In addition, limited command-line completion is installed. In various contexts, you can start typing your command and press Tab . A list of potential completions will be presented to you. You can then type a little more, hit Tab again, and eventually autocomplete what you want. When specifying file paths using the one-letter shortcuts, the CLI will attempt to complete pathnames for you. This is most handy for the -I (init image) and -M (init mask) paths. To initiate completion, start the path with a slash ( / ) or ./ . For example: invoke> zebra with a mustache -I./test-pictures<TAB> -I./test-pictures/Lincoln-and-Parrot.png -I./test-pictures/zebra.jpg -I./test-pictures/madonna.png -I./test-pictures/bad-sketch.png -I./test-pictures/man_with_eagle/ You can then type Z , hit Tab again, and it will autofill to zebra.jpg . More text completion features (such as autocompleting seeds) are on their way.","title":"Command-line editing and completion"},{"location":"features/CONCEPTS/","text":"The Hugging Face Concepts Library and Importing Textual Inversion files # Using Textual Inversion Files # Textual inversion (TI) files are small models that customize the output of Stable Diffusion image generation. They can augment SD with specialized subjects and artistic styles. They are also known as \"embeds\" in the machine learning world. Each TI file introduces one or more vocabulary terms to the SD model. These are known in InvokeAI as \"triggers.\" Triggers are often, but not always, denoted using angle brackets as in \"<trigger-phrase>\". The two most common type of TI files that you'll encounter are .pt and .bin files, which are produced by different TI training packages. InvokeAI supports both formats, but its built-in TI training system produces .pt . The Hugging Face company has amassed a large ligrary of >800 community-contributed TI files covering a broad range of subjects and styles. InvokeAI has built-in support for this library which downloads and merges TI files automatically upon request. You can also install your own or others' TI files by placing them in a designated directory. An Example # Here are a few examples to illustrate how it works. All these images were generated using the command-line client and the Stable Diffusion 1.5 model: Japanese gardener Japanese gardener <ghibli-face> Japanese gardener <hoi4-leaders> Japanese gardener <cartoona-animals> You can also combine styles and concepts: A portrait of <alf> in <cartoona-animal> style Using a Hugging Face Concept # Authenticating to HuggingFace Some concepts require valid authentication to HuggingFace. Without it, they will not be downloaded and will be silently ignored. If you used an installer to install InvokeAI, you may have already set a HuggingFace token. If you skipped this step, you can: run the InvokeAI configuration script again (if you used a manual installer): invokeai-configure set one of the HUGGINGFACE_TOKEN or HUGGING_FACE_HUB_TOKEN environment variables to contain your token Finally, if you already used any HuggingFace library on your computer, you might already have a token in your local cache. Check for a hidden .huggingface directory in your home folder. If it contains a token file, then you are all set. Hugging Face TI concepts are downloaded and installed automatically as you require them. This requires your machine to be connected to the Internet. To find out what each concept is for, you can browse the Hugging Face concepts library and look at examples of what each concept produces. When you have an idea of a concept you wish to try, go to the command-line client (CLI) and type a < character and the beginning of the Hugging Face concept name you wish to load. Press Tab , and the CLI will show you all matching concepts. You can also type < and hit Tab to get a listing of all ~800 concepts, but be prepared to scroll up to see them all! If there is more than one match you can continue to type and Tab until the concept is completed. Example if you type in <x and hit Tab , you'll be prompted with the completions: < xatu2 > < xatu > < xbh > < xi > < xidiversity > < xioboma > < xuna > < xyz > Now type id and press Tab . It will be autocompleted to <xidiversity> because this is a unique match. Finish your prompt and generate as usual. You may include multiple concept terms in the prompt. If you have never used this concept before, you will see a message that the TI model is being downloaded and installed. After this, the concept will be saved locally (in the models/sd-concepts-library directory) for future use. Several steps happen during downloading and installation, including a scan of the file for malicious code. Should any errors occur, you will be warned and the concept will fail to load. Generation will then continue treating the trigger term as a normal string of characters (e.g. as literal <ghibli-face> ). You can also use <concept-names> in the WebGUI's prompt textbox. There is no autocompletion at this time. Installing your Own TI Files # You may install any number of .pt and .bin files simply by copying them into the embeddings directory of the InvokeAI runtime directory (usually invokeai in your home directory). You may create subdirectories in order to organize the files in any way you wish. Be careful not to overwrite one file with another. For example, TI files generated by the Hugging Face toolkit share the named learned_embedding.bin . You can use subdirectories to keep them distinct. At startup time, InvokeAI will scan the embeddings directory and load any TI files it finds there. At startup you will see a message similar to this one: >> Current embedding manager terms: *, <HOI4-Leader>, <princess-knight> Note the * trigger term. This is a placeholder term that many early TI tutorials taught people to use rather than a more descriptive term. Unfortunately, if you have multiple TI files that all use this term, only the first one loaded will be triggered by use of the term. To avoid this problem, you can use the merge_embeddings.py script to merge two or more TI files together. If it encounters a collision of terms, the script will prompt you to select new terms that do not collide. See Textual Inversion for details. Further Reading # Please see the repository and associated paper for details and limitations.","title":"Concepts Library"},{"location":"features/CONCEPTS/#the-hugging-face-concepts-library-and-importing-textual-inversion-files","text":"","title":" The Hugging Face Concepts Library and Importing Textual Inversion files"},{"location":"features/CONCEPTS/#using-textual-inversion-files","text":"Textual inversion (TI) files are small models that customize the output of Stable Diffusion image generation. They can augment SD with specialized subjects and artistic styles. They are also known as \"embeds\" in the machine learning world. Each TI file introduces one or more vocabulary terms to the SD model. These are known in InvokeAI as \"triggers.\" Triggers are often, but not always, denoted using angle brackets as in \"<trigger-phrase>\". The two most common type of TI files that you'll encounter are .pt and .bin files, which are produced by different TI training packages. InvokeAI supports both formats, but its built-in TI training system produces .pt . The Hugging Face company has amassed a large ligrary of >800 community-contributed TI files covering a broad range of subjects and styles. InvokeAI has built-in support for this library which downloads and merges TI files automatically upon request. You can also install your own or others' TI files by placing them in a designated directory.","title":"Using Textual Inversion Files"},{"location":"features/CONCEPTS/#an-example","text":"Here are a few examples to illustrate how it works. All these images were generated using the command-line client and the Stable Diffusion 1.5 model: Japanese gardener Japanese gardener <ghibli-face> Japanese gardener <hoi4-leaders> Japanese gardener <cartoona-animals> You can also combine styles and concepts: A portrait of <alf> in <cartoona-animal> style","title":"An Example"},{"location":"features/CONCEPTS/#using-a-hugging-face-concept","text":"Authenticating to HuggingFace Some concepts require valid authentication to HuggingFace. Without it, they will not be downloaded and will be silently ignored. If you used an installer to install InvokeAI, you may have already set a HuggingFace token. If you skipped this step, you can: run the InvokeAI configuration script again (if you used a manual installer): invokeai-configure set one of the HUGGINGFACE_TOKEN or HUGGING_FACE_HUB_TOKEN environment variables to contain your token Finally, if you already used any HuggingFace library on your computer, you might already have a token in your local cache. Check for a hidden .huggingface directory in your home folder. If it contains a token file, then you are all set. Hugging Face TI concepts are downloaded and installed automatically as you require them. This requires your machine to be connected to the Internet. To find out what each concept is for, you can browse the Hugging Face concepts library and look at examples of what each concept produces. When you have an idea of a concept you wish to try, go to the command-line client (CLI) and type a < character and the beginning of the Hugging Face concept name you wish to load. Press Tab , and the CLI will show you all matching concepts. You can also type < and hit Tab to get a listing of all ~800 concepts, but be prepared to scroll up to see them all! If there is more than one match you can continue to type and Tab until the concept is completed. Example if you type in <x and hit Tab , you'll be prompted with the completions: < xatu2 > < xatu > < xbh > < xi > < xidiversity > < xioboma > < xuna > < xyz > Now type id and press Tab . It will be autocompleted to <xidiversity> because this is a unique match. Finish your prompt and generate as usual. You may include multiple concept terms in the prompt. If you have never used this concept before, you will see a message that the TI model is being downloaded and installed. After this, the concept will be saved locally (in the models/sd-concepts-library directory) for future use. Several steps happen during downloading and installation, including a scan of the file for malicious code. Should any errors occur, you will be warned and the concept will fail to load. Generation will then continue treating the trigger term as a normal string of characters (e.g. as literal <ghibli-face> ). You can also use <concept-names> in the WebGUI's prompt textbox. There is no autocompletion at this time.","title":"Using a Hugging Face Concept"},{"location":"features/CONCEPTS/#installing-your-own-ti-files","text":"You may install any number of .pt and .bin files simply by copying them into the embeddings directory of the InvokeAI runtime directory (usually invokeai in your home directory). You may create subdirectories in order to organize the files in any way you wish. Be careful not to overwrite one file with another. For example, TI files generated by the Hugging Face toolkit share the named learned_embedding.bin . You can use subdirectories to keep them distinct. At startup time, InvokeAI will scan the embeddings directory and load any TI files it finds there. At startup you will see a message similar to this one: >> Current embedding manager terms: *, <HOI4-Leader>, <princess-knight> Note the * trigger term. This is a placeholder term that many early TI tutorials taught people to use rather than a more descriptive term. Unfortunately, if you have multiple TI files that all use this term, only the first one loaded will be triggered by use of the term. To avoid this problem, you can use the merge_embeddings.py script to merge two or more TI files together. If it encounters a collision of terms, the script will prompt you to select new terms that do not collide. See Textual Inversion for details.","title":"Installing your Own TI Files"},{"location":"features/CONCEPTS/#further-reading","text":"Please see the repository and associated paper for details and limitations.","title":"Further Reading"},{"location":"features/EMBIGGEN/","text":"Embiggen # upscale your images on limited memory machines GFPGAN and Real-ESRGAN are both memory intensive. In order to avoid crashes and memory overloads during the Stable Diffusion process, these effects are applied after Stable Diffusion has completed its work. In single image generations, you will see the output right away but when you are using multiple iterations, the images will first be generated and then upscaled and face restored after that process is complete. While the image generation is taking place, you will still be able to preview the base images. If you wish to stop during the image generation but want to upscale or face restore a particular generated image, pass it again with the same prompt and generated seed along with the -U and -G prompt arguments to perform those actions. Embiggen # If you wanted to be able to do more (pixels) without running out of VRAM, or you want to upscale with details that couldn't possibly appear without the context of a prompt, this is the feature to try out. Embiggen automates the process of taking an init image, upscaling it, cutting it into smaller tiles that slightly overlap, running all the tiles through img2img to refine details with respect to the prompt, and \"stitching\" the tiles back together into a cohesive image. It automatically computes how many tiles are needed, and so it can be fed ANY size init image and perform Img2Img on it (though it will be run only one tile at a time, which can cause problems, see the Note at the end). If you're familiar with \"GoBig\" (ala progrock-stable ) it's similar to that, except it can work up to an arbitrarily large size (instead of just 2x), with tile overlaps configurable as a ratio, and has extra logic to re-run any number of the tile sub-sections of the image if for example a small part of a huge run got messed up. Usage # -embiggen <scaling_factor> <esrgan_strength> <overlap_ratio OR overlap_pixels> Takes a scaling factor relative to the size of the --init_img ( -I ), followed by ESRGAN upscaling strength (0 - 1.0), followed by minimum amount of overlap between tiles as a decimal ratio (0 - 1.0) OR a number of pixels. The scaling factor is how much larger than the --init_img the output should be, and will multiply both x and y axis, so an image that is a scaling factor of 3.0 has 3*3= 9 times as many pixels, and will take (at least) 9 times as long (see overlap for why it might be longer). If the --init_img is already the right size -embiggen 1 , and it can also be less than one if the init_img is too big. Esrgan_strength defaults to 0.75, and the overlap_ratio defaults to 0.25, both are optional. Unlike Img2Img, the --width ( -W ) and --height ( -H ) arguments do not control the size of the image as a whole, but the size of the tiles used to Embiggen the image. ESRGAN is used to upscale the --init_img prior to cutting it into tiles/pieces to run through img2img and then stitch back together. Embiggen can be run without ESRGAN; just set the strength to zero (e.g. -embiggen 1.75 0 ). The output of Embiggen can also be upscaled after it's finished ( -U ). The overlap is the minimum that tiles will overlap with adjacent tiles, specified as either a ratio or a number of pixels. How much the tiles overlap determines the likelihood the tiling will be noticable, really small overlaps (e.g. a couple of pixels) may produce noticeable grid-like fuzzy distortions in the final stitched image. Though, as the overlapping space doesn't contribute to making the image bigger, and the larger the overlap the more tiles (and the more time) it will take to finish. Because the overlapping parts of tiles don't \"contribute\" to increasing size, every tile after the first in a row or column effectively only covers an extra 1 - overlap_ratio on each axis. If the input/ --init_img is same size as a tile, the ideal (for time) scaling factors with the default overlap (0.25) are 1.75, 2.5, 3.25, 4.0, etc. -embiggen_tiles <spaced list of tiles> An advanced usage useful if you only want to alter parts of the image while running Embiggen. It takes a list of tiles by number to run and replace onto the initial image e.g. 1 3 5 . It's useful for either fixing problem spots from a previous Embiggen run, or selectively altering the prompt for sections of an image - for creative or coherency reasons. Tiles are numbered starting with one, and left-to-right, top-to-bottom. So, if you are generating a 3x3 tiled image, the middle row would be 4 5 6 . -embiggen_strength <strength> Another advanced option if you want to experiment with the strength parameter that embiggen uses when it calls Img2Img. Values range from 0.0 to 1.0 and lower values preserve more of the character of the initial image. Values that are too high will result in a completely different end image, while values that are too low will result in an image not dissimilar to one you would get with ESRGAN upscaling alone. The default value is 0.4. Examples # Running Embiggen with 512x512 tiles on an existing image, scaling up by a factor of 2.5x; and doing the same again (default ESRGAN strength is 0.75, default overlap between tiles is 0.25): invoke > a photo of a forest at sunset -s 100 -W 512 -H 512 -I outputs/forest.png -f 0 .4 -embiggen 2 .5 invoke > a photo of a forest at sunset -s 100 -W 512 -H 512 -I outputs/forest.png -f 0 .4 -embiggen 2 .5 0 .75 0 .25 If your starting image was also 512x512 this should have taken 9 tiles. If there weren't enough clouds in the sky of that forest you just made (and that image is about 1280 pixels (512*2.5) wide A.K.A. three 512x512 tiles with 0.25 overlaps wide) we can replace that top row of tiles: invoke> a photo of puffy clouds over a forest at sunset -s 100 -W 512 -H 512 -I outputs/000002.seed.png -f 0 .5 -embiggen_tiles 1 2 3 Fixing Previously-Generated Images # It is easy to apply embiggen to any previously-generated file without having to look up the original prompt and provide an initial image. Just use the syntax !fix path/to/file.png <embiggen> . For example, you can rewrite the previous command to look like this: invoke> !fix ./outputs/000002.seed.png -embiggen_tiles 1 2 3 A new file named 000002.seed.fixed.png will be created in the output directory. Note that the !fix command does not replace the original file, unlike the behavior at generate time. You do not need to provide the prompt, and !fix automatically selects a good strength for embiggen-ing. Note Because the same prompt is used on all the tiled images, and the model doesn't have the context of anything outside the tile being run - it can end up creating repeated pattern (also called 'motifs') across all the tiles based on that prompt. The best way to combat this is lowering the --strength ( -f ) to stay more true to the init image, and increasing the number of steps so there is more compute-time to create the detail. Anecdotally --strength 0.35-0.45 works pretty well on most things. It may also work great in some examples even with the --strength set high for patterns, landscapes, or subjects that are more abstract. Because this is (relatively) fast, you can also preserve the best parts from each. Author: Travco","title":"Embiggen"},{"location":"features/EMBIGGEN/#embiggen","text":"upscale your images on limited memory machines GFPGAN and Real-ESRGAN are both memory intensive. In order to avoid crashes and memory overloads during the Stable Diffusion process, these effects are applied after Stable Diffusion has completed its work. In single image generations, you will see the output right away but when you are using multiple iterations, the images will first be generated and then upscaled and face restored after that process is complete. While the image generation is taking place, you will still be able to preview the base images. If you wish to stop during the image generation but want to upscale or face restore a particular generated image, pass it again with the same prompt and generated seed along with the -U and -G prompt arguments to perform those actions.","title":" Embiggen"},{"location":"features/EMBIGGEN/#embiggen_1","text":"If you wanted to be able to do more (pixels) without running out of VRAM, or you want to upscale with details that couldn't possibly appear without the context of a prompt, this is the feature to try out. Embiggen automates the process of taking an init image, upscaling it, cutting it into smaller tiles that slightly overlap, running all the tiles through img2img to refine details with respect to the prompt, and \"stitching\" the tiles back together into a cohesive image. It automatically computes how many tiles are needed, and so it can be fed ANY size init image and perform Img2Img on it (though it will be run only one tile at a time, which can cause problems, see the Note at the end). If you're familiar with \"GoBig\" (ala progrock-stable ) it's similar to that, except it can work up to an arbitrarily large size (instead of just 2x), with tile overlaps configurable as a ratio, and has extra logic to re-run any number of the tile sub-sections of the image if for example a small part of a huge run got messed up.","title":"Embiggen"},{"location":"features/EMBIGGEN/#usage","text":"-embiggen <scaling_factor> <esrgan_strength> <overlap_ratio OR overlap_pixels> Takes a scaling factor relative to the size of the --init_img ( -I ), followed by ESRGAN upscaling strength (0 - 1.0), followed by minimum amount of overlap between tiles as a decimal ratio (0 - 1.0) OR a number of pixels. The scaling factor is how much larger than the --init_img the output should be, and will multiply both x and y axis, so an image that is a scaling factor of 3.0 has 3*3= 9 times as many pixels, and will take (at least) 9 times as long (see overlap for why it might be longer). If the --init_img is already the right size -embiggen 1 , and it can also be less than one if the init_img is too big. Esrgan_strength defaults to 0.75, and the overlap_ratio defaults to 0.25, both are optional. Unlike Img2Img, the --width ( -W ) and --height ( -H ) arguments do not control the size of the image as a whole, but the size of the tiles used to Embiggen the image. ESRGAN is used to upscale the --init_img prior to cutting it into tiles/pieces to run through img2img and then stitch back together. Embiggen can be run without ESRGAN; just set the strength to zero (e.g. -embiggen 1.75 0 ). The output of Embiggen can also be upscaled after it's finished ( -U ). The overlap is the minimum that tiles will overlap with adjacent tiles, specified as either a ratio or a number of pixels. How much the tiles overlap determines the likelihood the tiling will be noticable, really small overlaps (e.g. a couple of pixels) may produce noticeable grid-like fuzzy distortions in the final stitched image. Though, as the overlapping space doesn't contribute to making the image bigger, and the larger the overlap the more tiles (and the more time) it will take to finish. Because the overlapping parts of tiles don't \"contribute\" to increasing size, every tile after the first in a row or column effectively only covers an extra 1 - overlap_ratio on each axis. If the input/ --init_img is same size as a tile, the ideal (for time) scaling factors with the default overlap (0.25) are 1.75, 2.5, 3.25, 4.0, etc. -embiggen_tiles <spaced list of tiles> An advanced usage useful if you only want to alter parts of the image while running Embiggen. It takes a list of tiles by number to run and replace onto the initial image e.g. 1 3 5 . It's useful for either fixing problem spots from a previous Embiggen run, or selectively altering the prompt for sections of an image - for creative or coherency reasons. Tiles are numbered starting with one, and left-to-right, top-to-bottom. So, if you are generating a 3x3 tiled image, the middle row would be 4 5 6 . -embiggen_strength <strength> Another advanced option if you want to experiment with the strength parameter that embiggen uses when it calls Img2Img. Values range from 0.0 to 1.0 and lower values preserve more of the character of the initial image. Values that are too high will result in a completely different end image, while values that are too low will result in an image not dissimilar to one you would get with ESRGAN upscaling alone. The default value is 0.4.","title":"Usage"},{"location":"features/EMBIGGEN/#examples","text":"Running Embiggen with 512x512 tiles on an existing image, scaling up by a factor of 2.5x; and doing the same again (default ESRGAN strength is 0.75, default overlap between tiles is 0.25): invoke > a photo of a forest at sunset -s 100 -W 512 -H 512 -I outputs/forest.png -f 0 .4 -embiggen 2 .5 invoke > a photo of a forest at sunset -s 100 -W 512 -H 512 -I outputs/forest.png -f 0 .4 -embiggen 2 .5 0 .75 0 .25 If your starting image was also 512x512 this should have taken 9 tiles. If there weren't enough clouds in the sky of that forest you just made (and that image is about 1280 pixels (512*2.5) wide A.K.A. three 512x512 tiles with 0.25 overlaps wide) we can replace that top row of tiles: invoke> a photo of puffy clouds over a forest at sunset -s 100 -W 512 -H 512 -I outputs/000002.seed.png -f 0 .5 -embiggen_tiles 1 2 3","title":"Examples"},{"location":"features/EMBIGGEN/#fixing-previously-generated-images","text":"It is easy to apply embiggen to any previously-generated file without having to look up the original prompt and provide an initial image. Just use the syntax !fix path/to/file.png <embiggen> . For example, you can rewrite the previous command to look like this: invoke> !fix ./outputs/000002.seed.png -embiggen_tiles 1 2 3 A new file named 000002.seed.fixed.png will be created in the output directory. Note that the !fix command does not replace the original file, unlike the behavior at generate time. You do not need to provide the prompt, and !fix automatically selects a good strength for embiggen-ing. Note Because the same prompt is used on all the tiled images, and the model doesn't have the context of anything outside the tile being run - it can end up creating repeated pattern (also called 'motifs') across all the tiles based on that prompt. The best way to combat this is lowering the --strength ( -f ) to stay more true to the init image, and increasing the number of steps so there is more compute-time to create the detail. Anecdotally --strength 0.35-0.45 works pretty well on most things. It may also work great in some examples even with the --strength set high for patterns, landscapes, or subjects that are more abstract. Because this is (relatively) fast, you can also preserve the best parts from each. Author: Travco","title":"Fixing Previously-Generated Images"},{"location":"features/IMG2IMG/","text":"Image-to-Image # Both the Web and command-line interfaces provide an \"img2img\" feature that lets you seed your creations with an initial drawing or photo. This is a really cool feature that tells stable diffusion to build the prompt on top of the image you provide, preserving the original's basic shape and layout. See the WebUI Guide for a walkthrough of the img2img feature in the InvokeAI web server. This document describes how to use img2img in the command-line tool. Basic Usage # Launch the command-line client by launching invoke.sh / invoke.bat and choosing option (1). Alternative, activate the InvokeAI environment and issue the command invokeai . Once the invoke> prompt appears, you can start an img2img render by pointing to a seed file with the -I option as shown here: tree on a hill with a river, nature photograph, national geographic -I./test-pictures/tree-and-river-sketch.png -f 0.85 original image generated image The --init_img ( -I ) option gives the path to the seed picture. --strength ( -f ) controls how much the original will be modified, ranging from 0.0 (keep the original intact), to 1.0 (ignore the original completely). The default is 0.75 , and ranges from 0.25-0.90 give interesting results. Other relevant options include -C (classification free guidance scale), and -s (steps). Unlike txt2img , adding steps will continuously change the resulting image and it will not converge. You may also pass a -v<variation_amount> option to generate -n<iterations> count variants on the original image. This is done by passing the first generated image back into img2img the requested number of times. It generates interesting variants. Note that the prompt makes a big difference. For example, this slight variation on the prompt produces a very different image: photograph of a tree on a hill with a river Tip When designing prompts, think about how the images scraped from the internet were captioned. Very few photographs will be labeled \"photograph\" or \"photorealistic.\" They will, however, be captioned with the publication, photographer, camera model, or film settings. If the initial image contains transparent regions, then Stable Diffusion will only draw within the transparent regions, a process called inpainting . However, for this to work correctly, the color information underneath the transparent needs to be preserved, not erased. IMPORTANT ISSUE img2img does not work properly on initial images smaller than 512x512. Please scale your image to at least 512x512 before using it. Larger images are not a problem, but may run out of VRAM on your GPU card. To fix this, use the --fit option, which downscales the initial image to fit within the box specified by width x height: tree on a hill with a river, national geographic -I./test-pictures/big-sketch.png -H512 -W512 --fit How does it actually work, though? # The main difference between img2img and prompt2img is the starting point. While prompt2img always starts with pure gaussian noise and progressively refines it over the requested number of steps, img2img skips some of these earlier steps (how many it skips is indirectly controlled by the --strength parameter), and uses instead your initial image mixed with gaussian noise as the starting image. Let's start by thinking about vanilla prompt2img , just generating an image from a prompt. If the step count is 10, then the \"latent space\" (Stable Diffusion's internal representation of the image) for the prompt \"fire\" with seed 1592514025 develops something like this: invoke> \"fire\" -s10 -W384 -H384 -S1592514025 Put simply: starting from a frame of fuzz/static, SD finds details in each frame that it thinks look like \"fire\" and brings them a little bit more into focus, gradually scrubbing out the fuzz until a clear image remains. When you use img2img some of the earlier steps are cut, and instead an initial image of your choice is used. But because of how the maths behind Stable Diffusion works, this image needs to be mixed with just the right amount of noise (fuzz/static) for where it is being inserted. This is where the strength parameter comes in. Depending on the set strength, your image will be inserted into the sequence at the appropriate point, with just the right amount of noise. A concrete example # I want SD to draw a fire based on this hand-drawn image Let's only do 10 steps, to make it easier to see what's happening. If strength is 0.7 , this is what the internal steps the algorithm has to take will look like: With strength 0.4 , the steps look more like this: Notice how much more fuzzy the starting image is for strength 0.7 compared to 0.4 , and notice also how much longer the sequence is with 0.7 : strength = 0.7 strength = 0.4 initial image that SD sees steps argument to invoke> -S10 -S10 steps actually taken 7 4 latent space at each step output Both of the outputs look kind of like what I was thinking of. With the strength higher, my input becomes more vague, and Stable Diffusion has more steps to refine its output. But it's not really making what I want, which is a picture of cheery open fire. With the strength lower, my input is more clear, but Stable Diffusion has less chance to refine itself, so the result ends up inheriting all the problems of my bad drawing. If you want to try this out yourself, all of these are using a seed of 1592514025 with a width/height of 384 , step count 10 , the default sampler ( k_lms ), and the single-word prompt \"fire\" : invoke> \"fire\" -s10 -W384 -H384 -S1592514025 -I /tmp/fire-drawing.png --strength 0 .7 The code for rendering intermediates is on my (damian0815's) branch document-img2img - run invoke.py and check your outputs/img-samples/intermediates folder while generating an image. Compensating for the reduced step count # After putting this guide together I was curious to see how the difference would be if I increased the step count to compensate, so that SD could have the same amount of steps to develop the image regardless of the strength. So I ran the generation again using the same seed, but this time adapting the step count to give each generation 20 steps. Here's strength 0.4 (note step count 50 , which is 20 \u00f7 0.4 to make sure SD does 20 steps from my image): invoke> \"fire\" -s50 -W384 -H384 -S1592514025 -I /tmp/fire-drawing.png -f 0 .4 and here is strength 0.7 (note step count 30 , which is roughly 20 \u00f7 0.7 to make sure SD does 20 steps from my image): invoke> \"fire\" -s30 -W384 -H384 -S1592514025 -I /tmp/fire-drawing.png -f 0.7 In both cases the image is nice and clean and \"finished\", but because at strength 0.7 Stable Diffusion has been give so much more freedom to improve on my badly-drawn flames, they've come out looking much better. You can really see the difference when looking at the latent steps. There's more noise on the first image with strength 0.7 : than there is for strength 0.4 : and that extra noise gives the algorithm more choices when it is evaluating how to denoise any particular pixel in the image. Unfortunately, it seems that img2img is very sensitive to the step count. Here's strength 0.7 with a step count of 29 (SD did 19 steps from my image): By comparing the latents we can sort of see that something got interpreted differently enough on the third or fourth step to lead to a rather different interpretation of the flames. This is the result of a difference in the de-noising \"schedule\" - basically the noise has to be cleaned by a certain degree each step or the model won't \"converge\" on the image properly (see stable diffusion blog for more about that). A different step count means a different schedule, which means things get interpreted slightly differently at every step.","title":"Image-to-Image"},{"location":"features/IMG2IMG/#image-to-image","text":"Both the Web and command-line interfaces provide an \"img2img\" feature that lets you seed your creations with an initial drawing or photo. This is a really cool feature that tells stable diffusion to build the prompt on top of the image you provide, preserving the original's basic shape and layout. See the WebUI Guide for a walkthrough of the img2img feature in the InvokeAI web server. This document describes how to use img2img in the command-line tool.","title":" Image-to-Image"},{"location":"features/IMG2IMG/#basic-usage","text":"Launch the command-line client by launching invoke.sh / invoke.bat and choosing option (1). Alternative, activate the InvokeAI environment and issue the command invokeai . Once the invoke> prompt appears, you can start an img2img render by pointing to a seed file with the -I option as shown here: tree on a hill with a river, nature photograph, national geographic -I./test-pictures/tree-and-river-sketch.png -f 0.85 original image generated image The --init_img ( -I ) option gives the path to the seed picture. --strength ( -f ) controls how much the original will be modified, ranging from 0.0 (keep the original intact), to 1.0 (ignore the original completely). The default is 0.75 , and ranges from 0.25-0.90 give interesting results. Other relevant options include -C (classification free guidance scale), and -s (steps). Unlike txt2img , adding steps will continuously change the resulting image and it will not converge. You may also pass a -v<variation_amount> option to generate -n<iterations> count variants on the original image. This is done by passing the first generated image back into img2img the requested number of times. It generates interesting variants. Note that the prompt makes a big difference. For example, this slight variation on the prompt produces a very different image: photograph of a tree on a hill with a river Tip When designing prompts, think about how the images scraped from the internet were captioned. Very few photographs will be labeled \"photograph\" or \"photorealistic.\" They will, however, be captioned with the publication, photographer, camera model, or film settings. If the initial image contains transparent regions, then Stable Diffusion will only draw within the transparent regions, a process called inpainting . However, for this to work correctly, the color information underneath the transparent needs to be preserved, not erased. IMPORTANT ISSUE img2img does not work properly on initial images smaller than 512x512. Please scale your image to at least 512x512 before using it. Larger images are not a problem, but may run out of VRAM on your GPU card. To fix this, use the --fit option, which downscales the initial image to fit within the box specified by width x height: tree on a hill with a river, national geographic -I./test-pictures/big-sketch.png -H512 -W512 --fit","title":"Basic Usage"},{"location":"features/IMG2IMG/#how-does-it-actually-work-though","text":"The main difference between img2img and prompt2img is the starting point. While prompt2img always starts with pure gaussian noise and progressively refines it over the requested number of steps, img2img skips some of these earlier steps (how many it skips is indirectly controlled by the --strength parameter), and uses instead your initial image mixed with gaussian noise as the starting image. Let's start by thinking about vanilla prompt2img , just generating an image from a prompt. If the step count is 10, then the \"latent space\" (Stable Diffusion's internal representation of the image) for the prompt \"fire\" with seed 1592514025 develops something like this: invoke> \"fire\" -s10 -W384 -H384 -S1592514025 Put simply: starting from a frame of fuzz/static, SD finds details in each frame that it thinks look like \"fire\" and brings them a little bit more into focus, gradually scrubbing out the fuzz until a clear image remains. When you use img2img some of the earlier steps are cut, and instead an initial image of your choice is used. But because of how the maths behind Stable Diffusion works, this image needs to be mixed with just the right amount of noise (fuzz/static) for where it is being inserted. This is where the strength parameter comes in. Depending on the set strength, your image will be inserted into the sequence at the appropriate point, with just the right amount of noise.","title":"How does it actually work, though?"},{"location":"features/IMG2IMG/#a-concrete-example","text":"I want SD to draw a fire based on this hand-drawn image Let's only do 10 steps, to make it easier to see what's happening. If strength is 0.7 , this is what the internal steps the algorithm has to take will look like: With strength 0.4 , the steps look more like this: Notice how much more fuzzy the starting image is for strength 0.7 compared to 0.4 , and notice also how much longer the sequence is with 0.7 : strength = 0.7 strength = 0.4 initial image that SD sees steps argument to invoke> -S10 -S10 steps actually taken 7 4 latent space at each step output Both of the outputs look kind of like what I was thinking of. With the strength higher, my input becomes more vague, and Stable Diffusion has more steps to refine its output. But it's not really making what I want, which is a picture of cheery open fire. With the strength lower, my input is more clear, but Stable Diffusion has less chance to refine itself, so the result ends up inheriting all the problems of my bad drawing. If you want to try this out yourself, all of these are using a seed of 1592514025 with a width/height of 384 , step count 10 , the default sampler ( k_lms ), and the single-word prompt \"fire\" : invoke> \"fire\" -s10 -W384 -H384 -S1592514025 -I /tmp/fire-drawing.png --strength 0 .7 The code for rendering intermediates is on my (damian0815's) branch document-img2img - run invoke.py and check your outputs/img-samples/intermediates folder while generating an image.","title":"A concrete example"},{"location":"features/IMG2IMG/#compensating-for-the-reduced-step-count","text":"After putting this guide together I was curious to see how the difference would be if I increased the step count to compensate, so that SD could have the same amount of steps to develop the image regardless of the strength. So I ran the generation again using the same seed, but this time adapting the step count to give each generation 20 steps. Here's strength 0.4 (note step count 50 , which is 20 \u00f7 0.4 to make sure SD does 20 steps from my image): invoke> \"fire\" -s50 -W384 -H384 -S1592514025 -I /tmp/fire-drawing.png -f 0 .4 and here is strength 0.7 (note step count 30 , which is roughly 20 \u00f7 0.7 to make sure SD does 20 steps from my image): invoke> \"fire\" -s30 -W384 -H384 -S1592514025 -I /tmp/fire-drawing.png -f 0.7 In both cases the image is nice and clean and \"finished\", but because at strength 0.7 Stable Diffusion has been give so much more freedom to improve on my badly-drawn flames, they've come out looking much better. You can really see the difference when looking at the latent steps. There's more noise on the first image with strength 0.7 : than there is for strength 0.4 : and that extra noise gives the algorithm more choices when it is evaluating how to denoise any particular pixel in the image. Unfortunately, it seems that img2img is very sensitive to the step count. Here's strength 0.7 with a step count of 29 (SD did 19 steps from my image): By comparing the latents we can sort of see that something got interpreted differently enough on the third or fourth step to lead to a rather different interpretation of the flames. This is the result of a difference in the de-noising \"schedule\" - basically the noise has to be cleaned by a certain degree each step or the model won't \"converge\" on the image properly (see stable diffusion blog for more about that). A different step count means a different schedule, which means things get interpreted slightly differently at every step.","title":"Compensating for the reduced step count"},{"location":"features/INPAINTING/","text":"Inpainting # Creating Transparent Regions for Inpainting # Inpainting is really cool. To do it, you start with an initial image and use a photoeditor to make one or more regions transparent (i.e. they have a \"hole\" in them). You then provide the path to this image at the dream> command line using the -I switch. Stable Diffusion will only paint within the transparent region. There's a catch. In the current implementation, you have to prepare the initial image correctly so that the underlying colors are preserved under the transparent area. Many imaging editing applications will by default erase the color information under the transparent pixels and replace them with white or black, which will lead to suboptimal inpainting. It often helps to apply incomplete transparency, such as any value between 1 and 99% You also must take care to export the PNG file in such a way that the color information is preserved. There is often an option in the export dialog that lets you specify this. If your photoeditor is erasing the underlying color information, dream.py will give you a big fat warning. If you can't find a way to coax your photoeditor to retain color values under transparent areas, then you can combine the -I and -M switches to provide both the original unedited image and the masked (partially transparent) image: invoke> \"man with cat on shoulder\" -I./images/man.png -M./images/man-transparent.png Masking using Text # You can also create a mask using a text prompt to select the part of the image you want to alter, using the clipseg algorithm. This works on any image, not just ones generated by InvokeAI. The --text_mask (short form -tm ) option takes two arguments. The first argument is a text description of the part of the image you wish to mask (paint over). If the text description contains a space, you must surround it with quotation marks. The optional second argument is the minimum threshold for the mask classifier's confidence score, described in more detail below. To see how this works in practice, here's an image of a still life painting that I got off the web. You can selectively mask out the orange and replace it with a baseball in this way: invoke> a baseball -I /path/to/still_life.png -tm orange The clipseg classifier produces a confidence score for each region it identifies. Generally regions that score above 0.5 are reliable, but if you are getting too much or too little masking you can adjust the threshold down (to get more mask), or up (to get less). In this example, by passing -tm a higher value, we are insisting on a tigher mask. However, if you make it too high, the orange may not be picked up at all! invoke> a baseball -I /path/to/breakfast.png -tm orange 0 .6 The !mask command may be useful for debugging problems with the text2mask feature. The syntax is !mask /path/to/image.png -tm <text> <threshold> It will generate three files: The image with the selected area highlighted. it will be named XXXXX. . .selected.png The image with the un-selected area highlighted. it will be named XXXXX. . .deselected.png The image with the selected area converted into a black and white image according to the threshold level it will be named XXXXX. . .masked.png The .masked.png file can then be directly passed to the invoke> prompt in the CLI via the -M argument. Do not attempt this with the selected.png or deselected.png files, as they contain some transparency throughout the image and will not produce the desired results. Here is an example of how !mask works: invoke> !mask ./test-pictures/curly.png -tm hair 0 .5 >> generating masks from ./test-pictures/curly.png >> Initializing clipseg model for text to mask inference Outputs: [ 941 .1 ] outputs/img-samples/000019.curly.hair.deselected.png: !mask ./test-pictures/curly.png -tm hair 0 .5 [ 941 .2 ] outputs/img-samples/000019.curly.hair.selected.png: !mask ./test-pictures/curly.png -tm hair 0 .5 [ 941 .3 ] outputs/img-samples/000019.curly.hair.masked.png: !mask ./test-pictures/curly.png -tm hair 0 .5 Original image \"curly.png\" 000019.curly.hair.selected.png 000019.curly.hair.deselected.png 000019.curly.hair.masked.png It looks like we selected the hair pretty well at the 0.5 threshold (which is the default, so we didn't actually have to specify it), so let's have some fun: invoke> medusa with cobras -I ./test-pictures/curly.png -M 000019 .curly.hair.masked.png -C20 >> loaded input image of size 512x512 from ./test-pictures/curly.png ... Outputs: [ 946 ] outputs/img-samples/000024.801380492.png: \"medusa with cobras\" -s 50 -S 801380492 -W 512 -H 512 -C 20 .0 -I ./test-pictures/curly.png -A k_lms -f 0 .75 You can also skip the !mask creation step and just select the masked region directly: invoke> medusa with cobras -I ./test-pictures/curly.png -tm hair -C20 Using the RunwayML inpainting model # The RunwayML Inpainting Model v1.5 is a specialized version of Stable Diffusion v1.5 that contains extra channels specifically designed to enhance inpainting and outpainting. While it can do regular txt2img and img2img , it really shines when filling in missing regions. It has an almost uncanny ability to blend the new regions with existing ones in a semantically coherent way. To install the inpainting model, follow the instructions for installing a new model. You may use either the CLI ( invoke.py script) or directly edit the configs/models.yaml configuration file to do this. The main thing to watch out for is that the the model config option must be set up to use v1-inpainting-inference.yaml rather than the v1-inference.yaml file that is used by Stable Diffusion 1.4 and 1.5. After installation, your models.yaml should contain an entry that looks like this one: inpainting-1.5: weights: models/ldm/stable-diffusion-v1/sd-v1-5-inpainting.ckpt description: SD inpainting v1.5 config: configs/stable-diffusion/v1-inpainting-inference.yaml vae: models/ldm/stable-diffusion-v1/vae-ft-mse-840000-ema-pruned.ckpt width: 512 height: 512 As shown in the example, you may include a VAE fine-tuning weights file as well. This is strongly recommended. To use the custom inpainting model, launch invoke.py with the argument --model inpainting-1.5 or alternatively from within the script use the !switch inpainting-1.5 command to load and switch to the inpainting model. You can now do inpainting and outpainting exactly as described above, but there will (likely) be a noticeable improvement in coherence. Txt2img and Img2img will work as well. There are a few caveats to be aware of: The inpainting model is larger than the standard model, and will use nearly 4 GB of GPU VRAM. This makes it unlikely to run on a 4 GB graphics card. When operating in Img2img mode, the inpainting model is much less steerable than the standard model. It is great for making small changes, such as changing the pattern of a fabric, or slightly changing a subject's expression or hair, but the model will resist making the dramatic alterations that the standard model lets you do. While the --hires option works fine with the inpainting model, some special features, such as --embiggen are disabled. Prompt weighting ( banana++ sushi ) and merging work well with the inpainting model, but prompt swapping ( a (\"fluffy cat\").swap(\"smiling dog\") eating a hotdog ) will not have any effect due to the way the model is set up. You may use text masking (with -tm thing-to-mask ) as an effective replacement. The model tends to oversharpen image if you use high step or CFG values. If you need to do large steps, use the standard model. The --strength ( -f ) option has no effect on the inpainting model due to its fundamental differences with the standard model. It will always take the full number of steps you specify. Troubleshooting # Here are some troubleshooting tips for inpainting and outpainting. Inpainting is not changing the masked region enough! # One of the things to understand about how inpainting works is that it is equivalent to running img2img on just the masked (transparent) area. img2img builds on top of the existing image data, and therefore will attempt to preserve colors, shapes and textures to the best of its ability. Unfortunately this means that if you want to make a dramatic change in the inpainted region, for example replacing a red wall with a blue one, the algorithm will fight you. You have a couple of options. The first is to increase the values of the requested steps ( -sXXX ), strength ( -f0.XX ), and/or condition-free guidance ( -CXX.X ). If this is not working for you, a more extreme step is to provide the --inpaint_replace 0.X ( -r0.X ) option. This value ranges from 0.0 to 1.0. The higher it is the less attention the algorithm will pay to the data underneath the masked region. At high values this will enable you to replace colored regions entirely, but beware that the masked region mayl not blend in with the surrounding unmasked regions as well. Recipe for GIMP # GIMP is a popular Linux photoediting tool. Open image in GIMP. Layer->Transparency->Add Alpha Channel Use lasso tool to select region to mask Choose Select -> Float to create a floating selection Open the Layers toolbar (^L) and select \"Floating Selection\" Set opacity to a value between 0% and 99% Export as PNG In the export dialogue, Make sure the \"Save colour values from transparent pixels\" checkbox is selected. Recipe for Adobe Photoshop # Open image in Photoshop Use any of the selection tools (Marquee, Lasso, or Wand) to select the area you desire to inpaint. Because we'll be applying a mask over the area we want to preserve, you should now select the inverse by using the Shift + Ctrl + I shortcut, or right clicking and using the \"Select Inverse\" option. You'll now create a mask by selecting the image layer, and Masking the selection. Make sure that you don't delete any of the underlying image, or your inpainting results will be dramatically impacted. Make sure to hide any background layers that are present. You should see the mask applied to your image layer, and the image on your canvas should display the checkered background. Save the image as a transparent PNG by using File \u2192 Save a Copy from the menu bar, or by using the keyboard shortcut Alt + Ctrl + S After following the inpainting instructions above (either through the CLI or the Web UI), marvel at your newfound ability to selectively invoke. Lookin' good! In the export dialogue, Make sure the \"Save colour values from transparent pixels\" checkbox is selected.","title":"Inpainting"},{"location":"features/INPAINTING/#inpainting","text":"","title":" Inpainting"},{"location":"features/INPAINTING/#creating-transparent-regions-for-inpainting","text":"Inpainting is really cool. To do it, you start with an initial image and use a photoeditor to make one or more regions transparent (i.e. they have a \"hole\" in them). You then provide the path to this image at the dream> command line using the -I switch. Stable Diffusion will only paint within the transparent region. There's a catch. In the current implementation, you have to prepare the initial image correctly so that the underlying colors are preserved under the transparent area. Many imaging editing applications will by default erase the color information under the transparent pixels and replace them with white or black, which will lead to suboptimal inpainting. It often helps to apply incomplete transparency, such as any value between 1 and 99% You also must take care to export the PNG file in such a way that the color information is preserved. There is often an option in the export dialog that lets you specify this. If your photoeditor is erasing the underlying color information, dream.py will give you a big fat warning. If you can't find a way to coax your photoeditor to retain color values under transparent areas, then you can combine the -I and -M switches to provide both the original unedited image and the masked (partially transparent) image: invoke> \"man with cat on shoulder\" -I./images/man.png -M./images/man-transparent.png","title":"Creating Transparent Regions for Inpainting"},{"location":"features/INPAINTING/#masking-using-text","text":"You can also create a mask using a text prompt to select the part of the image you want to alter, using the clipseg algorithm. This works on any image, not just ones generated by InvokeAI. The --text_mask (short form -tm ) option takes two arguments. The first argument is a text description of the part of the image you wish to mask (paint over). If the text description contains a space, you must surround it with quotation marks. The optional second argument is the minimum threshold for the mask classifier's confidence score, described in more detail below. To see how this works in practice, here's an image of a still life painting that I got off the web. You can selectively mask out the orange and replace it with a baseball in this way: invoke> a baseball -I /path/to/still_life.png -tm orange The clipseg classifier produces a confidence score for each region it identifies. Generally regions that score above 0.5 are reliable, but if you are getting too much or too little masking you can adjust the threshold down (to get more mask), or up (to get less). In this example, by passing -tm a higher value, we are insisting on a tigher mask. However, if you make it too high, the orange may not be picked up at all! invoke> a baseball -I /path/to/breakfast.png -tm orange 0 .6 The !mask command may be useful for debugging problems with the text2mask feature. The syntax is !mask /path/to/image.png -tm <text> <threshold> It will generate three files: The image with the selected area highlighted. it will be named XXXXX. . .selected.png The image with the un-selected area highlighted. it will be named XXXXX. . .deselected.png The image with the selected area converted into a black and white image according to the threshold level it will be named XXXXX. . .masked.png The .masked.png file can then be directly passed to the invoke> prompt in the CLI via the -M argument. Do not attempt this with the selected.png or deselected.png files, as they contain some transparency throughout the image and will not produce the desired results. Here is an example of how !mask works: invoke> !mask ./test-pictures/curly.png -tm hair 0 .5 >> generating masks from ./test-pictures/curly.png >> Initializing clipseg model for text to mask inference Outputs: [ 941 .1 ] outputs/img-samples/000019.curly.hair.deselected.png: !mask ./test-pictures/curly.png -tm hair 0 .5 [ 941 .2 ] outputs/img-samples/000019.curly.hair.selected.png: !mask ./test-pictures/curly.png -tm hair 0 .5 [ 941 .3 ] outputs/img-samples/000019.curly.hair.masked.png: !mask ./test-pictures/curly.png -tm hair 0 .5 Original image \"curly.png\" 000019.curly.hair.selected.png 000019.curly.hair.deselected.png 000019.curly.hair.masked.png It looks like we selected the hair pretty well at the 0.5 threshold (which is the default, so we didn't actually have to specify it), so let's have some fun: invoke> medusa with cobras -I ./test-pictures/curly.png -M 000019 .curly.hair.masked.png -C20 >> loaded input image of size 512x512 from ./test-pictures/curly.png ... Outputs: [ 946 ] outputs/img-samples/000024.801380492.png: \"medusa with cobras\" -s 50 -S 801380492 -W 512 -H 512 -C 20 .0 -I ./test-pictures/curly.png -A k_lms -f 0 .75 You can also skip the !mask creation step and just select the masked region directly: invoke> medusa with cobras -I ./test-pictures/curly.png -tm hair -C20","title":"Masking using Text"},{"location":"features/INPAINTING/#using-the-runwayml-inpainting-model","text":"The RunwayML Inpainting Model v1.5 is a specialized version of Stable Diffusion v1.5 that contains extra channels specifically designed to enhance inpainting and outpainting. While it can do regular txt2img and img2img , it really shines when filling in missing regions. It has an almost uncanny ability to blend the new regions with existing ones in a semantically coherent way. To install the inpainting model, follow the instructions for installing a new model. You may use either the CLI ( invoke.py script) or directly edit the configs/models.yaml configuration file to do this. The main thing to watch out for is that the the model config option must be set up to use v1-inpainting-inference.yaml rather than the v1-inference.yaml file that is used by Stable Diffusion 1.4 and 1.5. After installation, your models.yaml should contain an entry that looks like this one: inpainting-1.5: weights: models/ldm/stable-diffusion-v1/sd-v1-5-inpainting.ckpt description: SD inpainting v1.5 config: configs/stable-diffusion/v1-inpainting-inference.yaml vae: models/ldm/stable-diffusion-v1/vae-ft-mse-840000-ema-pruned.ckpt width: 512 height: 512 As shown in the example, you may include a VAE fine-tuning weights file as well. This is strongly recommended. To use the custom inpainting model, launch invoke.py with the argument --model inpainting-1.5 or alternatively from within the script use the !switch inpainting-1.5 command to load and switch to the inpainting model. You can now do inpainting and outpainting exactly as described above, but there will (likely) be a noticeable improvement in coherence. Txt2img and Img2img will work as well. There are a few caveats to be aware of: The inpainting model is larger than the standard model, and will use nearly 4 GB of GPU VRAM. This makes it unlikely to run on a 4 GB graphics card. When operating in Img2img mode, the inpainting model is much less steerable than the standard model. It is great for making small changes, such as changing the pattern of a fabric, or slightly changing a subject's expression or hair, but the model will resist making the dramatic alterations that the standard model lets you do. While the --hires option works fine with the inpainting model, some special features, such as --embiggen are disabled. Prompt weighting ( banana++ sushi ) and merging work well with the inpainting model, but prompt swapping ( a (\"fluffy cat\").swap(\"smiling dog\") eating a hotdog ) will not have any effect due to the way the model is set up. You may use text masking (with -tm thing-to-mask ) as an effective replacement. The model tends to oversharpen image if you use high step or CFG values. If you need to do large steps, use the standard model. The --strength ( -f ) option has no effect on the inpainting model due to its fundamental differences with the standard model. It will always take the full number of steps you specify.","title":"Using the RunwayML inpainting model"},{"location":"features/INPAINTING/#troubleshooting","text":"Here are some troubleshooting tips for inpainting and outpainting.","title":"Troubleshooting"},{"location":"features/INPAINTING/#inpainting-is-not-changing-the-masked-region-enough","text":"One of the things to understand about how inpainting works is that it is equivalent to running img2img on just the masked (transparent) area. img2img builds on top of the existing image data, and therefore will attempt to preserve colors, shapes and textures to the best of its ability. Unfortunately this means that if you want to make a dramatic change in the inpainted region, for example replacing a red wall with a blue one, the algorithm will fight you. You have a couple of options. The first is to increase the values of the requested steps ( -sXXX ), strength ( -f0.XX ), and/or condition-free guidance ( -CXX.X ). If this is not working for you, a more extreme step is to provide the --inpaint_replace 0.X ( -r0.X ) option. This value ranges from 0.0 to 1.0. The higher it is the less attention the algorithm will pay to the data underneath the masked region. At high values this will enable you to replace colored regions entirely, but beware that the masked region mayl not blend in with the surrounding unmasked regions as well.","title":"Inpainting is not changing the masked region enough!"},{"location":"features/INPAINTING/#recipe-for-gimp","text":"GIMP is a popular Linux photoediting tool. Open image in GIMP. Layer->Transparency->Add Alpha Channel Use lasso tool to select region to mask Choose Select -> Float to create a floating selection Open the Layers toolbar (^L) and select \"Floating Selection\" Set opacity to a value between 0% and 99% Export as PNG In the export dialogue, Make sure the \"Save colour values from transparent pixels\" checkbox is selected.","title":"Recipe for GIMP"},{"location":"features/INPAINTING/#recipe-for-adobe-photoshop","text":"Open image in Photoshop Use any of the selection tools (Marquee, Lasso, or Wand) to select the area you desire to inpaint. Because we'll be applying a mask over the area we want to preserve, you should now select the inverse by using the Shift + Ctrl + I shortcut, or right clicking and using the \"Select Inverse\" option. You'll now create a mask by selecting the image layer, and Masking the selection. Make sure that you don't delete any of the underlying image, or your inpainting results will be dramatically impacted. Make sure to hide any background layers that are present. You should see the mask applied to your image layer, and the image on your canvas should display the checkered background. Save the image as a transparent PNG by using File \u2192 Save a Copy from the menu bar, or by using the keyboard shortcut Alt + Ctrl + S After following the inpainting instructions above (either through the CLI or the Web UI), marvel at your newfound ability to selectively invoke. Lookin' good! In the export dialogue, Make sure the \"Save colour values from transparent pixels\" checkbox is selected.","title":"Recipe for Adobe Photoshop"},{"location":"features/MODEL_MERGING/","text":"Model Merging # How to Merge Models # As of version 2.3, InvokeAI comes with a script that allows you to merge two or three diffusers-type models into a new merged model. The resulting model will combine characteristics of the original, and can be used to teach an old model new tricks. You may run the merge script by starting the invoke launcher ( invoke.sh or invoke.bat ) and choosing the option for merge models . This will launch a text-based interactive user interface that prompts you to select the models to merge, how to merge them, and the merged model name. Alternatively you may activate InvokeAI's virtual environment from the command line, and call the script via merge_models --gui to open up a version that has a nice graphical front end. To get the commandline- only version, omit --gui . The user interface for the text-based interactive script is straightforward. It shows you a series of setting fields. Use control-N (^N) to move to the next field, and control-P (^P) to move to the previous one. You can also use TAB and shift-TAB to move forward and backward. Once you are in a multiple choice field, use the up and down cursor arrows to move to your desired selection, and press or to select it. Change text fields by typing in them, and adjust scrollbars using the left and right arrow keys. Once you are happy with your settings, press the OK button. Note that there may be two pages of settings, depending on the height of your screen, and the OK button may be on the second page. Advance past the last field of the first page to get to the second page, and reverse this to get back. If the merge runs successfully, it will create a new diffusers model under the selected name and register it with InvokeAI. The Settings # Model Selection -- there are three multiple choice fields that display all the diffusers-style models that InvokeAI knows about. If you do not see the model you are looking for, then it is probably a legacy checkpoint model and needs to be converted using the invoke command-line client and its !optimize command. You must select at least two models to merge. The third can be left at \"None\" if you desire. Alpha -- This is the ratio to use when combining models. It ranges from 0 to 1. The higher the value, the more weight is given to the 2d and (optionally) 3d models. So if you have two models named \"A\" and \"B\", an alpha value of 0.25 will give you a merged model that is 25% A and 75% B. Interpolation Method -- This is the method used to combine weights. The options are \"weighted_sum\" (the default), \"sigmoid\", \"inv_sigmoid\" and \"add_difference\". Each produces slightly different results. When three models are in use, only \"add_difference\" is available. (TODO: cite a reference that describes what these interpolation methods actually do and how to decide among them). Force -- Not all models are compatible with each other. The merge script will check for compatibility and refuse to merge ones that are incompatible. Set this checkbox to try merging anyway. Name for merged model - This is the name for the new model. Please use InvokeAI conventions - only alphanumeric letters and the characters \".+-\". Caveats # This is a new script and may contain bugs.","title":"Model Merging"},{"location":"features/MODEL_MERGING/#model-merging","text":"","title":" Model Merging"},{"location":"features/MODEL_MERGING/#how-to-merge-models","text":"As of version 2.3, InvokeAI comes with a script that allows you to merge two or three diffusers-type models into a new merged model. The resulting model will combine characteristics of the original, and can be used to teach an old model new tricks. You may run the merge script by starting the invoke launcher ( invoke.sh or invoke.bat ) and choosing the option for merge models . This will launch a text-based interactive user interface that prompts you to select the models to merge, how to merge them, and the merged model name. Alternatively you may activate InvokeAI's virtual environment from the command line, and call the script via merge_models --gui to open up a version that has a nice graphical front end. To get the commandline- only version, omit --gui . The user interface for the text-based interactive script is straightforward. It shows you a series of setting fields. Use control-N (^N) to move to the next field, and control-P (^P) to move to the previous one. You can also use TAB and shift-TAB to move forward and backward. Once you are in a multiple choice field, use the up and down cursor arrows to move to your desired selection, and press or to select it. Change text fields by typing in them, and adjust scrollbars using the left and right arrow keys. Once you are happy with your settings, press the OK button. Note that there may be two pages of settings, depending on the height of your screen, and the OK button may be on the second page. Advance past the last field of the first page to get to the second page, and reverse this to get back. If the merge runs successfully, it will create a new diffusers model under the selected name and register it with InvokeAI.","title":"How to Merge Models"},{"location":"features/MODEL_MERGING/#the-settings","text":"Model Selection -- there are three multiple choice fields that display all the diffusers-style models that InvokeAI knows about. If you do not see the model you are looking for, then it is probably a legacy checkpoint model and needs to be converted using the invoke command-line client and its !optimize command. You must select at least two models to merge. The third can be left at \"None\" if you desire. Alpha -- This is the ratio to use when combining models. It ranges from 0 to 1. The higher the value, the more weight is given to the 2d and (optionally) 3d models. So if you have two models named \"A\" and \"B\", an alpha value of 0.25 will give you a merged model that is 25% A and 75% B. Interpolation Method -- This is the method used to combine weights. The options are \"weighted_sum\" (the default), \"sigmoid\", \"inv_sigmoid\" and \"add_difference\". Each produces slightly different results. When three models are in use, only \"add_difference\" is available. (TODO: cite a reference that describes what these interpolation methods actually do and how to decide among them). Force -- Not all models are compatible with each other. The merge script will check for compatibility and refuse to merge ones that are incompatible. Set this checkbox to try merging anyway. Name for merged model - This is the name for the new model. Please use InvokeAI conventions - only alphanumeric letters and the characters \".+-\".","title":"The Settings"},{"location":"features/MODEL_MERGING/#caveats","text":"This is a new script and may contain bugs.","title":"Caveats"},{"location":"features/NSFW/","text":"NSFW Checker # The NSFW (\"Safety\") Checker # The Stable Diffusion image generation models will produce sexual imagery if deliberately prompted, and will occasionally produce such images when this is not intended. Such images are colloquially known as \"Not Safe for Work\" (NSFW). This behavior is due to the nature of the training set that Stable Diffusion was trained on, which culled millions of \"aesthetic\" images from the Internet. You may not wish to be exposed to these images, and in some jurisdictions it may be illegal to publicly distribute such imagery, including mounting a publicly-available server that provides unfiltered images to the public. Furthermore, the Stable Diffusion weights License forbids the model from being used to \"exploit any of the vulnerabilities of a specific group of persons.\" For these reasons Stable Diffusion offers a \"safety checker,\" a machine learning model trained to recognize potentially disturbing imagery. When a potentially NSFW image is detected, the checker will blur the image and paste a warning icon on top. The checker can be turned on and off on the command line using --nsfw_checker and --no-nsfw_checker . At installation time, InvokeAI will ask whether the checker should be activated by default (neither argument given on the command line). The response is stored in the InvokeAI initialization file (usually .invokeai in your home directory). You can change the default at any time by opening this file in a text editor and commenting or uncommenting the line --nsfw_checker . Caveats # There are a number of caveats that you need to be aware of. Accuracy # The checker is not perfect .It will occasionally flag innocuous images (false positives), and will frequently miss violent and gory imagery (false negatives). It rarely fails to flag sexual imagery, but this has been known to happen. For these reasons, the InvokeAI team prefers to refer to the software as a \"NSFW Checker\" rather than \"safety checker.\" Memory Usage and Performance # The NSFW checker consumes an additional 1.2G of GPU VRAM on top of the 3.4G of VRAM used by Stable Diffusion v1.5 (this is with half-precision arithmetic). This means that the checker will not run successfully on GPU cards with less than 6GB VRAM, and will reduce the size of the images that you can produce. The checker also introduces a slight performance penalty. Images will take ~1 second longer to generate when the checker is activated. Generally this is not noticeable. Intermediate Images in the Web UI # The checker only operates on the final image produced by the Stable Diffusion algorithm. If you are using the Web UI and have enabled the display of intermediate images, you will briefly be exposed to a low-resolution (mosaicized) version of the final image before it is flagged by the checker and replaced by a fully blurred version. You are encouraged to turn off intermediate image rendering when you are using the checker. Future versions of InvokeAI will apply additional blurring to intermediate images when the checker is active. Watermarking # InvokeAI does not apply any sort of watermark to images it generates. However, it does write metadata into the PNG data area, including the prompt used to generate the image and relevant parameter settings. These fields can be examined using the sd-metadata.py script that comes with the InvokeAI package. Note that several other Stable Diffusion distributions offer wavelet-based \"invisible\" watermarking. We have experimented with the library used to generate these watermarks and have reached the conclusion that while the watermarking library may be adding watermarks to PNG images, the currently available version is unable to retrieve them successfully. If and when a functioning version of the library becomes available, we will offer this feature as well.","title":"The NSFW Checker"},{"location":"features/NSFW/#nsfw-checker","text":"","title":" NSFW Checker"},{"location":"features/NSFW/#the-nsfw-safety-checker","text":"The Stable Diffusion image generation models will produce sexual imagery if deliberately prompted, and will occasionally produce such images when this is not intended. Such images are colloquially known as \"Not Safe for Work\" (NSFW). This behavior is due to the nature of the training set that Stable Diffusion was trained on, which culled millions of \"aesthetic\" images from the Internet. You may not wish to be exposed to these images, and in some jurisdictions it may be illegal to publicly distribute such imagery, including mounting a publicly-available server that provides unfiltered images to the public. Furthermore, the Stable Diffusion weights License forbids the model from being used to \"exploit any of the vulnerabilities of a specific group of persons.\" For these reasons Stable Diffusion offers a \"safety checker,\" a machine learning model trained to recognize potentially disturbing imagery. When a potentially NSFW image is detected, the checker will blur the image and paste a warning icon on top. The checker can be turned on and off on the command line using --nsfw_checker and --no-nsfw_checker . At installation time, InvokeAI will ask whether the checker should be activated by default (neither argument given on the command line). The response is stored in the InvokeAI initialization file (usually .invokeai in your home directory). You can change the default at any time by opening this file in a text editor and commenting or uncommenting the line --nsfw_checker .","title":"The NSFW (\"Safety\") Checker"},{"location":"features/NSFW/#caveats","text":"There are a number of caveats that you need to be aware of.","title":"Caveats"},{"location":"features/NSFW/#accuracy","text":"The checker is not perfect .It will occasionally flag innocuous images (false positives), and will frequently miss violent and gory imagery (false negatives). It rarely fails to flag sexual imagery, but this has been known to happen. For these reasons, the InvokeAI team prefers to refer to the software as a \"NSFW Checker\" rather than \"safety checker.\"","title":"Accuracy"},{"location":"features/NSFW/#memory-usage-and-performance","text":"The NSFW checker consumes an additional 1.2G of GPU VRAM on top of the 3.4G of VRAM used by Stable Diffusion v1.5 (this is with half-precision arithmetic). This means that the checker will not run successfully on GPU cards with less than 6GB VRAM, and will reduce the size of the images that you can produce. The checker also introduces a slight performance penalty. Images will take ~1 second longer to generate when the checker is activated. Generally this is not noticeable.","title":"Memory Usage and Performance"},{"location":"features/NSFW/#intermediate-images-in-the-web-ui","text":"The checker only operates on the final image produced by the Stable Diffusion algorithm. If you are using the Web UI and have enabled the display of intermediate images, you will briefly be exposed to a low-resolution (mosaicized) version of the final image before it is flagged by the checker and replaced by a fully blurred version. You are encouraged to turn off intermediate image rendering when you are using the checker. Future versions of InvokeAI will apply additional blurring to intermediate images when the checker is active.","title":"Intermediate Images in the Web UI"},{"location":"features/NSFW/#watermarking","text":"InvokeAI does not apply any sort of watermark to images it generates. However, it does write metadata into the PNG data area, including the prompt used to generate the image and relevant parameter settings. These fields can be examined using the sd-metadata.py script that comes with the InvokeAI package. Note that several other Stable Diffusion distributions offer wavelet-based \"invisible\" watermarking. We have experimented with the library used to generate these watermarks and have reached the conclusion that while the watermarking library may be adding watermarks to PNG images, the currently available version is unable to retrieve them successfully. If and when a functioning version of the library becomes available, we will offer this feature as well.","title":"Watermarking"},{"location":"features/OTHER/","text":"Others # Google Colab # Open and follow instructions to use an isolated environment running Dream. Output Example: Seamless Tiling # The seamless tiling mode causes generated images to seamlessly tile with itself. To use it, add the --seamless option when starting the script which will result in all generated images to tile, or for each invoke> prompt as shown here: invoke > \"pond garden with lotus by claude monet\" -- seamless - s100 - n4 By default this will tile on both the X and Y axes. However, you can also specify specific axes to tile on with --seamless_axes . Possible values are x , y , and x,y : invoke > \"pond garden with lotus by claude monet\" -- seamless -- seamless_axes = x - s100 - n4 Shortcuts: Reusing Seeds # Since it is so common to reuse seeds while refining a prompt, there is now a shortcut as of version 1.11. Provide a -S (or --seed ) switch of -1 to use the seed of the most recent image generated. If you produced multiple images with the -n switch, then you can go back further using -2 , -3 , etc. up to the first image generated by the previous command. Sorry, but you can't go back further than one command. Here's an example of using this to do a quick refinement. It also illustrates using the new -G switch to turn on upscaling and face enhancement (see previous section): invoke> a cute child playing hopscotch -G0.5 [ ... ] outputs/img-samples/000039.3498014304.png: \"a cute child playing hopscotch\" -s50 -W512 -H512 -C7.5 -mk_lms -S3498014304 # I wonder what it will look like if I bump up the steps and set facial enhancement to full strength? invoke> a cute child playing hopscotch -G1.0 -s100 -S -1 reusing previous seed 3498014304 [ ... ] outputs/img-samples/000040.3498014304.png: \"a cute child playing hopscotch\" -G1.0 -s100 -W512 -H512 -C7.5 -mk_lms -S3498014304 Weighted Prompts # You may weight different sections of the prompt to tell the sampler to attach different levels of priority to them, by adding :<percent> to the end of the section you wish to up- or downweight. For example consider this prompt: tabby cat:0.25 white duck:0.75 hybrid This will tell the sampler to invest 25% of its effort on the tabby cat aspect of the image and 75% on the white duck aspect (surprisingly, this example actually works). The prompt weights can use any combination of integers and floating point numbers, and they do not need to add up to 1. Filename Format # The argument --fnformat allows to specify the filename of the image. Supported wildcards are all arguments what can be set such as perlin , seed , threshold , height , width , gfpgan_strength , sampler_name , steps , model , upscale , prompt , cfg_scale , prefix . The following prompt dream> a red car --steps 25 -C 9 .8 --perlin 0 .1 --fnformat { prompt } _steps. { steps } _cfg. { cfg_scale } _perlin. { perlin } .png generates a file with the name: outputs/img-samples/a red car_steps.25_cfg.9.8_perlin.0.1.png Thresholding and Perlin Noise Initialization Options # Two new options are the thresholding ( --threshold ) and the perlin noise initialization ( --perlin ) options. Thresholding limits the range of the latent values during optimization, which helps combat oversaturation with higher CFG scale values. Perlin noise initialization starts with a percentage (a value ranging from 0 to 1) of perlin noise mixed into the initial noise. Both features allow for more variations and options in the course of generating images. For better intuition into what these options do in practice: In generating this graphic, perlin noise at initialization was programmatically varied going across on the diagram by values 0.0, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 0.9, 1.0; and the threshold was varied going down from 0, 1, 2, 3, 4, 5, 10, 20, 100. The other options are fixed, so the initial prompt is as follows (no thresholding or perlin noise): invoke> \"a portrait of a beautiful young lady\" -S 1950357039 -s 100 -C 20 -A k_euler_a --threshold 0 --perlin 0 Here's an example of another prompt used when setting the threshold to 5 and perlin noise to 0.2: invoke> \"a portrait of a beautiful young lady\" -S 1950357039 -s 100 -C 20 -A k_euler_a --threshold 5 --perlin 0 .2 Note currently the thresholding feature is only implemented for the k-diffusion style samplers, and empirically appears to work best with k_euler_a and k_dpm_2_a . Using 0 disables thresholding. Using 0 for perlin noise disables using perlin noise for initialization. Finally, using 1 for perlin noise uses only perlin noise for initialization. Simplified API # For programmers who wish to incorporate stable-diffusion into other products, this repository includes a simplified API for text to image generation, which lets you create images from a prompt in just three lines of code: from ldm.generate import Generate g = Generate () outputs = g.txt2img ( \"a unicorn in manhattan\" ) Outputs is a list of lists in the format [filename1,seed1],[filename2,seed2]...]. Please see the documentation in ldm/generate.py for more information.","title":"Others"},{"location":"features/OTHER/#others","text":"","title":" Others"},{"location":"features/OTHER/#google-colab","text":"Open and follow instructions to use an isolated environment running Dream. Output Example:","title":"Google Colab"},{"location":"features/OTHER/#seamless-tiling","text":"The seamless tiling mode causes generated images to seamlessly tile with itself. To use it, add the --seamless option when starting the script which will result in all generated images to tile, or for each invoke> prompt as shown here: invoke > \"pond garden with lotus by claude monet\" -- seamless - s100 - n4 By default this will tile on both the X and Y axes. However, you can also specify specific axes to tile on with --seamless_axes . Possible values are x , y , and x,y : invoke > \"pond garden with lotus by claude monet\" -- seamless -- seamless_axes = x - s100 - n4","title":"Seamless Tiling"},{"location":"features/OTHER/#shortcuts-reusing-seeds","text":"Since it is so common to reuse seeds while refining a prompt, there is now a shortcut as of version 1.11. Provide a -S (or --seed ) switch of -1 to use the seed of the most recent image generated. If you produced multiple images with the -n switch, then you can go back further using -2 , -3 , etc. up to the first image generated by the previous command. Sorry, but you can't go back further than one command. Here's an example of using this to do a quick refinement. It also illustrates using the new -G switch to turn on upscaling and face enhancement (see previous section): invoke> a cute child playing hopscotch -G0.5 [ ... ] outputs/img-samples/000039.3498014304.png: \"a cute child playing hopscotch\" -s50 -W512 -H512 -C7.5 -mk_lms -S3498014304 # I wonder what it will look like if I bump up the steps and set facial enhancement to full strength? invoke> a cute child playing hopscotch -G1.0 -s100 -S -1 reusing previous seed 3498014304 [ ... ] outputs/img-samples/000040.3498014304.png: \"a cute child playing hopscotch\" -G1.0 -s100 -W512 -H512 -C7.5 -mk_lms -S3498014304","title":"Shortcuts: Reusing Seeds"},{"location":"features/OTHER/#weighted-prompts","text":"You may weight different sections of the prompt to tell the sampler to attach different levels of priority to them, by adding :<percent> to the end of the section you wish to up- or downweight. For example consider this prompt: tabby cat:0.25 white duck:0.75 hybrid This will tell the sampler to invest 25% of its effort on the tabby cat aspect of the image and 75% on the white duck aspect (surprisingly, this example actually works). The prompt weights can use any combination of integers and floating point numbers, and they do not need to add up to 1.","title":"Weighted Prompts"},{"location":"features/OTHER/#filename-format","text":"The argument --fnformat allows to specify the filename of the image. Supported wildcards are all arguments what can be set such as perlin , seed , threshold , height , width , gfpgan_strength , sampler_name , steps , model , upscale , prompt , cfg_scale , prefix . The following prompt dream> a red car --steps 25 -C 9 .8 --perlin 0 .1 --fnformat { prompt } _steps. { steps } _cfg. { cfg_scale } _perlin. { perlin } .png generates a file with the name: outputs/img-samples/a red car_steps.25_cfg.9.8_perlin.0.1.png","title":"Filename Format"},{"location":"features/OTHER/#thresholding-and-perlin-noise-initialization-options","text":"Two new options are the thresholding ( --threshold ) and the perlin noise initialization ( --perlin ) options. Thresholding limits the range of the latent values during optimization, which helps combat oversaturation with higher CFG scale values. Perlin noise initialization starts with a percentage (a value ranging from 0 to 1) of perlin noise mixed into the initial noise. Both features allow for more variations and options in the course of generating images. For better intuition into what these options do in practice: In generating this graphic, perlin noise at initialization was programmatically varied going across on the diagram by values 0.0, 0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 0.9, 1.0; and the threshold was varied going down from 0, 1, 2, 3, 4, 5, 10, 20, 100. The other options are fixed, so the initial prompt is as follows (no thresholding or perlin noise): invoke> \"a portrait of a beautiful young lady\" -S 1950357039 -s 100 -C 20 -A k_euler_a --threshold 0 --perlin 0 Here's an example of another prompt used when setting the threshold to 5 and perlin noise to 0.2: invoke> \"a portrait of a beautiful young lady\" -S 1950357039 -s 100 -C 20 -A k_euler_a --threshold 5 --perlin 0 .2 Note currently the thresholding feature is only implemented for the k-diffusion style samplers, and empirically appears to work best with k_euler_a and k_dpm_2_a . Using 0 disables thresholding. Using 0 for perlin noise disables using perlin noise for initialization. Finally, using 1 for perlin noise uses only perlin noise for initialization.","title":"Thresholding and Perlin Noise Initialization Options"},{"location":"features/OTHER/#simplified-api","text":"For programmers who wish to incorporate stable-diffusion into other products, this repository includes a simplified API for text to image generation, which lets you create images from a prompt in just three lines of code: from ldm.generate import Generate g = Generate () outputs = g.txt2img ( \"a unicorn in manhattan\" ) Outputs is a list of lists in the format [filename1,seed1],[filename2,seed2]...]. Please see the documentation in ldm/generate.py for more information.","title":"Simplified API"},{"location":"features/OUTPAINTING/","text":"Outpainting # Outpainting and outcropping # Outpainting is a process by which the AI generates parts of the image that are outside its original frame. It can be used to fix up images in which the subject is off center, or when some detail (often the top of someone's head!) is cut off. InvokeAI supports two versions of outpainting, one called \"outpaint\" and the other \"outcrop.\" They work slightly differently and each has its advantages and drawbacks. Outpainting # Outpainting is the same as inpainting, except that the painting occurs in the regions outside of the original image. To outpaint using the invoke.py command line script, prepare an image in which the borders to be extended are pure black. Add an alpha channel (if there isn't one already), and make the borders completely transparent and the interior completely opaque. If you wish to modify the interior as well, you may create transparent holes in the transparency layer, which img2img will paint into as usual. Pass the image as the argument to the -I switch as you would for regular inpainting: invoke> a stream by a river -I /path/to/transparent_img.png You'll likely be delighted by the results. Tips # Do not try to expand the image too much at once. Generally it is best to expand the margins in 64-pixel increments. 128 pixels often works, but your mileage may vary depending on the nature of the image you are trying to outpaint into. There are a series of switches that can be used to adjust how the inpainting algorithm operates. In particular, you can use these to minimize the seam that sometimes appears between the original image and the extended part. These switches are: switch default description --seam_size SEAM_SIZE 0 Size of the mask around the seam between original and outpainted image --seam_blur SEAM_BLUR 0 The amount to blur the seam inwards --seam_strength STRENGTH 0.7 The img2img strength to use when filling the seam --seam_steps SEAM_STEPS 10 The number of steps to use to fill the seam. --tile_size TILE_SIZE 32 The tile size to use for filling outpaint areas Outcrop # The outcrop extension gives you a convenient !fix postprocessing command that allows you to extend a previously-generated image in 64 pixel increments in any direction. You can apply the module to any image previously-generated by InvokeAI. Note that it works with arbitrary PNG photographs, but not currently with JPG or other formats. Outcropping is particularly effective when combined with the runwayML custom inpainting model . Consider this image: Pretty nice, but it's annoying that the top of her head is cut off. She's also a bit off center. Let's fix that! invoke> !fix images/curly.png --outcrop top 128 right 64 bottom 64 This is saying to apply the outcrop extension by extending the top of the image by 128 pixels, and the right and bottom of the image by 64 pixels. You can use any combination of top|left|right|bottom, and specify any number of pixels to extend. You can also abbreviate --outcrop to -c . The result looks like this: The new image is larger than the original (576x704) because 64 pixels were added to the top and right sides. You will need enough VRAM to process an image of this size. Outcropping non-InvokeAI images # You can outcrop an arbitrary image that was not generated by InvokeAI, but your results will vary. The inpainting-1.5 model is highly recommended, but if not feasible, then you may be able to improve the output by conditioning the outcropping with a text prompt that describes the scene using the --new_prompt argument: invoke> !fix images/vacation.png --outcrop top 128 --new_prompt \"family vacation\" You may also provide a different seed for outcropping to use by passing -S<seed> . A negative seed will generate a new random seed. A number of caveats: Although you can specify any pixel values, they will be rounded up to the nearest multiple of 64. Smaller values are better. Larger extensions are more likely to generate artefacts. However, if you wish you can run the !fix command repeatedly to cautiously expand the image. The extension is stochastic, meaning that each time you run it you'll get a slightly different result. You can run it repeatedly until you get an image you like. Unfortunately !fix does not currently respect the -n ( --iterations ) argument. Your results will be much better if you use the inpaint-1.5 model released by runwayML and installed by default by invokeai-configure . This model was trained specifically to harmoniously fill in image gaps. The standard model will work as well, but you may notice color discontinuities at the border. When using the inpaint-1.5 model, you may notice subtle changes to the area outside the masked region. This is because the model performs an encoding/decoding on the image as a whole. This does not occur with the standard model. Outpaint # The outpaint extension does the same thing, but with subtle differences. Starting with the same image, here is how we would add an additional 64 pixels to the top of the image: invoke> !fix images/curly.png --out_direction top 64 (you can abbreviate --out_direction as -D . The result is shown here: Although the effect is similar, there are significant differences from outcropping: You can only specify one direction to extend at a time. The image is not resized. Instead, the image is shifted by the specified number of pixels. If you look carefully, you'll see that less of the lady's torso is visible in the image. Because the image dimensions remain the same, there's no rounding to multiples of 64. Attempting to outpaint larger areas will frequently give rise to ugly ghosting effects. For best results, try increasing the step number. If you don't specify a pixel value in -D , it will default to half of the whole image, which is likely not what you want. Tip Neither outpaint nor outcrop are perfect, but we continue to tune and improve them. If one doesn't work, try the other. You may also wish to experiment with other img2img arguments, such as -C , -f and -s .","title":"Outpainting"},{"location":"features/OUTPAINTING/#outpainting","text":"","title":" Outpainting"},{"location":"features/OUTPAINTING/#outpainting-and-outcropping","text":"Outpainting is a process by which the AI generates parts of the image that are outside its original frame. It can be used to fix up images in which the subject is off center, or when some detail (often the top of someone's head!) is cut off. InvokeAI supports two versions of outpainting, one called \"outpaint\" and the other \"outcrop.\" They work slightly differently and each has its advantages and drawbacks.","title":"Outpainting and outcropping"},{"location":"features/OUTPAINTING/#outpainting_1","text":"Outpainting is the same as inpainting, except that the painting occurs in the regions outside of the original image. To outpaint using the invoke.py command line script, prepare an image in which the borders to be extended are pure black. Add an alpha channel (if there isn't one already), and make the borders completely transparent and the interior completely opaque. If you wish to modify the interior as well, you may create transparent holes in the transparency layer, which img2img will paint into as usual. Pass the image as the argument to the -I switch as you would for regular inpainting: invoke> a stream by a river -I /path/to/transparent_img.png You'll likely be delighted by the results.","title":"Outpainting"},{"location":"features/OUTPAINTING/#tips","text":"Do not try to expand the image too much at once. Generally it is best to expand the margins in 64-pixel increments. 128 pixels often works, but your mileage may vary depending on the nature of the image you are trying to outpaint into. There are a series of switches that can be used to adjust how the inpainting algorithm operates. In particular, you can use these to minimize the seam that sometimes appears between the original image and the extended part. These switches are: switch default description --seam_size SEAM_SIZE 0 Size of the mask around the seam between original and outpainted image --seam_blur SEAM_BLUR 0 The amount to blur the seam inwards --seam_strength STRENGTH 0.7 The img2img strength to use when filling the seam --seam_steps SEAM_STEPS 10 The number of steps to use to fill the seam. --tile_size TILE_SIZE 32 The tile size to use for filling outpaint areas","title":"Tips"},{"location":"features/OUTPAINTING/#outcrop","text":"The outcrop extension gives you a convenient !fix postprocessing command that allows you to extend a previously-generated image in 64 pixel increments in any direction. You can apply the module to any image previously-generated by InvokeAI. Note that it works with arbitrary PNG photographs, but not currently with JPG or other formats. Outcropping is particularly effective when combined with the runwayML custom inpainting model . Consider this image: Pretty nice, but it's annoying that the top of her head is cut off. She's also a bit off center. Let's fix that! invoke> !fix images/curly.png --outcrop top 128 right 64 bottom 64 This is saying to apply the outcrop extension by extending the top of the image by 128 pixels, and the right and bottom of the image by 64 pixels. You can use any combination of top|left|right|bottom, and specify any number of pixels to extend. You can also abbreviate --outcrop to -c . The result looks like this: The new image is larger than the original (576x704) because 64 pixels were added to the top and right sides. You will need enough VRAM to process an image of this size.","title":"Outcrop"},{"location":"features/OUTPAINTING/#outcropping-non-invokeai-images","text":"You can outcrop an arbitrary image that was not generated by InvokeAI, but your results will vary. The inpainting-1.5 model is highly recommended, but if not feasible, then you may be able to improve the output by conditioning the outcropping with a text prompt that describes the scene using the --new_prompt argument: invoke> !fix images/vacation.png --outcrop top 128 --new_prompt \"family vacation\" You may also provide a different seed for outcropping to use by passing -S<seed> . A negative seed will generate a new random seed. A number of caveats: Although you can specify any pixel values, they will be rounded up to the nearest multiple of 64. Smaller values are better. Larger extensions are more likely to generate artefacts. However, if you wish you can run the !fix command repeatedly to cautiously expand the image. The extension is stochastic, meaning that each time you run it you'll get a slightly different result. You can run it repeatedly until you get an image you like. Unfortunately !fix does not currently respect the -n ( --iterations ) argument. Your results will be much better if you use the inpaint-1.5 model released by runwayML and installed by default by invokeai-configure . This model was trained specifically to harmoniously fill in image gaps. The standard model will work as well, but you may notice color discontinuities at the border. When using the inpaint-1.5 model, you may notice subtle changes to the area outside the masked region. This is because the model performs an encoding/decoding on the image as a whole. This does not occur with the standard model.","title":"Outcropping non-InvokeAI images"},{"location":"features/OUTPAINTING/#outpaint","text":"The outpaint extension does the same thing, but with subtle differences. Starting with the same image, here is how we would add an additional 64 pixels to the top of the image: invoke> !fix images/curly.png --out_direction top 64 (you can abbreviate --out_direction as -D . The result is shown here: Although the effect is similar, there are significant differences from outcropping: You can only specify one direction to extend at a time. The image is not resized. Instead, the image is shifted by the specified number of pixels. If you look carefully, you'll see that less of the lady's torso is visible in the image. Because the image dimensions remain the same, there's no rounding to multiples of 64. Attempting to outpaint larger areas will frequently give rise to ugly ghosting effects. For best results, try increasing the step number. If you don't specify a pixel value in -D , it will default to half of the whole image, which is likely not what you want. Tip Neither outpaint nor outcrop are perfect, but we continue to tune and improve them. If one doesn't work, try the other. You may also wish to experiment with other img2img arguments, such as -C , -f and -s .","title":"Outpaint"},{"location":"features/POSTPROCESS/","text":"Postprocessing # Intro # This extension provides the ability to restore faces and upscale images. Face restoration and upscaling can be applied at the time you generate the images, or at any later time against a previously-generated PNG file, using the !fix command. Outpainting and outcropping can only be applied after the fact. Face Fixing # The default face restoration module is GFPGAN. The default upscale is Real-ESRGAN. For an alternative face restoration module, see CodeFormer Support below. As of version 1.14, environment.yaml will install the Real-ESRGAN package into the standard install location for python packages, and will put GFPGAN into a subdirectory of \"src\" in the InvokeAI directory. Upscaling with Real-ESRGAN should \"just work\" without further intervention. Simply pass the --upscale ( -U ) option on the invoke> command line, or indicate the desired scale on the popup in the Web GUI. GFPGAN requires a series of downloadable model files to work. These are loaded when you run invokeai-configure . If GFPAN is failing with an error, please run the following from the InvokeAI directory: invokeai-configure If you do not run this script in advance, the GFPGAN module will attempt to download the models files the first time you try to perform facial reconstruction. Upscaling # -U : <upscaling_factor> <upscaling_strength> The upscaling prompt argument takes two values. The first value is a scaling factor and should be set to either 2 or 4 only. This will either scale the image 2x or 4x respectively using different models. You can set the scaling stength between 0 and 1.0 to control intensity of the of the scaling. This is handy because AI upscalers generally tend to smooth out texture details. If you wish to retain some of those for natural looking results, we recommend using values between 0.5 to 0.8 . If you do not explicitly specify an upscaling_strength, it will default to 0.75. Face Restoration # -G : <facetool_strength> This prompt argument controls the strength of the face restoration that is being applied. Similar to upscaling, values between 0.5 to 0.8 are recommended. You can use either one or both without any conflicts. In cases where you use both, the image will be first upscaled and then the face restoration process will be executed to ensure you get the highest quality facial features. --save_orig When you use either -U or -G , the final result you get is upscaled or face modified. If you want to save the original Stable Diffusion generation, you can use the -save_orig prompt argument to save the original unaffected version too. Example Usage # invoke> \"superman dancing with a panda bear\" -U 2 0 .6 -G 0 .4 This also works with img2img: invoke> \"a man wearing a pineapple hat\" -I path/to/your/file.png -U 2 0 .5 -G 0 .6 Note GFPGAN and Real-ESRGAN are both memory intensive. In order to avoid crashes and memory overloads during the Stable Diffusion process, these effects are applied after Stable Diffusion has completed its work. In single image generations, you will see the output right away but when you are using multiple iterations, the images will first be generated and then upscaled and face restored after that process is complete. While the image generation is taking place, you will still be able to preview the base images. If you wish to stop during the image generation but want to upscale or face restore a particular generated image, pass it again with the same prompt and generated seed along with the -U and -G prompt arguments to perform those actions. CodeFormer Support # This repo also allows you to perform face restoration using CodeFormer . In order to setup CodeFormer to work, you need to download the models like with GFPGAN. You can do this either by running invokeai-configure or by manually downloading the model file and saving it to ldm/invoke/restoration/codeformer/weights folder. You can use -ft prompt argument to swap between CodeFormer and the default GFPGAN. The above mentioned -G prompt argument will allow you to control the strength of the restoration effect. CodeFormer Usage # The following command will perform face restoration with CodeFormer instead of the default gfpgan. <prompt> -G 0.8 -ft codeformer Other Options # -cf - cf or CodeFormer Fidelity takes values between 0 and 1 . 0 produces high quality results but low accuracy and 1 produces lower quality results but higher accuacy to your original face. The following command will perform face restoration with CodeFormer. CodeFormer will output a result that is closely matching to the input face. <prompt> -G 1.0 -ft codeformer -cf 0.9 The following command will perform face restoration with CodeFormer. CodeFormer will output a result that is the best restoration possible. This may deviate slightly from the original face. This is an excellent option to use in situations when there is very little facial data to work with. <prompt> -G 1.0 -ft codeformer -cf 0.1 Fixing Previously-Generated Images # It is easy to apply face restoration and/or upscaling to any previously-generated file. Just use the syntax !fix path/to/file.png <options> . For example, to apply GFPGAN at strength 0.8 and upscale 2X for a file named ./outputs/img-samples/000044.2945021133.png , just run: invoke> !fix ./outputs/img-samples/000044.2945021133.png -G 0 .8 -U 2 A new file named 000044.2945021133.fixed.png will be created in the output directory. Note that the !fix command does not replace the original file, unlike the behavior at generate time. How to disable # If, for some reason, you do not wish to load the GFPGAN and/or ESRGAN libraries, you can disable them on the invoke.py command line with the --no_restore and --no_upscale options, respectively.","title":"Postprocessing"},{"location":"features/POSTPROCESS/#postprocessing","text":"","title":" Postprocessing"},{"location":"features/POSTPROCESS/#intro","text":"This extension provides the ability to restore faces and upscale images. Face restoration and upscaling can be applied at the time you generate the images, or at any later time against a previously-generated PNG file, using the !fix command. Outpainting and outcropping can only be applied after the fact.","title":"Intro"},{"location":"features/POSTPROCESS/#face-fixing","text":"The default face restoration module is GFPGAN. The default upscale is Real-ESRGAN. For an alternative face restoration module, see CodeFormer Support below. As of version 1.14, environment.yaml will install the Real-ESRGAN package into the standard install location for python packages, and will put GFPGAN into a subdirectory of \"src\" in the InvokeAI directory. Upscaling with Real-ESRGAN should \"just work\" without further intervention. Simply pass the --upscale ( -U ) option on the invoke> command line, or indicate the desired scale on the popup in the Web GUI. GFPGAN requires a series of downloadable model files to work. These are loaded when you run invokeai-configure . If GFPAN is failing with an error, please run the following from the InvokeAI directory: invokeai-configure If you do not run this script in advance, the GFPGAN module will attempt to download the models files the first time you try to perform facial reconstruction.","title":"Face Fixing"},{"location":"features/POSTPROCESS/#upscaling","text":"-U : <upscaling_factor> <upscaling_strength> The upscaling prompt argument takes two values. The first value is a scaling factor and should be set to either 2 or 4 only. This will either scale the image 2x or 4x respectively using different models. You can set the scaling stength between 0 and 1.0 to control intensity of the of the scaling. This is handy because AI upscalers generally tend to smooth out texture details. If you wish to retain some of those for natural looking results, we recommend using values between 0.5 to 0.8 . If you do not explicitly specify an upscaling_strength, it will default to 0.75.","title":"Upscaling"},{"location":"features/POSTPROCESS/#face-restoration","text":"-G : <facetool_strength> This prompt argument controls the strength of the face restoration that is being applied. Similar to upscaling, values between 0.5 to 0.8 are recommended. You can use either one or both without any conflicts. In cases where you use both, the image will be first upscaled and then the face restoration process will be executed to ensure you get the highest quality facial features. --save_orig When you use either -U or -G , the final result you get is upscaled or face modified. If you want to save the original Stable Diffusion generation, you can use the -save_orig prompt argument to save the original unaffected version too.","title":"Face Restoration"},{"location":"features/POSTPROCESS/#example-usage","text":"invoke> \"superman dancing with a panda bear\" -U 2 0 .6 -G 0 .4 This also works with img2img: invoke> \"a man wearing a pineapple hat\" -I path/to/your/file.png -U 2 0 .5 -G 0 .6 Note GFPGAN and Real-ESRGAN are both memory intensive. In order to avoid crashes and memory overloads during the Stable Diffusion process, these effects are applied after Stable Diffusion has completed its work. In single image generations, you will see the output right away but when you are using multiple iterations, the images will first be generated and then upscaled and face restored after that process is complete. While the image generation is taking place, you will still be able to preview the base images. If you wish to stop during the image generation but want to upscale or face restore a particular generated image, pass it again with the same prompt and generated seed along with the -U and -G prompt arguments to perform those actions.","title":"Example Usage"},{"location":"features/POSTPROCESS/#codeformer-support","text":"This repo also allows you to perform face restoration using CodeFormer . In order to setup CodeFormer to work, you need to download the models like with GFPGAN. You can do this either by running invokeai-configure or by manually downloading the model file and saving it to ldm/invoke/restoration/codeformer/weights folder. You can use -ft prompt argument to swap between CodeFormer and the default GFPGAN. The above mentioned -G prompt argument will allow you to control the strength of the restoration effect.","title":"CodeFormer Support"},{"location":"features/POSTPROCESS/#codeformer-usage","text":"The following command will perform face restoration with CodeFormer instead of the default gfpgan. <prompt> -G 0.8 -ft codeformer","title":"CodeFormer Usage"},{"location":"features/POSTPROCESS/#other-options","text":"-cf - cf or CodeFormer Fidelity takes values between 0 and 1 . 0 produces high quality results but low accuracy and 1 produces lower quality results but higher accuacy to your original face. The following command will perform face restoration with CodeFormer. CodeFormer will output a result that is closely matching to the input face. <prompt> -G 1.0 -ft codeformer -cf 0.9 The following command will perform face restoration with CodeFormer. CodeFormer will output a result that is the best restoration possible. This may deviate slightly from the original face. This is an excellent option to use in situations when there is very little facial data to work with. <prompt> -G 1.0 -ft codeformer -cf 0.1","title":"Other Options"},{"location":"features/POSTPROCESS/#fixing-previously-generated-images","text":"It is easy to apply face restoration and/or upscaling to any previously-generated file. Just use the syntax !fix path/to/file.png <options> . For example, to apply GFPGAN at strength 0.8 and upscale 2X for a file named ./outputs/img-samples/000044.2945021133.png , just run: invoke> !fix ./outputs/img-samples/000044.2945021133.png -G 0 .8 -U 2 A new file named 000044.2945021133.fixed.png will be created in the output directory. Note that the !fix command does not replace the original file, unlike the behavior at generate time.","title":"Fixing Previously-Generated Images"},{"location":"features/POSTPROCESS/#how-to-disable","text":"If, for some reason, you do not wish to load the GFPGAN and/or ESRGAN libraries, you can disable them on the invoke.py command line with the --no_restore and --no_upscale options, respectively.","title":"How to disable"},{"location":"features/PROMPTS/","text":"Prompting-Features # Reading Prompts from a File # You can automate invoke.py by providing a text file with the prompts you want to run, one line per prompt. The text file must be composed with a text editor (e.g. Notepad) and not a word processor. Each line should look like what you would type at the invoke> prompt: \"a beautiful sunny day in the park, children playing\" -n4 -C10 \"stormy weather on a mountain top, goats grazing\" -s100 \"innovative packaging for a squid's dinner\" -S137038382 Then pass this file's name to invoke.py when you invoke it: python scripts/invoke.py --from_file \"/path/to/prompts.txt\" You may also read a series of prompts from standard input by providing a filename of - . For example, here is a python script that creates a matrix of prompts, each one varying slightly: #!/usr/bin/env python adjectives = [ 'sunny' , 'rainy' , 'overcast' ] samplers = [ 'k_lms' , 'k_euler_a' , 'k_heun' ] cfg = [ 7 .5, 9 , 11 ] for adj in adjectives: for samp in samplers: for cg in cfg: print ( f 'a {adj} day -A{samp} -C{cg}' ) It's output looks like this (abbreviated): a sunny day -Aklms -C7.5 a sunny day -Aklms -C9 a sunny day -Aklms -C11 a sunny day -Ak_euler_a -C7.5 a sunny day -Ak_euler_a -C9 ... a overcast day -Ak_heun -C9 a overcast day -Ak_heun -C11 To feed it to invoke.py, pass the filename of \"-\" python matrix.py | python scripts/invoke.py --from_file - When the script is finished, each of the 27 combinations of adjective, sampler and CFG will be executed. The command-line interface provides !fetch and !replay commands which allow you to read the prompts from a single previously-generated image or a whole directory of them, write the prompts to a file, and then replay them. Or you can create your own file of prompts and feed them to the command-line client from within an interactive session. See Command-Line Interface for details. Negative and Unconditioned Prompts # Any words between a pair of square brackets will instruct Stable Diffusion to attempt to ban the concept from the generated image. this is a test prompt [not really] to make you understand [cool] how this works. In the above statement, the words 'not really cool` will be ignored by Stable Diffusion. Here's a prompt that depicts what it does. original prompt: \"A fantastical translucent pony made of water and foam, ethereal, radiant, hyperalism, scottish folklore, digital painting, artstation, concept art, smooth, 8 k frostbite 3 engine, ultra detailed, art by artgerm and greg rutkowski and magali villeneuve\" -s 20 -W 512 -H 768 -C 7 .5 -A k_euler_a -S 1654590180 That image has a woman, so if we want the horse without a rider, we can influence the image not to have a woman by putting [woman] in the prompt, like this: \"A fantastical translucent poney made of water and foam, ethereal, radiant, hyperalism, scottish folklore, digital painting, artstation, concept art, smooth, 8 k frostbite 3 engine, ultra detailed, art by artgerm and greg rutkowski and magali villeneuve [woman]\" -s 20 -W 512 -H 768 -C 7 .5 -A k_euler_a -S 1654590180 That's nice - but say we also don't want the image to be quite so blue. We can add \"blue\" to the list of negative prompts, so it's now [woman blue]: \"A fantastical translucent poney made of water and foam, ethereal, radiant, hyperalism, scottish folklore, digital painting, artstation, concept art, smooth, 8 k frostbite 3 engine, ultra detailed, art by artgerm and greg rutkowski and magali villeneuve [woman blue]\" -s 20 -W 512 -H 768 -C 7 .5 -A k_euler_a -S 1654590180 Getting close - but there's no sense in having a saddle when our horse doesn't have a rider, so we'll add one more negative prompt: [woman blue saddle]. \"A fantastical translucent poney made of water and foam, ethereal, radiant, hyperalism, scottish folklore, digital painting, artstation, concept art, smooth, 8 k frostbite 3 engine, ultra detailed, art by artgerm and greg rutkowski and magali villeneuve [woman blue saddle]\" -s 20 -W 512 -H 768 -C 7 .5 -A k_euler_a -S 1654590180 Notes about this feature: The only requirement for words to be ignored is that they are in between a pair of square brackets. You can provide multiple words within the same bracket. You can provide multiple brackets with multiple words in different places of your prompt. That works just fine. To improve typical anatomy problems, you can add negative prompts like [bad anatomy, extra legs, extra arms, extra fingers, poorly drawn hands, poorly drawn feet, disfigured, out of frame, tiling, bad art, deformed, mutated] . Prompt Syntax Features # The InvokeAI prompting language has the following features: Attention weighting # Append a word or phrase with - or + , or a weight between 0 and 2 ( 1 =default), to decrease or increase \"attention\" (= a mix of per-token CFG weighting multiplier and, for - , a weighted blend with the prompt without the term). The following syntax is recognised: single words without parentheses: a tall thin man picking apricots+ single or multiple words with parentheses: a tall thin man picking (apricots)+ a tall thin man picking (apricots)- a tall thin man (picking apricots)+ a tall thin man (picking apricots)- more effect with more symbols a tall thin man (picking apricots)++ nesting a tall thin man (picking apricots+)++ ( apricots effectively gets +++ ) all of the above with explicit numbers a tall thin man picking (apricots)1.1 a tall thin man (picking (apricots)1.3)1.1 . ( + is equivalent to 1.1, ++ is pow(1.1,2), +++ is pow(1.1,3), etc; - means 0.9, -- means pow(0.9,2), etc.) attention also applies to [unconditioning] so a tall thin man picking apricots [(ladder)0.01] will very gently nudge SD away from trying to draw the man on a ladder You can use this to increase or decrease the amount of something. Starting from this prompt of a man picking apricots from a tree , let's see what happens if we increase and decrease how much attention we want Stable Diffusion to pay to the word apricots : Using - to reduce apricot-ness: a man picking apricots- from a tree a man picking apricots-- from a tree a man picking apricots--- from a tree Using + to increase apricot-ness: a man picking apricots+ from a tree a man picking apricots++ from a tree a man picking apricots+++ from a tree a man picking apricots++++ from a tree a man picking apricots+++++ from a tree You can also change the balance between different parts of a prompt. For example, below is a mountain man : And here he is with more mountain: mountain+ man mountain++ man mountain+++ man Or, alternatively, with more man: mountain man+ mountain man++ mountain man+++ mountain man++++ Blending between prompts # (\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,1) The existing prompt blending using :<weight> will continue to be supported - (\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,1) is equivalent to a tall thin man picking apricots:1 a tall thin man picking pears:1 in the old syntax. Attention weights can be nested inside blends. Non-normalized blends are supported by passing no_normalize as an additional argument to the blend weights, eg (\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,-1,no_normalize) . very fun to explore local maxima in the feature space, but also easy to produce garbage output. See the section below on \"Prompt Blending\" for more information about how this works. Cross-Attention Control ('prompt2prompt') # Sometimes an image you generate is almost right, and you just want to change one detail without affecting the rest. You could use a photo editor and inpainting to overpaint the area, but that's a pain. Here's where prompt2prompt comes in handy. Generate an image with a given prompt, record the seed of the image, and then use the prompt2prompt syntax to substitute words in the original prompt for words in a new prompt. This works for img2img as well. For example, consider the prompt a cat.swap(dog) playing with a ball in the forest . Normally, because of the word words interact with each other when doing a stable diffusion image generation, these two prompts would generate different compositions: - a cat playing with a ball in the forest - a dog playing with a ball in the forest a cat playing with a ball in the forest a dog playing with a ball in the forest img img - For multiple word swaps, use parentheses: `a (fluffy cat).swap(barking dog) playing with a ball in the forest`. - To swap a comma, use quotes: `a (\"fluffy, grey cat\").swap(\"big, barking dog\") playing with a ball in the forest`. Supports options t_start and t_end (each 0-1) loosely corresponding to bloc97's prompt_edit_tokens_start/_end but with the math swapped to make it easier to intuitively understand. t_start and t_end are used to control on which steps cross-attention control should run. With the default values t_start=0 and t_end=1 , cross-attention control is active on every step of image generation. Other values can be used to turn cross-attention control off for part of the image generation process. For example, if doing a diffusion with 10 steps for the prompt is a cat.swap(dog, t_start=0.3, t_end=1.0) playing with a ball in the forest , the first 3 steps will be run as a cat playing with a ball in the forest , while the last 7 steps will run as a dog playing with a ball in the forest , but the pixels that represent dog will be locked to the pixels that would have represented cat if the cat prompt had been used instead. Conversely, for a cat.swap(dog, t_start=0, t_end=0.7) playing with a ball in the forest , the first 7 steps will run as a dog playing with a ball in the forest with the pixels that represent dog locked to the same pixels that would have represented cat if the cat prompt was being used instead. The final 3 steps will just run a cat playing with a ball in the forest . For img2img, the step sequence does not start at 0 but instead at (1.0-strength) - so if the img2img strength is 0.7 , t_start and t_end must both be greater than 0.3 ( 1.0-0.7 ) to have any effect. Prompt2prompt .swap() is not compatible with xformers, which will be temporarily disabled when doing a .swap() - so you should expect to use more VRAM and run slower that with xformers enabled. The prompt2prompt code is based off bloc97's colab . Note that prompt2prompt is not currently working with the runwayML inpainting model, and may never work due to the way this model is set up. If you attempt to use prompt2prompt you will get the original image back. However, since this model is so good at inpainting, a good substitute is to use the clipseg text masking option: invoke> a fluffy cat eating a hotdot Outputs: [ 1010 ] outputs/000025.2182095108.png: a fluffy cat eating a hotdog invoke> a smiling dog eating a hotdog -I 000025 .2182095108.png -tm cat Escaping parantheses () and speech marks \"\" # If the model you are using has parentheses () or speech marks \"\" as part of its syntax, you will need to \"escape\" these using a backslash, so that (my_keyword) becomes \\(my_keyword\\) . Otherwise, the prompt parser will attempt to interpret the parentheses as part of the prompt syntax and it will get confused. Prompt Blending # You may blend together different sections of the prompt to explore the AI's latent semantic space and generate interesting (and often surprising!) variations. The syntax is: blue sphere:0.25 red cube:0.75 hybrid This will tell the sampler to blend 25% of the concept of a blue sphere with 75% of the concept of a red cube. The blend weights can use any combination of integers and floating point numbers, and they do not need to add up to 1. Everything to the left of the :XX up to the previous :XX is used for merging, so the overall effect is: 0 .25 * \"blue sphere\" + 0 .75 * \"white duck\" + hybrid Because you are exploring the \"mind\" of the AI, the AI's way of mixing two concepts may not match yours, leading to surprising effects. To illustrate, here are three images generated using various combinations of blend weights. As usual, unless you fix the seed, the prompts will give you different results each time you run them. \"blue sphere, red cube, hybrid\" # This example doesn't use melding at all and represents the default way of mixing concepts. It's interesting to see how the AI expressed the concept of \"cube\" as the four quadrants of the enclosing frame. If you look closely, there is depth there, so the enclosing frame is actually a cube. \"blue sphere:0.25 red cube:0.75 hybrid\" # Now that's interesting. We get neither a blue sphere nor a red cube, but a red sphere embedded in a brick wall, which represents a melding of concepts within the AI's \"latent space\" of semantic representations. Where is Ludwig Wittgenstein when you need him? \"blue sphere:0.75 red cube:0.25 hybrid\" # Definitely more blue-spherey. The cube is gone entirely, but it's really cool abstract art. \"blue sphere:0.5 red cube:0.5 hybrid\" # Whoa...! I see blue and red, but no spheres or cubes. Is the word \"hybrid\" summoning up the concept of some sort of scifi creature? Let's find out. \"blue sphere:0.5 red cube:0.5\" # Indeed, removing the word \"hybrid\" produces an image that is more like what we'd expect. In conclusion, prompt blending is great for exploring creative space, but can be difficult to direct. A forthcoming release of InvokeAI will feature more deterministic prompt weighting.","title":"Prompting-Features"},{"location":"features/PROMPTS/#prompting-features","text":"","title":" Prompting-Features"},{"location":"features/PROMPTS/#reading-prompts-from-a-file","text":"You can automate invoke.py by providing a text file with the prompts you want to run, one line per prompt. The text file must be composed with a text editor (e.g. Notepad) and not a word processor. Each line should look like what you would type at the invoke> prompt: \"a beautiful sunny day in the park, children playing\" -n4 -C10 \"stormy weather on a mountain top, goats grazing\" -s100 \"innovative packaging for a squid's dinner\" -S137038382 Then pass this file's name to invoke.py when you invoke it: python scripts/invoke.py --from_file \"/path/to/prompts.txt\" You may also read a series of prompts from standard input by providing a filename of - . For example, here is a python script that creates a matrix of prompts, each one varying slightly: #!/usr/bin/env python adjectives = [ 'sunny' , 'rainy' , 'overcast' ] samplers = [ 'k_lms' , 'k_euler_a' , 'k_heun' ] cfg = [ 7 .5, 9 , 11 ] for adj in adjectives: for samp in samplers: for cg in cfg: print ( f 'a {adj} day -A{samp} -C{cg}' ) It's output looks like this (abbreviated): a sunny day -Aklms -C7.5 a sunny day -Aklms -C9 a sunny day -Aklms -C11 a sunny day -Ak_euler_a -C7.5 a sunny day -Ak_euler_a -C9 ... a overcast day -Ak_heun -C9 a overcast day -Ak_heun -C11 To feed it to invoke.py, pass the filename of \"-\" python matrix.py | python scripts/invoke.py --from_file - When the script is finished, each of the 27 combinations of adjective, sampler and CFG will be executed. The command-line interface provides !fetch and !replay commands which allow you to read the prompts from a single previously-generated image or a whole directory of them, write the prompts to a file, and then replay them. Or you can create your own file of prompts and feed them to the command-line client from within an interactive session. See Command-Line Interface for details.","title":"Reading Prompts from a File"},{"location":"features/PROMPTS/#negative-and-unconditioned-prompts","text":"Any words between a pair of square brackets will instruct Stable Diffusion to attempt to ban the concept from the generated image. this is a test prompt [not really] to make you understand [cool] how this works. In the above statement, the words 'not really cool` will be ignored by Stable Diffusion. Here's a prompt that depicts what it does. original prompt: \"A fantastical translucent pony made of water and foam, ethereal, radiant, hyperalism, scottish folklore, digital painting, artstation, concept art, smooth, 8 k frostbite 3 engine, ultra detailed, art by artgerm and greg rutkowski and magali villeneuve\" -s 20 -W 512 -H 768 -C 7 .5 -A k_euler_a -S 1654590180 That image has a woman, so if we want the horse without a rider, we can influence the image not to have a woman by putting [woman] in the prompt, like this: \"A fantastical translucent poney made of water and foam, ethereal, radiant, hyperalism, scottish folklore, digital painting, artstation, concept art, smooth, 8 k frostbite 3 engine, ultra detailed, art by artgerm and greg rutkowski and magali villeneuve [woman]\" -s 20 -W 512 -H 768 -C 7 .5 -A k_euler_a -S 1654590180 That's nice - but say we also don't want the image to be quite so blue. We can add \"blue\" to the list of negative prompts, so it's now [woman blue]: \"A fantastical translucent poney made of water and foam, ethereal, radiant, hyperalism, scottish folklore, digital painting, artstation, concept art, smooth, 8 k frostbite 3 engine, ultra detailed, art by artgerm and greg rutkowski and magali villeneuve [woman blue]\" -s 20 -W 512 -H 768 -C 7 .5 -A k_euler_a -S 1654590180 Getting close - but there's no sense in having a saddle when our horse doesn't have a rider, so we'll add one more negative prompt: [woman blue saddle]. \"A fantastical translucent poney made of water and foam, ethereal, radiant, hyperalism, scottish folklore, digital painting, artstation, concept art, smooth, 8 k frostbite 3 engine, ultra detailed, art by artgerm and greg rutkowski and magali villeneuve [woman blue saddle]\" -s 20 -W 512 -H 768 -C 7 .5 -A k_euler_a -S 1654590180 Notes about this feature: The only requirement for words to be ignored is that they are in between a pair of square brackets. You can provide multiple words within the same bracket. You can provide multiple brackets with multiple words in different places of your prompt. That works just fine. To improve typical anatomy problems, you can add negative prompts like [bad anatomy, extra legs, extra arms, extra fingers, poorly drawn hands, poorly drawn feet, disfigured, out of frame, tiling, bad art, deformed, mutated] .","title":"Negative and Unconditioned Prompts"},{"location":"features/PROMPTS/#prompt-syntax-features","text":"The InvokeAI prompting language has the following features:","title":"Prompt Syntax Features"},{"location":"features/PROMPTS/#attention-weighting","text":"Append a word or phrase with - or + , or a weight between 0 and 2 ( 1 =default), to decrease or increase \"attention\" (= a mix of per-token CFG weighting multiplier and, for - , a weighted blend with the prompt without the term). The following syntax is recognised: single words without parentheses: a tall thin man picking apricots+ single or multiple words with parentheses: a tall thin man picking (apricots)+ a tall thin man picking (apricots)- a tall thin man (picking apricots)+ a tall thin man (picking apricots)- more effect with more symbols a tall thin man (picking apricots)++ nesting a tall thin man (picking apricots+)++ ( apricots effectively gets +++ ) all of the above with explicit numbers a tall thin man picking (apricots)1.1 a tall thin man (picking (apricots)1.3)1.1 . ( + is equivalent to 1.1, ++ is pow(1.1,2), +++ is pow(1.1,3), etc; - means 0.9, -- means pow(0.9,2), etc.) attention also applies to [unconditioning] so a tall thin man picking apricots [(ladder)0.01] will very gently nudge SD away from trying to draw the man on a ladder You can use this to increase or decrease the amount of something. Starting from this prompt of a man picking apricots from a tree , let's see what happens if we increase and decrease how much attention we want Stable Diffusion to pay to the word apricots : Using - to reduce apricot-ness: a man picking apricots- from a tree a man picking apricots-- from a tree a man picking apricots--- from a tree Using + to increase apricot-ness: a man picking apricots+ from a tree a man picking apricots++ from a tree a man picking apricots+++ from a tree a man picking apricots++++ from a tree a man picking apricots+++++ from a tree You can also change the balance between different parts of a prompt. For example, below is a mountain man : And here he is with more mountain: mountain+ man mountain++ man mountain+++ man Or, alternatively, with more man: mountain man+ mountain man++ mountain man+++ mountain man++++","title":"Attention weighting"},{"location":"features/PROMPTS/#blending-between-prompts","text":"(\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,1) The existing prompt blending using :<weight> will continue to be supported - (\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,1) is equivalent to a tall thin man picking apricots:1 a tall thin man picking pears:1 in the old syntax. Attention weights can be nested inside blends. Non-normalized blends are supported by passing no_normalize as an additional argument to the blend weights, eg (\"a tall thin man picking apricots\", \"a tall thin man picking pears\").blend(1,-1,no_normalize) . very fun to explore local maxima in the feature space, but also easy to produce garbage output. See the section below on \"Prompt Blending\" for more information about how this works.","title":"Blending between prompts"},{"location":"features/PROMPTS/#cross-attention-control-prompt2prompt","text":"Sometimes an image you generate is almost right, and you just want to change one detail without affecting the rest. You could use a photo editor and inpainting to overpaint the area, but that's a pain. Here's where prompt2prompt comes in handy. Generate an image with a given prompt, record the seed of the image, and then use the prompt2prompt syntax to substitute words in the original prompt for words in a new prompt. This works for img2img as well. For example, consider the prompt a cat.swap(dog) playing with a ball in the forest . Normally, because of the word words interact with each other when doing a stable diffusion image generation, these two prompts would generate different compositions: - a cat playing with a ball in the forest - a dog playing with a ball in the forest a cat playing with a ball in the forest a dog playing with a ball in the forest img img - For multiple word swaps, use parentheses: `a (fluffy cat).swap(barking dog) playing with a ball in the forest`. - To swap a comma, use quotes: `a (\"fluffy, grey cat\").swap(\"big, barking dog\") playing with a ball in the forest`. Supports options t_start and t_end (each 0-1) loosely corresponding to bloc97's prompt_edit_tokens_start/_end but with the math swapped to make it easier to intuitively understand. t_start and t_end are used to control on which steps cross-attention control should run. With the default values t_start=0 and t_end=1 , cross-attention control is active on every step of image generation. Other values can be used to turn cross-attention control off for part of the image generation process. For example, if doing a diffusion with 10 steps for the prompt is a cat.swap(dog, t_start=0.3, t_end=1.0) playing with a ball in the forest , the first 3 steps will be run as a cat playing with a ball in the forest , while the last 7 steps will run as a dog playing with a ball in the forest , but the pixels that represent dog will be locked to the pixels that would have represented cat if the cat prompt had been used instead. Conversely, for a cat.swap(dog, t_start=0, t_end=0.7) playing with a ball in the forest , the first 7 steps will run as a dog playing with a ball in the forest with the pixels that represent dog locked to the same pixels that would have represented cat if the cat prompt was being used instead. The final 3 steps will just run a cat playing with a ball in the forest . For img2img, the step sequence does not start at 0 but instead at (1.0-strength) - so if the img2img strength is 0.7 , t_start and t_end must both be greater than 0.3 ( 1.0-0.7 ) to have any effect. Prompt2prompt .swap() is not compatible with xformers, which will be temporarily disabled when doing a .swap() - so you should expect to use more VRAM and run slower that with xformers enabled. The prompt2prompt code is based off bloc97's colab . Note that prompt2prompt is not currently working with the runwayML inpainting model, and may never work due to the way this model is set up. If you attempt to use prompt2prompt you will get the original image back. However, since this model is so good at inpainting, a good substitute is to use the clipseg text masking option: invoke> a fluffy cat eating a hotdot Outputs: [ 1010 ] outputs/000025.2182095108.png: a fluffy cat eating a hotdog invoke> a smiling dog eating a hotdog -I 000025 .2182095108.png -tm cat","title":"Cross-Attention Control ('prompt2prompt')"},{"location":"features/PROMPTS/#escaping-parantheses-and-speech-marks","text":"If the model you are using has parentheses () or speech marks \"\" as part of its syntax, you will need to \"escape\" these using a backslash, so that (my_keyword) becomes \\(my_keyword\\) . Otherwise, the prompt parser will attempt to interpret the parentheses as part of the prompt syntax and it will get confused.","title":"Escaping parantheses () and speech marks \"\""},{"location":"features/PROMPTS/#prompt-blending","text":"You may blend together different sections of the prompt to explore the AI's latent semantic space and generate interesting (and often surprising!) variations. The syntax is: blue sphere:0.25 red cube:0.75 hybrid This will tell the sampler to blend 25% of the concept of a blue sphere with 75% of the concept of a red cube. The blend weights can use any combination of integers and floating point numbers, and they do not need to add up to 1. Everything to the left of the :XX up to the previous :XX is used for merging, so the overall effect is: 0 .25 * \"blue sphere\" + 0 .75 * \"white duck\" + hybrid Because you are exploring the \"mind\" of the AI, the AI's way of mixing two concepts may not match yours, leading to surprising effects. To illustrate, here are three images generated using various combinations of blend weights. As usual, unless you fix the seed, the prompts will give you different results each time you run them.","title":"Prompt Blending"},{"location":"features/PROMPTS/#blue-sphere-red-cube-hybrid","text":"This example doesn't use melding at all and represents the default way of mixing concepts. It's interesting to see how the AI expressed the concept of \"cube\" as the four quadrants of the enclosing frame. If you look closely, there is depth there, so the enclosing frame is actually a cube.","title":"\"blue sphere, red cube, hybrid\""},{"location":"features/PROMPTS/#blue-sphere025-red-cube075-hybrid","text":"Now that's interesting. We get neither a blue sphere nor a red cube, but a red sphere embedded in a brick wall, which represents a melding of concepts within the AI's \"latent space\" of semantic representations. Where is Ludwig Wittgenstein when you need him?","title":"\"blue sphere:0.25 red cube:0.75 hybrid\""},{"location":"features/PROMPTS/#blue-sphere075-red-cube025-hybrid","text":"Definitely more blue-spherey. The cube is gone entirely, but it's really cool abstract art.","title":"\"blue sphere:0.75 red cube:0.25 hybrid\""},{"location":"features/PROMPTS/#blue-sphere05-red-cube05-hybrid","text":"Whoa...! I see blue and red, but no spheres or cubes. Is the word \"hybrid\" summoning up the concept of some sort of scifi creature? Let's find out.","title":"\"blue sphere:0.5 red cube:0.5 hybrid\""},{"location":"features/PROMPTS/#blue-sphere05-red-cube05","text":"Indeed, removing the word \"hybrid\" produces an image that is more like what we'd expect. In conclusion, prompt blending is great for exploring creative space, but can be difficult to direct. A forthcoming release of InvokeAI will feature more deterministic prompt weighting.","title":"\"blue sphere:0.5 red cube:0.5\""},{"location":"features/TEXTUAL_INVERSION/","text":"Textual Inversion # Personalizing Text-to-Image Generation # You may personalize the generated images to provide your own styles or objects by training a new LDM checkpoint and introducing a new vocabulary to the fixed model as a (.pt) embeddings file. Alternatively, you may use or train HuggingFace Concepts embeddings files (.bin) from https://huggingface.co/sd-concepts-library and its associated notebooks. Hardware and Software Requirements # You will need a GPU to perform training in a reasonable length of time, and at least 12 GB of VRAM. We recommend using the xformers library to accelerate the training process further. During training, about ~8 GB is temporarily needed in order to store intermediate models, checkpoints and logs. Preparing for Training # To train, prepare a folder that contains 3-5 images that illustrate the object or concept. It is good to provide a variety of examples or poses to avoid overtraining the system. Format these images as PNG (preferred) or JPG. You do not need to resize or crop the images in advance, but for more control you may wish to do so. Place the training images in a directory on the machine InvokeAI runs on. We recommend placing them in a subdirectory of the text-inversion-training-data folder located in the InvokeAI root directory, ordinarily ~/invokeai (Linux/Mac), or C:\\Users\\your_name\\invokeai (Windows). For example, to create an embedding for the \"psychedelic\" style, you'd place the training images into the directory ~invokeai/text-inversion-training-data/psychedelic . Launching Training Using the Console Front End # InvokeAI 2.3 and higher comes with a text console-based training front end. From within the invoke.sh / invoke.bat Invoke launcher script, start the front end by selecting choice (3): Do you want to generate images using the 1 . command-line 2 . browser-based UI 3 . textual inversion training 4 . open the developer console Please enter 1 , 2 , 3 , or 4 : [ 1 ] 3 From the command line, with the InvokeAI virtual environment active, you can launch the front end with the command invokeai-ti --gui . This will launch a text-based front end that will look like this: The interface is keyboard-based. Move from field to field using control-N (^N) to move to the next field and control-P (^P) to the previous one. and work as well. Once a field is active, use the cursor keys. In a checkbox group, use the up and down cursor keys to move from choice to choice, and to select a choice. In a scrollbar, use the left and right cursor keys to increase and decrease the value of the scroll. In textfields, type the desired values. The number of parameters may look intimidating, but in most cases the predefined defaults work fine. The red circled fields in the above illustration are the ones you will adjust most frequently. Model Name # This will list all the diffusers models that are currently installed. Select the one you wish to use as the basis for your embedding. Be aware that if you use a SD-1.X-based model for your training, you will only be able to use this embedding with other SD-1.X-based models. Similarly, if you train on SD-2.X, you will only be able to use the embeddings with models based on SD-2.X. Trigger Term # This is the prompt term you will use to trigger the embedding. Type a single word or phrase you wish to use as the trigger, example \"psychedelic\" (without angle brackets). Within InvokeAI, you will then be able to activate the trigger using the syntax <psychedelic> . Initializer # This is a single character that is used internally during the training process as a placeholder for the trigger term. It defaults to \"*\" and can usually be left alone. Resume from last saved checkpoint # As training proceeds, textual inversion will write a series of intermediate files that can be used to resume training from where it was left off in the case of an interruption. This checkbox will be automatically selected if you provide a previously used trigger term and at least one checkpoint file is found on disk. Note that as of 20 January 2023, resume does not seem to be working properly due to an issue with the upstream code. Data Training Directory # This is the location of the images to be used for training. When you select a trigger term like \"my-trigger\", the frontend will prepopulate this field with ~/invokeai/text-inversion-training-data/my-trigger , but you can change the path to wherever you want. Output Destination Directory # This is the location of the logs, checkpoint files, and embedding files created during training. When you select a trigger term like \"my-trigger\", the frontend will prepopulate this field with ~/invokeai/text-inversion-output/my-trigger , but you can change the path to wherever you want. Image resolution # The images in the training directory will be automatically scaled to the value you use here. For best results, you will want to use the same default resolution of the underlying model (512 pixels for SD-1.5, 768 for the larger version of SD-2.1). Center crop images # If this is selected, your images will be center cropped to make them square before resizing them to the desired resolution. Center cropping can indiscriminately cut off the top of subjects' heads for portrait aspect images, so if you have images like this, you may wish to use a photoeditor to manually crop them to a square aspect ratio. Mixed precision # Select the floating point precision for the embedding. \"no\" will result in a full 32-bit precision, \"fp16\" will provide 16-bit precision, and \"bf16\" will provide mixed precision (only available when XFormers is used). Max training steps # How many steps the training will take before the model converges. Most training sets will converge with 2000-3000 steps. Batch size # This adjusts how many training images are processed simultaneously in each step. Higher values will cause the training process to run more quickly, but use more memory. The default size will run with GPUs with as little as 12 GB. Learning rate # The rate at which the system adjusts its internal weights during training. Higher values risk overtraining (getting the same image each time), and lower values will take more steps to train a good model. The default of 0.0005 is conservative; you may wish to increase it to 0.005 to speed up training. Scale learning rate by number of GPUs, steps and batch size # If this is selected (the default) the system will adjust the provided learning rate to improve performance. Use xformers acceleration # This will activate XFormers memory-efficient attention. You need to have XFormers installed for this to have an effect. Learning rate scheduler # This adjusts how the learning rate changes over the course of training. The default \"constant\" means to use a constant learning rate for the entire training session. The other values scale the learning rate according to various formulas. Only \"constant\" is supported by the XFormers library. Gradient accumulation steps # This is a parameter that allows you to use bigger batch sizes than your GPU's VRAM would ordinarily accommodate, at the cost of some performance. Warmup steps # If \"constant_with_warmup\" is selected in the learning rate scheduler, then this provides the number of warmup steps. Warmup steps have a very low learning rate, and are one way of preventing early overtraining. The training run # Start the training run by advancing to the OK button (bottom right) and pressing . A series of progress messages will be displayed as the training process proceeds. This may take an hour or two, depending on settings and the speed of your system. Various log and checkpoint files will be written into the output directory (ordinarily ~/invokeai/text-inversion-output/my-model/ ) At the end of successful training, the system will copy the file learned_embeds.bin into the InvokeAI root directory's embeddings directory, using a subdirectory named after the trigger token. For example, if the trigger token was psychedelic , then look for the embeddings file in ~/invokeai/embeddings/psychedelic/learned_embeds.bin You may now launch InvokeAI and try out a prompt that uses the trigger term. For example a plate of banana sushi in <psychedelic> style . Training with the Command-Line Script # Training can also be done using a traditional command-line script. It can be launched from within the \"developer's console\", or from the command line after activating InvokeAI's virtual environment. It accepts a large number of arguments, which can be summarized by passing the --help argument: invokeai-ti --help Typical usage is shown here: invokeai-ti \\ --model = stable-diffusion-1.5 \\ --resolution = 512 \\ --learnable_property = style \\ --initializer_token = '*' \\ --placeholder_token = '<psychedelic>' \\ --train_data_dir = /home/lstein/invokeai/training-data/psychedelic \\ --output_dir = /home/lstein/invokeai/text-inversion-training/psychedelic \\ --scale_lr \\ --train_batch_size = 8 \\ --gradient_accumulation_steps = 4 \\ --max_train_steps = 3000 \\ --learning_rate = 0 .0005 \\ --resume_from_checkpoint = latest \\ --lr_scheduler = constant \\ --mixed_precision = fp16 \\ --only_save_embeds Reading # For more information on textual inversion, please see the following resources: The textual inversion repository and associated paper for details and limitations. HuggingFace's textual inversion training page HuggingFace example script documentation (Note that this script is similar to, but not identical, to textual_inversion , but produces embed files that are completely compatible. copyright \u00a9 2023, Lincoln Stein and the InvokeAI Development Team","title":"Textual-Inversion"},{"location":"features/TEXTUAL_INVERSION/#textual-inversion","text":"","title":" Textual Inversion"},{"location":"features/TEXTUAL_INVERSION/#personalizing-text-to-image-generation","text":"You may personalize the generated images to provide your own styles or objects by training a new LDM checkpoint and introducing a new vocabulary to the fixed model as a (.pt) embeddings file. Alternatively, you may use or train HuggingFace Concepts embeddings files (.bin) from https://huggingface.co/sd-concepts-library and its associated notebooks.","title":"Personalizing Text-to-Image Generation"},{"location":"features/TEXTUAL_INVERSION/#hardware-and-software-requirements","text":"You will need a GPU to perform training in a reasonable length of time, and at least 12 GB of VRAM. We recommend using the xformers library to accelerate the training process further. During training, about ~8 GB is temporarily needed in order to store intermediate models, checkpoints and logs.","title":"Hardware and Software Requirements"},{"location":"features/TEXTUAL_INVERSION/#preparing-for-training","text":"To train, prepare a folder that contains 3-5 images that illustrate the object or concept. It is good to provide a variety of examples or poses to avoid overtraining the system. Format these images as PNG (preferred) or JPG. You do not need to resize or crop the images in advance, but for more control you may wish to do so. Place the training images in a directory on the machine InvokeAI runs on. We recommend placing them in a subdirectory of the text-inversion-training-data folder located in the InvokeAI root directory, ordinarily ~/invokeai (Linux/Mac), or C:\\Users\\your_name\\invokeai (Windows). For example, to create an embedding for the \"psychedelic\" style, you'd place the training images into the directory ~invokeai/text-inversion-training-data/psychedelic .","title":"Preparing for Training"},{"location":"features/TEXTUAL_INVERSION/#launching-training-using-the-console-front-end","text":"InvokeAI 2.3 and higher comes with a text console-based training front end. From within the invoke.sh / invoke.bat Invoke launcher script, start the front end by selecting choice (3): Do you want to generate images using the 1 . command-line 2 . browser-based UI 3 . textual inversion training 4 . open the developer console Please enter 1 , 2 , 3 , or 4 : [ 1 ] 3 From the command line, with the InvokeAI virtual environment active, you can launch the front end with the command invokeai-ti --gui . This will launch a text-based front end that will look like this: The interface is keyboard-based. Move from field to field using control-N (^N) to move to the next field and control-P (^P) to the previous one. and work as well. Once a field is active, use the cursor keys. In a checkbox group, use the up and down cursor keys to move from choice to choice, and to select a choice. In a scrollbar, use the left and right cursor keys to increase and decrease the value of the scroll. In textfields, type the desired values. The number of parameters may look intimidating, but in most cases the predefined defaults work fine. The red circled fields in the above illustration are the ones you will adjust most frequently.","title":"Launching Training Using the Console Front End"},{"location":"features/TEXTUAL_INVERSION/#model-name","text":"This will list all the diffusers models that are currently installed. Select the one you wish to use as the basis for your embedding. Be aware that if you use a SD-1.X-based model for your training, you will only be able to use this embedding with other SD-1.X-based models. Similarly, if you train on SD-2.X, you will only be able to use the embeddings with models based on SD-2.X.","title":"Model Name"},{"location":"features/TEXTUAL_INVERSION/#trigger-term","text":"This is the prompt term you will use to trigger the embedding. Type a single word or phrase you wish to use as the trigger, example \"psychedelic\" (without angle brackets). Within InvokeAI, you will then be able to activate the trigger using the syntax <psychedelic> .","title":"Trigger Term"},{"location":"features/TEXTUAL_INVERSION/#initializer","text":"This is a single character that is used internally during the training process as a placeholder for the trigger term. It defaults to \"*\" and can usually be left alone.","title":"Initializer"},{"location":"features/TEXTUAL_INVERSION/#resume-from-last-saved-checkpoint","text":"As training proceeds, textual inversion will write a series of intermediate files that can be used to resume training from where it was left off in the case of an interruption. This checkbox will be automatically selected if you provide a previously used trigger term and at least one checkpoint file is found on disk. Note that as of 20 January 2023, resume does not seem to be working properly due to an issue with the upstream code.","title":"Resume from last saved checkpoint"},{"location":"features/TEXTUAL_INVERSION/#data-training-directory","text":"This is the location of the images to be used for training. When you select a trigger term like \"my-trigger\", the frontend will prepopulate this field with ~/invokeai/text-inversion-training-data/my-trigger , but you can change the path to wherever you want.","title":"Data Training Directory"},{"location":"features/TEXTUAL_INVERSION/#output-destination-directory","text":"This is the location of the logs, checkpoint files, and embedding files created during training. When you select a trigger term like \"my-trigger\", the frontend will prepopulate this field with ~/invokeai/text-inversion-output/my-trigger , but you can change the path to wherever you want.","title":"Output Destination Directory"},{"location":"features/TEXTUAL_INVERSION/#image-resolution","text":"The images in the training directory will be automatically scaled to the value you use here. For best results, you will want to use the same default resolution of the underlying model (512 pixels for SD-1.5, 768 for the larger version of SD-2.1).","title":"Image resolution"},{"location":"features/TEXTUAL_INVERSION/#center-crop-images","text":"If this is selected, your images will be center cropped to make them square before resizing them to the desired resolution. Center cropping can indiscriminately cut off the top of subjects' heads for portrait aspect images, so if you have images like this, you may wish to use a photoeditor to manually crop them to a square aspect ratio.","title":"Center crop images"},{"location":"features/TEXTUAL_INVERSION/#mixed-precision","text":"Select the floating point precision for the embedding. \"no\" will result in a full 32-bit precision, \"fp16\" will provide 16-bit precision, and \"bf16\" will provide mixed precision (only available when XFormers is used).","title":"Mixed precision"},{"location":"features/TEXTUAL_INVERSION/#max-training-steps","text":"How many steps the training will take before the model converges. Most training sets will converge with 2000-3000 steps.","title":"Max training steps"},{"location":"features/TEXTUAL_INVERSION/#batch-size","text":"This adjusts how many training images are processed simultaneously in each step. Higher values will cause the training process to run more quickly, but use more memory. The default size will run with GPUs with as little as 12 GB.","title":"Batch size"},{"location":"features/TEXTUAL_INVERSION/#learning-rate","text":"The rate at which the system adjusts its internal weights during training. Higher values risk overtraining (getting the same image each time), and lower values will take more steps to train a good model. The default of 0.0005 is conservative; you may wish to increase it to 0.005 to speed up training.","title":"Learning rate"},{"location":"features/TEXTUAL_INVERSION/#scale-learning-rate-by-number-of-gpus-steps-and-batch-size","text":"If this is selected (the default) the system will adjust the provided learning rate to improve performance.","title":"Scale learning rate by number of GPUs, steps and batch size"},{"location":"features/TEXTUAL_INVERSION/#use-xformers-acceleration","text":"This will activate XFormers memory-efficient attention. You need to have XFormers installed for this to have an effect.","title":"Use xformers acceleration"},{"location":"features/TEXTUAL_INVERSION/#learning-rate-scheduler","text":"This adjusts how the learning rate changes over the course of training. The default \"constant\" means to use a constant learning rate for the entire training session. The other values scale the learning rate according to various formulas. Only \"constant\" is supported by the XFormers library.","title":"Learning rate scheduler"},{"location":"features/TEXTUAL_INVERSION/#gradient-accumulation-steps","text":"This is a parameter that allows you to use bigger batch sizes than your GPU's VRAM would ordinarily accommodate, at the cost of some performance.","title":"Gradient accumulation steps"},{"location":"features/TEXTUAL_INVERSION/#warmup-steps","text":"If \"constant_with_warmup\" is selected in the learning rate scheduler, then this provides the number of warmup steps. Warmup steps have a very low learning rate, and are one way of preventing early overtraining.","title":"Warmup steps"},{"location":"features/TEXTUAL_INVERSION/#the-training-run","text":"Start the training run by advancing to the OK button (bottom right) and pressing . A series of progress messages will be displayed as the training process proceeds. This may take an hour or two, depending on settings and the speed of your system. Various log and checkpoint files will be written into the output directory (ordinarily ~/invokeai/text-inversion-output/my-model/ ) At the end of successful training, the system will copy the file learned_embeds.bin into the InvokeAI root directory's embeddings directory, using a subdirectory named after the trigger token. For example, if the trigger token was psychedelic , then look for the embeddings file in ~/invokeai/embeddings/psychedelic/learned_embeds.bin You may now launch InvokeAI and try out a prompt that uses the trigger term. For example a plate of banana sushi in <psychedelic> style .","title":"The training run"},{"location":"features/TEXTUAL_INVERSION/#training-with-the-command-line-script","text":"Training can also be done using a traditional command-line script. It can be launched from within the \"developer's console\", or from the command line after activating InvokeAI's virtual environment. It accepts a large number of arguments, which can be summarized by passing the --help argument: invokeai-ti --help Typical usage is shown here: invokeai-ti \\ --model = stable-diffusion-1.5 \\ --resolution = 512 \\ --learnable_property = style \\ --initializer_token = '*' \\ --placeholder_token = '<psychedelic>' \\ --train_data_dir = /home/lstein/invokeai/training-data/psychedelic \\ --output_dir = /home/lstein/invokeai/text-inversion-training/psychedelic \\ --scale_lr \\ --train_batch_size = 8 \\ --gradient_accumulation_steps = 4 \\ --max_train_steps = 3000 \\ --learning_rate = 0 .0005 \\ --resume_from_checkpoint = latest \\ --lr_scheduler = constant \\ --mixed_precision = fp16 \\ --only_save_embeds","title":"Training with the Command-Line Script"},{"location":"features/TEXTUAL_INVERSION/#reading","text":"For more information on textual inversion, please see the following resources: The textual inversion repository and associated paper for details and limitations. HuggingFace's textual inversion training page HuggingFace example script documentation (Note that this script is similar to, but not identical, to textual_inversion , but produces embed files that are completely compatible. copyright \u00a9 2023, Lincoln Stein and the InvokeAI Development Team","title":"Reading"},{"location":"features/UNIFIED_CANVAS/","text":"The Unified Canvas is a tool designed to streamline and simplify the process of composing an image using Stable Diffusion. It offers artists all of the available Stable Diffusion generation modes (Text To Image, Image To Image, Inpainting, and Outpainting) as a single unified workflow. The flexibility of the tool allows you to tweak and edit image generations, extend images beyond their initial size, and to create new content in a freeform way both inside and outside of existing images. This document explains the basics of using the Unified Canvas, introducing you to its features and tools one by one. It also describes some of the more advanced tools available to power users of the Canvas. Basics # The Unified Canvas consists of two layers: the Base Layer and the Mask Layer . You can swap from one layer to the other by selecting the layer you want in the drop-down menu on the top left corner of the Unified Canvas, or by pressing the (Q) hotkey. Base Layer # The Base Layer is the image content currently managed by the Canvas, and can be exported at any time to the gallery by using the Save to Gallery option. When the Base Layer is selected, the Brush (B) and Eraser (E) tools will directly manipulate the base layer. Any images uploaded to the Canvas, or sent to the Unified Canvas from the gallery, will clear out all existing content and set the Base layer to the new image. Staging Area # When you generate images, they will display in the Canvas's Staging Area , alongside the Staging Area toolbar buttons. While the Staging Area is active, you cannot interact with the Canvas itself. Accepting generations will commit the new generation to the Base Layer . You can review all generated images using the Prev/Next arrows, save any individual generations to your gallery (without committing to the Base layer) or discard generations. While you can Undo a discard in an individual Canvas session, any generations that are not saved will be lost when the Canvas resets. Mask Layer # The Mask Layer consists of any masked sections that have been created to inform Inpainting generations. You can paint a new mask, or edit an existing mask, using the Brush tool and the Eraser with the Mask layer set as your Active layer. Any masked areas will only affect generation inside of the current bounding box. Bounding Box # When generating a new image, Invoke will process and apply new images within the area denoted by the Bounding Box . The Width & Height settings of the Bounding Box, as well as its location within the Unified Canvas and pixels or empty space that it encloses, determine how new invocations are generated - see Inpainting & Outpainting below. The Bounding Box can be moved and resized using the Move (V) tool. It can also be resized using the Bounding Box options in the Options Panel. By using these controls you can generate larger or smaller images, control which sections of the image are being processed, as well as control Bounding Box tools like the Bounding Box fill/erase. Inpainting & Outpainting # \"Inpainting\" means asking the AI to refine part of an image while leaving the rest alone. For example, updating a portrait of your grandmother to have her wear a biker's jacket. masked original inpaint result \"Outpainting\" means asking the AI to expand the original image beyond its original borders, making a bigger image that's still based on the original. For example, extending the above image of your Grandmother in a biker's jacket to include her wearing jeans (and while we're at it, a motorcycle!) When you are using the Unified Canvas, Invoke decides automatically whether to do Inpainting, Outpainting, ImageToImage, or TextToImage by looking inside the area enclosed by the Bounding Box. It chooses the appropriate type of generation based on whether the Bounding Box contains empty (transparent) areas on the Base layer, or whether it contains colored areas from previous generations (or from painted brushstrokes) on the Base layer, and/or whether the Mask layer contains any brushstrokes. See Generation Methods below for more information. Getting Started # To get started with the Unified Canvas, you will want to generate a new base layer using Txt2Img or importing an initial image. We'll refer to either of these methods as the \"initial image\" in the below guide. From there, you can consider the following techniques to augment your image: New Images : Move the bounding box to an empty area of the Canvas, type in your prompt, and Invoke, to generate a new image using the Text to Image function. Image Correction : Use the color picker and brush tool to paint corrections on the image, switch to the Mask layer, and brush a mask over your painted area to use Inpainting . You can also use the ImageToImage generation method to invoke new interpretations of the image. Image Expansion : Move the bounding box to include a portion of your initial image, and a portion of transparent/empty pixels, then Invoke using a prompt that describes what you'd like to see in that area. This will Outpaint the image. You'll typically find more coherent results if you keep about 50-60% of the original image in the bounding box. Make sure that the Image To Image Strength slider is set to a high value - you may need to set it higher than you are used to. New Content on Existing Images : If you want to add new details or objects into your image, use the brush tool to paint a sketch of what you'd like to see on the image, switch to the Mask layer, and brush a mask over your painted area to use Inpainting . If the masked area is small, consider using a smaller bounding box to take advantage of Invoke's automatic Scaling features, which can help to produce better details. And more : There are a number of creative ways to use the Canvas, and the above are just starting points. We're excited to see what you come up with! Generation Methods # The Canvas can use all generation methods available (Txt2Img, Img2Img, Inpainting, and Outpainting), and these will be automatically selected and used based on the current selection area within the Bounding Box. Text to Image # If the Bounding Box is placed over an area of Canvas with an empty Base Layer , invoking a new image will use TextToImage . This generates an entirely new image based on your prompt. Image to Image # If the Bounding Box is placed over an area of Canvas with an existing Base Layer area with no transparent pixels or masks , invoking a new image will use ImageToImage . This uses the image within the bounding box and your prompt to interpret a new image. The image will be closer to your original image at lower Image to Image strengths. Inpainting # If the Bounding Box is placed over an area of Canvas with an existing Base Layer and any pixels selected using the Mask layer , invoking a new image will use Inpainting . Inpainting uses the existing colors/forms in the masked area in order to generate a new image for the masked area only. The unmasked portion of the image will remain the same. Image to Image strength applies to the inpainted area. If you desire something completely different from the original image in your new generation (i.e., if you want Invoke to ignore existing colors/forms), consider toggling the Inpaint Replace setting on, and use high values for both Inpaint Replace and Image To Image Strength. Note By default, the Scale Before Processing option \u2014 which inpaints more coherent details by generating at a larger resolution and then scaling \u2014 is only activated when the Bounding Box is relatively small. To get the best inpainting results you should therefore resize your Bounding Box to the smallest area that contains your mask and enough surrounding detail to help Stable Diffusion understand the context of what you want it to draw. You should also update your prompt so that it describes just the area within the Bounding Box. Outpainting # If the Bounding Box is placed over an area of Canvas partially filled by an existing Base Layer area and partially by transparent pixels or masks, invoking a new image will use Outpainting , as well as Inpainting any masked areas. Advanced Features # Features with non-obvious behavior are detailed below, in order to provide clarity on the intent and common use cases we expect for utilizing them. Toolbar # Mask Options # Enable Mask - This flag can be used to Enable or Disable the currently painted mask. If you have painted a mask, but you don't want it affect the next invocation, but you also don't want to delete it, then you can set this option to Disable. When you want the mask back, set this back to Enable. Preserve Masked Area - When enabled, Preserve Masked Area inverts the effect of the Mask on the Inpainting process. Pixels in masked areas will be kept unchanged, and unmasked areas will be regenerated. Creative Tools # Brush - Base/Mask Modes - The Brush tool switches automatically between different modes of operation for the Base and Mask layers respectively. On the Base layer, the brush will directly paint on the Canvas using the color selected on the Brush Options menu. On the Mask layer, the brush will create a new mask. If you're finding the mask difficult to see over the existing content of the Unified Canvas, you can change the color it is drawn with using the color selector on the Mask Options dropdown. Erase Bounding Box - On the Base layer, erases all pixels within the Bounding Box. Fill Bounding Box - On the Base layer, fills all pixels within the Bounding Box with the currently selected color. Canvas Tools # Move Tool - Allows for manipulation of the Canvas view (by dragging on the Canvas, outside the bounding box), the Bounding Box (by dragging the edges of the box), or the Width/Height of the Bounding Box (by dragging one of the 9 directional handles). Reset View - Click to re-orients the view to the center of the Bounding Box. Merge Visible - If your browser is having performance problems drawing the image in the Unified Canvas, click this to consolidate all of the information currently being rendered by your browser into a merged copy of the image. This lowers the resource requirements and should improve performance. Seam Correction # When doing Inpainting or Outpainting, Invoke needs to merge the pixels generated by Stable Diffusion into your existing image. To do this, the area around the seam at the boundary between your image and the new generation is automatically blended to produce a seamless output. In a fully automatic process, a mask is generated to cover the seam, and then the area of the seam is Inpainted. Although the default options should work well most of the time, sometimes it can help to alter the parameters that control the seam Inpainting. A wider seam and a blur setting of about \u2153 of the seam have been noted as producing consistently strong results (e.g. 96 wide and 16 blur - adds up to 32 blur with both sides). Seam strength of 0.7 is best for reducing hard seams. Seam Size - The size of the seam masked area. Set higher to make a larger mask around the seam. Seam Blur - The size of the blur that is applied on each side of the masked area. Seam Strength - The Image To Image Strength parameter used for the Inpainting generation that is applied to the seam area. Seam Steps - The number of generation steps that should be used to Inpaint the seam. Infill & Scaling # Scale Before Processing & W/H : When generating images with a bounding box smaller than the optimized W/H of the model (e.g., 512x512 for SD1.5), this feature first generates at a larger size with the same aspect ratio, and then scales that image down to fill the selected area. This is particularly useful when inpainting very small details. Scaling is optional but is enabled by default. Inpaint Replace : When Inpainting, the default method is to utilize the existing RGB values of the Base layer to inform the generation process. If Inpaint Replace is enabled, noise is generated and blended with the existing pixels (completely replacing the original RGB values at an Inpaint Replace value of 1). This can help generate more variation from the pixels on the Base layers. When using Inpaint Replace you should use a higher Image To Image Strength value, especially at higher Inpaint Replace values Infill Method : Invoke currently supports two methods for producing RGB values for use in the Outpainting process: Patchmatch and Tile. We believe that Patchmatch is the superior method, however we provide support for Tile in case Patchmatch cannot be installed or is unavailable on your computer. Tile Size : The Tile method for Outpainting sources small portions of the original image and randomly place these into the areas being Outpainted. This value sets the size of those tiles. Hot Keys # The Unified Canvas is a tool that excels when you use hotkeys. You can view the full list of keyboard shortcuts, updated with all new features, by clicking the Keyboard Shortcuts icon at the top right of the InvokeAI WebUI.","title":"Unified Canvas"},{"location":"features/UNIFIED_CANVAS/#basics","text":"The Unified Canvas consists of two layers: the Base Layer and the Mask Layer . You can swap from one layer to the other by selecting the layer you want in the drop-down menu on the top left corner of the Unified Canvas, or by pressing the (Q) hotkey.","title":"Basics"},{"location":"features/UNIFIED_CANVAS/#base-layer","text":"The Base Layer is the image content currently managed by the Canvas, and can be exported at any time to the gallery by using the Save to Gallery option. When the Base Layer is selected, the Brush (B) and Eraser (E) tools will directly manipulate the base layer. Any images uploaded to the Canvas, or sent to the Unified Canvas from the gallery, will clear out all existing content and set the Base layer to the new image.","title":"Base Layer"},{"location":"features/UNIFIED_CANVAS/#staging-area","text":"When you generate images, they will display in the Canvas's Staging Area , alongside the Staging Area toolbar buttons. While the Staging Area is active, you cannot interact with the Canvas itself. Accepting generations will commit the new generation to the Base Layer . You can review all generated images using the Prev/Next arrows, save any individual generations to your gallery (without committing to the Base layer) or discard generations. While you can Undo a discard in an individual Canvas session, any generations that are not saved will be lost when the Canvas resets.","title":"Staging Area"},{"location":"features/UNIFIED_CANVAS/#mask-layer","text":"The Mask Layer consists of any masked sections that have been created to inform Inpainting generations. You can paint a new mask, or edit an existing mask, using the Brush tool and the Eraser with the Mask layer set as your Active layer. Any masked areas will only affect generation inside of the current bounding box.","title":"Mask Layer"},{"location":"features/UNIFIED_CANVAS/#bounding-box","text":"When generating a new image, Invoke will process and apply new images within the area denoted by the Bounding Box . The Width & Height settings of the Bounding Box, as well as its location within the Unified Canvas and pixels or empty space that it encloses, determine how new invocations are generated - see Inpainting & Outpainting below. The Bounding Box can be moved and resized using the Move (V) tool. It can also be resized using the Bounding Box options in the Options Panel. By using these controls you can generate larger or smaller images, control which sections of the image are being processed, as well as control Bounding Box tools like the Bounding Box fill/erase.","title":"Bounding Box"},{"location":"features/UNIFIED_CANVAS/#inpainting-outpainting","text":"\"Inpainting\" means asking the AI to refine part of an image while leaving the rest alone. For example, updating a portrait of your grandmother to have her wear a biker's jacket. masked original inpaint result \"Outpainting\" means asking the AI to expand the original image beyond its original borders, making a bigger image that's still based on the original. For example, extending the above image of your Grandmother in a biker's jacket to include her wearing jeans (and while we're at it, a motorcycle!) When you are using the Unified Canvas, Invoke decides automatically whether to do Inpainting, Outpainting, ImageToImage, or TextToImage by looking inside the area enclosed by the Bounding Box. It chooses the appropriate type of generation based on whether the Bounding Box contains empty (transparent) areas on the Base layer, or whether it contains colored areas from previous generations (or from painted brushstrokes) on the Base layer, and/or whether the Mask layer contains any brushstrokes. See Generation Methods below for more information.","title":" Inpainting &amp; Outpainting"},{"location":"features/UNIFIED_CANVAS/#getting-started","text":"To get started with the Unified Canvas, you will want to generate a new base layer using Txt2Img or importing an initial image. We'll refer to either of these methods as the \"initial image\" in the below guide. From there, you can consider the following techniques to augment your image: New Images : Move the bounding box to an empty area of the Canvas, type in your prompt, and Invoke, to generate a new image using the Text to Image function. Image Correction : Use the color picker and brush tool to paint corrections on the image, switch to the Mask layer, and brush a mask over your painted area to use Inpainting . You can also use the ImageToImage generation method to invoke new interpretations of the image. Image Expansion : Move the bounding box to include a portion of your initial image, and a portion of transparent/empty pixels, then Invoke using a prompt that describes what you'd like to see in that area. This will Outpaint the image. You'll typically find more coherent results if you keep about 50-60% of the original image in the bounding box. Make sure that the Image To Image Strength slider is set to a high value - you may need to set it higher than you are used to. New Content on Existing Images : If you want to add new details or objects into your image, use the brush tool to paint a sketch of what you'd like to see on the image, switch to the Mask layer, and brush a mask over your painted area to use Inpainting . If the masked area is small, consider using a smaller bounding box to take advantage of Invoke's automatic Scaling features, which can help to produce better details. And more : There are a number of creative ways to use the Canvas, and the above are just starting points. We're excited to see what you come up with!","title":"Getting Started"},{"location":"features/UNIFIED_CANVAS/#generation-methods","text":"The Canvas can use all generation methods available (Txt2Img, Img2Img, Inpainting, and Outpainting), and these will be automatically selected and used based on the current selection area within the Bounding Box.","title":" Generation Methods"},{"location":"features/UNIFIED_CANVAS/#text-to-image","text":"If the Bounding Box is placed over an area of Canvas with an empty Base Layer , invoking a new image will use TextToImage . This generates an entirely new image based on your prompt.","title":"Text to Image"},{"location":"features/UNIFIED_CANVAS/#image-to-image","text":"If the Bounding Box is placed over an area of Canvas with an existing Base Layer area with no transparent pixels or masks , invoking a new image will use ImageToImage . This uses the image within the bounding box and your prompt to interpret a new image. The image will be closer to your original image at lower Image to Image strengths.","title":"Image to Image"},{"location":"features/UNIFIED_CANVAS/#inpainting","text":"If the Bounding Box is placed over an area of Canvas with an existing Base Layer and any pixels selected using the Mask layer , invoking a new image will use Inpainting . Inpainting uses the existing colors/forms in the masked area in order to generate a new image for the masked area only. The unmasked portion of the image will remain the same. Image to Image strength applies to the inpainted area. If you desire something completely different from the original image in your new generation (i.e., if you want Invoke to ignore existing colors/forms), consider toggling the Inpaint Replace setting on, and use high values for both Inpaint Replace and Image To Image Strength. Note By default, the Scale Before Processing option \u2014 which inpaints more coherent details by generating at a larger resolution and then scaling \u2014 is only activated when the Bounding Box is relatively small. To get the best inpainting results you should therefore resize your Bounding Box to the smallest area that contains your mask and enough surrounding detail to help Stable Diffusion understand the context of what you want it to draw. You should also update your prompt so that it describes just the area within the Bounding Box.","title":"Inpainting"},{"location":"features/UNIFIED_CANVAS/#outpainting","text":"If the Bounding Box is placed over an area of Canvas partially filled by an existing Base Layer area and partially by transparent pixels or masks, invoking a new image will use Outpainting , as well as Inpainting any masked areas.","title":"Outpainting"},{"location":"features/UNIFIED_CANVAS/#advanced-features","text":"Features with non-obvious behavior are detailed below, in order to provide clarity on the intent and common use cases we expect for utilizing them.","title":"Advanced Features"},{"location":"features/UNIFIED_CANVAS/#toolbar","text":"","title":"Toolbar"},{"location":"features/UNIFIED_CANVAS/#mask-options","text":"Enable Mask - This flag can be used to Enable or Disable the currently painted mask. If you have painted a mask, but you don't want it affect the next invocation, but you also don't want to delete it, then you can set this option to Disable. When you want the mask back, set this back to Enable. Preserve Masked Area - When enabled, Preserve Masked Area inverts the effect of the Mask on the Inpainting process. Pixels in masked areas will be kept unchanged, and unmasked areas will be regenerated.","title":"Mask Options"},{"location":"features/UNIFIED_CANVAS/#creative-tools","text":"Brush - Base/Mask Modes - The Brush tool switches automatically between different modes of operation for the Base and Mask layers respectively. On the Base layer, the brush will directly paint on the Canvas using the color selected on the Brush Options menu. On the Mask layer, the brush will create a new mask. If you're finding the mask difficult to see over the existing content of the Unified Canvas, you can change the color it is drawn with using the color selector on the Mask Options dropdown. Erase Bounding Box - On the Base layer, erases all pixels within the Bounding Box. Fill Bounding Box - On the Base layer, fills all pixels within the Bounding Box with the currently selected color.","title":"Creative Tools"},{"location":"features/UNIFIED_CANVAS/#canvas-tools","text":"Move Tool - Allows for manipulation of the Canvas view (by dragging on the Canvas, outside the bounding box), the Bounding Box (by dragging the edges of the box), or the Width/Height of the Bounding Box (by dragging one of the 9 directional handles). Reset View - Click to re-orients the view to the center of the Bounding Box. Merge Visible - If your browser is having performance problems drawing the image in the Unified Canvas, click this to consolidate all of the information currently being rendered by your browser into a merged copy of the image. This lowers the resource requirements and should improve performance.","title":"Canvas Tools"},{"location":"features/UNIFIED_CANVAS/#seam-correction","text":"When doing Inpainting or Outpainting, Invoke needs to merge the pixels generated by Stable Diffusion into your existing image. To do this, the area around the seam at the boundary between your image and the new generation is automatically blended to produce a seamless output. In a fully automatic process, a mask is generated to cover the seam, and then the area of the seam is Inpainted. Although the default options should work well most of the time, sometimes it can help to alter the parameters that control the seam Inpainting. A wider seam and a blur setting of about \u2153 of the seam have been noted as producing consistently strong results (e.g. 96 wide and 16 blur - adds up to 32 blur with both sides). Seam strength of 0.7 is best for reducing hard seams. Seam Size - The size of the seam masked area. Set higher to make a larger mask around the seam. Seam Blur - The size of the blur that is applied on each side of the masked area. Seam Strength - The Image To Image Strength parameter used for the Inpainting generation that is applied to the seam area. Seam Steps - The number of generation steps that should be used to Inpaint the seam.","title":"Seam Correction"},{"location":"features/UNIFIED_CANVAS/#infill-scaling","text":"Scale Before Processing & W/H : When generating images with a bounding box smaller than the optimized W/H of the model (e.g., 512x512 for SD1.5), this feature first generates at a larger size with the same aspect ratio, and then scales that image down to fill the selected area. This is particularly useful when inpainting very small details. Scaling is optional but is enabled by default. Inpaint Replace : When Inpainting, the default method is to utilize the existing RGB values of the Base layer to inform the generation process. If Inpaint Replace is enabled, noise is generated and blended with the existing pixels (completely replacing the original RGB values at an Inpaint Replace value of 1). This can help generate more variation from the pixels on the Base layers. When using Inpaint Replace you should use a higher Image To Image Strength value, especially at higher Inpaint Replace values Infill Method : Invoke currently supports two methods for producing RGB values for use in the Outpainting process: Patchmatch and Tile. We believe that Patchmatch is the superior method, however we provide support for Tile in case Patchmatch cannot be installed or is unavailable on your computer. Tile Size : The Tile method for Outpainting sources small portions of the original image and randomly place these into the areas being Outpainted. This value sets the size of those tiles.","title":"Infill &amp; Scaling"},{"location":"features/UNIFIED_CANVAS/#hot-keys","text":"The Unified Canvas is a tool that excels when you use hotkeys. You can view the full list of keyboard shortcuts, updated with all new features, by clicking the Keyboard Shortcuts icon at the top right of the InvokeAI WebUI.","title":"Hot Keys"},{"location":"features/VARIATIONS/","text":"Variations # Intro # Release 1.13 of SD-Dream adds support for image variations. You are able to do the following: Generate a series of systematic variations of an image, given a prompt. The amount of variation from one image to the next can be controlled. Given two or more variations that you like, you can combine them in a weighted fashion. This cheat sheet provides a quick guide for how this works in practice, using variations to create the desired image of Xena, Warrior Princess. Step 1 -- Find a base image that you like # The prompt we will use throughout is: \"lucy lawless as xena, warrior princess, character portrait, high resolution.\" This will be indicated as \"prompt\" in the examples below. First we let SD create a series of images in the usual way, in this case requesting six iterations: invoke> lucy lawless as xena, warrior princess, character portrait, high resolution -n6 ... Outputs: ./outputs/Xena/000001.1579445059.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S1579445059 ./outputs/Xena/000001.1880768722.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S1880768722 ./outputs/Xena/000001.332057179.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S332057179 ./outputs/Xena/000001.2224800325.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S2224800325 ./outputs/Xena/000001.465250761.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S465250761 ./outputs/Xena/000001.3357757885.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S3357757885 Seed 3357757885 looks nice Step 2 - Generating Variations # Let's try to generate some variations. Using the same seed, we pass the argument -v0.1 (or --variant_amount), which generates a series of variations each differing by a variation amount of 0.2. This number ranges from 0 to 1.0 , with higher numbers being larger amounts of variation. invoke> \"prompt\" -n6 -S3357757885 -v0.2 ... Outputs: ./outputs/Xena/000002.784039624.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 784039624 :0.2 -S3357757885 ./outputs/Xena/000002.3647897225.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.2 -S3357757885 ./outputs/Xena/000002.917731034.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 917731034 :0.2 -S3357757885 ./outputs/Xena/000002.4116285959.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 4116285959 :0.2 -S3357757885 ./outputs/Xena/000002.1614299449.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 1614299449 :0.2 -S3357757885 ./outputs/Xena/000002.1335553075.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 1335553075 :0.2 -S3357757885 Variation Sub Seeding # Note that the output for each image has a -V option giving the \"variant subseed\" for that image, consisting of a seed followed by the variation amount used to generate it. This gives us a series of closely-related variations, including the two shown here. subseed 3647897225 subseed 1614299449 I like the expression on Xena's face in the first one (subseed 3647897225), and the armor on her shoulder in the second one (subseed 1614299449). Can we combine them to get the best of both worlds? We combine the two variations using -V ( --with_variations ). Again, we must provide the seed for the originally-chosen image in order for this to work. invoke> \"prompt\" -S3357757885 -V3647897225,0.1,1614299449,0.1 Outputs: ./outputs/Xena/000003.1614299449.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1 -S3357757885 Here we are providing equal weights (0.1 and 0.1) for both the subseeds. The resulting image is close, but not exactly what I wanted: subseed 1614299449 We could either try combining the images with different weights, or we can generate more variations around the almost-but-not-quite image. We do the latter, using both the -V (combining) and -v (variation strength) options. Note that we use -n6 to generate 6 variations: invoke> \"prompt\" -S3357757885 -V3647897225,0.1,1614299449,0.1 -v0.05 -n6 Outputs: ./outputs/Xena/000004.3279757577.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,3279757577:0.05 -S3357757885 ./outputs/Xena/000004.2853129515.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,2853129515:0.05 -S3357757885 ./outputs/Xena/000004.3747154981.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,3747154981:0.05 -S3357757885 ./outputs/Xena/000004.2664260391.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,2664260391:0.05 -S3357757885 ./outputs/Xena/000004.1642517170.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,1642517170:0.05 -S3357757885 ./outputs/Xena/000004.2183375608.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,2183375608:0.05 -S3357757885 This produces six images, all slight variations on the combination of the chosen two images. Here's the one I like best: subseed 3747154981 As you can see, this is a very powerful tool, which when combined with subprompt weighting, gives you great control over the content and quality of your generated images. Variations and Samplers # The sampler you choose has a strong effect on variation strength. Some samplers, such as k_euler_a are very \"creative\" and produce significant amounts of image-to-image variation even when the seed is fixed and the -v argument is very low. Others are more deterministic. Feel free to experiment until you find the combination that you like. Also be aware of the Perlin Noise feature, which provides another way of introducing variability into your image generation requests.","title":"Variations"},{"location":"features/VARIATIONS/#variations","text":"","title":" Variations"},{"location":"features/VARIATIONS/#intro","text":"Release 1.13 of SD-Dream adds support for image variations. You are able to do the following: Generate a series of systematic variations of an image, given a prompt. The amount of variation from one image to the next can be controlled. Given two or more variations that you like, you can combine them in a weighted fashion. This cheat sheet provides a quick guide for how this works in practice, using variations to create the desired image of Xena, Warrior Princess.","title":"Intro"},{"location":"features/VARIATIONS/#step-1-find-a-base-image-that-you-like","text":"The prompt we will use throughout is: \"lucy lawless as xena, warrior princess, character portrait, high resolution.\" This will be indicated as \"prompt\" in the examples below. First we let SD create a series of images in the usual way, in this case requesting six iterations: invoke> lucy lawless as xena, warrior princess, character portrait, high resolution -n6 ... Outputs: ./outputs/Xena/000001.1579445059.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S1579445059 ./outputs/Xena/000001.1880768722.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S1880768722 ./outputs/Xena/000001.332057179.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S332057179 ./outputs/Xena/000001.2224800325.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S2224800325 ./outputs/Xena/000001.465250761.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S465250761 ./outputs/Xena/000001.3357757885.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -S3357757885 Seed 3357757885 looks nice","title":"Step 1 -- Find a base image that you like"},{"location":"features/VARIATIONS/#step-2-generating-variations","text":"Let's try to generate some variations. Using the same seed, we pass the argument -v0.1 (or --variant_amount), which generates a series of variations each differing by a variation amount of 0.2. This number ranges from 0 to 1.0 , with higher numbers being larger amounts of variation. invoke> \"prompt\" -n6 -S3357757885 -v0.2 ... Outputs: ./outputs/Xena/000002.784039624.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 784039624 :0.2 -S3357757885 ./outputs/Xena/000002.3647897225.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.2 -S3357757885 ./outputs/Xena/000002.917731034.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 917731034 :0.2 -S3357757885 ./outputs/Xena/000002.4116285959.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 4116285959 :0.2 -S3357757885 ./outputs/Xena/000002.1614299449.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 1614299449 :0.2 -S3357757885 ./outputs/Xena/000002.1335553075.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 1335553075 :0.2 -S3357757885","title":"Step 2 - Generating Variations"},{"location":"features/VARIATIONS/#variation-sub-seeding","text":"Note that the output for each image has a -V option giving the \"variant subseed\" for that image, consisting of a seed followed by the variation amount used to generate it. This gives us a series of closely-related variations, including the two shown here. subseed 3647897225 subseed 1614299449 I like the expression on Xena's face in the first one (subseed 3647897225), and the armor on her shoulder in the second one (subseed 1614299449). Can we combine them to get the best of both worlds? We combine the two variations using -V ( --with_variations ). Again, we must provide the seed for the originally-chosen image in order for this to work. invoke> \"prompt\" -S3357757885 -V3647897225,0.1,1614299449,0.1 Outputs: ./outputs/Xena/000003.1614299449.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1 -S3357757885 Here we are providing equal weights (0.1 and 0.1) for both the subseeds. The resulting image is close, but not exactly what I wanted: subseed 1614299449 We could either try combining the images with different weights, or we can generate more variations around the almost-but-not-quite image. We do the latter, using both the -V (combining) and -v (variation strength) options. Note that we use -n6 to generate 6 variations: invoke> \"prompt\" -S3357757885 -V3647897225,0.1,1614299449,0.1 -v0.05 -n6 Outputs: ./outputs/Xena/000004.3279757577.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,3279757577:0.05 -S3357757885 ./outputs/Xena/000004.2853129515.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,2853129515:0.05 -S3357757885 ./outputs/Xena/000004.3747154981.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,3747154981:0.05 -S3357757885 ./outputs/Xena/000004.2664260391.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,2664260391:0.05 -S3357757885 ./outputs/Xena/000004.1642517170.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,1642517170:0.05 -S3357757885 ./outputs/Xena/000004.2183375608.png: \"prompt\" -s50 -W512 -H512 -C7.5 -Ak_lms -V 3647897225 :0.1,1614299449:0.1,2183375608:0.05 -S3357757885 This produces six images, all slight variations on the combination of the chosen two images. Here's the one I like best: subseed 3747154981 As you can see, this is a very powerful tool, which when combined with subprompt weighting, gives you great control over the content and quality of your generated images.","title":"Variation Sub Seeding"},{"location":"features/VARIATIONS/#variations-and-samplers","text":"The sampler you choose has a strong effect on variation strength. Some samplers, such as k_euler_a are very \"creative\" and produce significant amounts of image-to-image variation even when the seed is fixed and the -v argument is very low. Others are more deterministic. Feel free to experiment until you find the combination that you like. Also be aware of the Perlin Noise feature, which provides another way of introducing variability into your image generation requests.","title":"Variations and Samplers"},{"location":"features/WEB/","text":"InvokeAI Web Server # As of version 2.0.0, this distribution comes with a full-featured web server (see screenshot). To use it, launch the invoke.sh / invoke.bat script and select option (2). Alternatively, with the InvokeAI environment active, run the invokeai script by adding the --web option: invokeai --web You can then connect to the server by pointing your web browser at http://localhost:9090 . To reach the server from a different machine on your LAN, you may launch the web server with the --host argument and either the IP address of the host you are running it on, or the wildcard 0.0.0.0 . For example: invoke.sh --host 0 .0.0.0 or invokeai --web --host 0 .0.0.0 Quick guided walkthrough of the WebUI's features # While most of the WebUI's features are intuitive, here is a guided walkthrough through its various components. The screenshot above shows the Text to Image tab of the WebUI. There are three main sections: A control panel on the left, which contains various settings for text to image generation. The most important part is the text field (currently showing strawberry sushi ) for entering the text prompt, and the camera icon directly underneath that will render the image. We'll call this the Invoke button from now on. The current image section in the middle, which shows a large format version of the image you are currently working on. A series of buttons at the top (\"image to image\", \"Use All\", \"Use Seed\", etc) lets you modify the image in various ways. A * gallery section on the left that contains a history of the images you have generated. These images are read and written to the directory specified at launch time in --outdir . In addition to these three elements, there are a series of icons for changing global settings, reporting bugs, and changing the theme on the upper right. There are also a series of icons to the left of the control panel (see highlighted area in the screenshot below) which select among a series of tabs for performing different types of operations. From top to bottom, these are: Text to Image - generate images from text Image to Image - from an uploaded starting image (drawing or photograph) generate a new one, modified by the text prompt Unified Canvas - Interactively combine multiple images, extend them with outpainting,and modify interior portions of the image with inpainting, erase portions of a starting image and have the AI fill in the erased region from a text prompt. Workflow Management (not yet implemented) - this panel will allow you to create pipelines of common operations and combine them into workflows. Training (not yet implemented) - this panel will provide an interface to textual inversion training and fine tuning. The inpainting, outpainting and postprocessing tabs are currently in development. However, limited versions of their features can already be accessed through the Text to Image and Image to Image tabs. Walkthrough # The following walkthrough will exercise most (but not all) of the WebUI's feature set. Text to Image # Launch the WebUI using python scripts/invoke.py --web and connect to it with your browser by accessing http://localhost:9090 . If the browser and server are running on different machines on your LAN, add the option --host 0.0.0.0 to the launch command line and connect to the machine hosting the web server using its IP address or domain name. If all goes well, the WebUI should come up and you'll see a green connected message on the upper right. Basics # Generate an image by typing strawberry sushi into the large prompt field on the upper left and then clicking on the Invoke button (the one with the Camera icon). After a short wait, you'll see a large image of sushi in the image panel, and a new thumbnail in the gallery on the right. If you need more room on the screen, you can turn the gallery off by clicking on the x to the right of \"Your Invocations\". You can turn it back on later by clicking the image icon that appears in the gallery's place. The images are written into the directory indicated by the --outdir option provided at script launch time. By default, this is outputs/img-samples under the InvokeAI directory. Generate a bunch of strawberry sushi images by increasing the number of requested images by adjusting the Images counter just below the Camera button. As each is generated, it will be added to the gallery. You can switch the active image by clicking on the gallery thumbnails. Try playing with different settings, including image width and height, the Sampler, the Steps and the CFG scale. Image Width and Height do what you'd expect. However, be aware that larger images consume more VRAM memory and take longer to generate. The Sampler controls how the AI selects the image to display. Some samplers are more \"creative\" than others and will produce a wider range of variations (see next section). Some samplers run faster than others. Steps controls how many noising/denoising/sampling steps the AI will take. The higher this value, the more refined the image will be, but the longer the image will take to generate. A typical strategy is to generate images with a low number of steps in order to select one to work on further, and then regenerate it using a higher number of steps. The CFG Scale controls how hard the AI tries to match the generated image to the input prompt. You can go as high or low as you like, but generally values greater than 20 won't improve things much, and values lower than 5 will produce unexpected images. There are complex interactions between Steps , CFG Scale and the Sampler , so experiment to find out what works for you. To regenerate a previously-generated image, select the image you want and click Use All . This loads the text prompt and other original settings into the control panel. If you then press Invoke it will regenerate the image exactly. You can also selectively modify the prompt or other settings to tweak the image. Alternatively, you may click on Use Seed to load just the image's seed, and leave other settings unchanged. To regenerate a Stable Diffusion image that was generated by another SD package, you need to know its text prompt and its Seed . Copy-paste the prompt into the prompt box, unset the Randomize Seed control in the control panel, and copy-paste the desired Seed into its text field. When you Invoke, you will get something similar to the original image. It will not be exact unless you also set the correct values for the original sampler, CFG, steps and dimensions, but it will (usually) be close. Variations on a theme # Let's try generating some variations. Select your favorite sushi image from the gallery to load it. Then select \"Use All\" from the list of buttons above. This will load up all the settings used to generate this image, including its unique seed. Go down to the Variations section of the Control Panel and set the button to On. Set Variation Amount to 0.2 to generate a modest number of variations on the image, and also set the Image counter to 4 . Press the invoke button. This will generate a series of related images. To obtain smaller variations, just lower the Variation Amount. You may also experiment with changing the Sampler. Some samplers generate more variability than others. k_euler_a is particularly creative, while ddim is pretty conservative. For even more variations, experiment with increasing the setting for Perlin . This adds a bit of noise to the image generation process. Note that values of Perlin noise greater than 0.15 produce poor images for several of the samplers. Facial reconstruction and upscaling # Stable Diffusion frequently produces mangled faces, particularly when there are multiple figures in the same scene. Stable Diffusion has particular issues with generating reallistic eyes. InvokeAI provides the ability to reconstruct faces using either the GFPGAN or CodeFormer libraries. For more information see POSTPROCESS . Invoke a prompt that generates a mangled face. A prompt that often gives this is \"portrait of a lawyer, \u00be shot\" (this is not intended as a slur against lawyers!) Once you have an image that needs some touching up, load it into the Image panel, and press the button with the face icon (highlighted in the first screenshot below). A dialog box will appear. Leave Strength at 0.8 and press *Restore Faces\". If all goes well, the eyes and other aspects of the face will be improved (see the second screenshot) The facial reconstruction Strength field adjusts how aggressively the face library will try to alter the face. It can be as high as 1.0, but be aware that this often softens the face airbrush style, losing some details. The default 0.8 is usually sufficient. \"Upscaling\" is the process of increasing the size of an image while retaining the sharpness. InvokeAI uses an external library called \"ESRGAN\" to do this. To invoke upscaling, simply select an image and press the HD button above it. You can select between 2X and 4X upscaling, and adjust the upscaling strength, which has much the same meaning as in facial reconstruction. Try running this on one of your previously-generated images. Finally, you can run facial reconstruction and/or upscaling automatically after each Invocation. Go to the Advanced Options section of the Control Panel and turn on Restore Face and/or Upscale . Image to Image # InvokeAI lets you take an existing image and use it as the basis for a new creation. You can use any sort of image, including a photograph, a scanned sketch, or a digital drawing, as long as it is in PNG or JPEG format. For this tutorial, we'll use files named Lincoln-and-Parrot-512.png , and Lincoln-and-Parrot-512-transparent.png . Download these images to your local machine now to continue with the walkthrough. Click on the Image to Image tab icon, which is the second icon from the top on the left-hand side of the screen: This will bring you to a screen similar to the one shown here: Drag-and-drop the Lincoln-and-Parrot image into the Image panel, or click the blank area to get an upload dialog. The image will load into an area marked Initial Image . (The WebUI will also load the most recently-generated image from the gallery into a section on the left, but this image will be replaced in the next step.) Go to the prompt box and type old sea captain with raven on shoulder and press Invoke. A derived image will appear to the right of the original one: Experiment with the different settings. The most influential one in Image to Image is Image to Image Strength located about midway down the control panel. By default it is set to 0.75, but can range from 0.0 to 0.99. The higher the value, the more of the original image the AI will replace. A value of 0 will leave the initial image completely unchanged, while 0.99 will replace it completely. However, the Sampler and CFG Scale also influence the final result. You can also generate variations in the same way as described in Text to Image. What if we only want to change certain part(s) of the image and leave the rest intact? This is called Inpainting, and a future version of the InvokeAI web server will provide an interactive painting canvas on which you can directly draw the areas you wish to Inpaint into. For now, you can achieve this effect by using an external photoeditor tool to make one or more regions of the image transparent as described in [INPAINTING.md] and uploading that. The file Lincoln-and-Parrot-512-transparent.png is a version of the earlier image in which the area around the parrot has been replaced with transparency. Click on the \"x\" in the upper right of the Initial Image and upload the transparent version. Using the same prompt \"old sea captain with raven on shoulder\" try Invoking an image. This time, only the parrot will be replaced, leaving the rest of the original image intact: Would you like to modify a previously-generated image using the Image to Image facility? Easy! While in the Image to Image panel, hover over any of the gallery images to see a little menu of icons pop up. Click the picture icon to instantly send the selected image to Image to Image as the initial image. You can do the same from the Text to Image tab by clicking on the picture icon above the central image panel. The screenshot below shows where the \"use as initial image\" icons are located. Unified Canvas # See the Unified Canvas Guide Parting remarks # This concludes the walkthrough, but there are several more features that you can explore. Please check out the Command Line Interface documentation for further explanation of the advanced features that were not covered here. The WebUI is only rapid development. Check back regularly for updates! Reference # Additional Options # parameter effect --web_develop Starts the web server in development mode. --web_verbose Enables verbose logging --cors [CORS ...] Additional allowed origins, comma-separated --host HOST Web server: Host or IP to listen on. Set to 0.0.0.0 to accept traffic from other devices on your network. --port PORT Web server: Port to listen on --certfile CERTFILE Web server: Path to certificate file to use for SSL. Use together with --keyfile --keyfile KEYFILE Web server: Path to private key file to use for SSL. Use together with --certfile' --gui Start InvokeAI GUI - This is the \"desktop mode\" version of the web app. It uses Flask to create a desktop app experience of the webserver. Web Specific Features # The web experience offers an incredibly easy-to-use experience for interacting with the InvokeAI toolkit. For detailed guidance on individual features, see the Feature-specific help documents available in this directory. Note that the latest functionality available in the CLI may not always be available in the Web interface. Dark Mode & Light Mode # The InvokeAI interface is available in a nano-carbon black & purple Dark Mode, and a \"burn your eyes out Nosferatu\" Light Mode. These can be toggled by clicking the Sun/Moon icons at the top right of the interface. Invocation Toolbar # The left side of the InvokeAI interface is available for customizing the prompt and the settings used for invoking your new image. Typing your prompt into the open text field and clicking the Invoke button will produce the image based on the settings configured in the toolbar. See below for additional documentation related to each feature: Core Prompt Settings Variations Upscaling Image to Image Inpainting Other Invocation Gallery # The currently selected --outdir (or the default outputs folder) will display all previously generated files on load. As new invocations are generated, these will be dynamically added to the gallery, and can be previewed by selecting them. Each image also has a simple set of actions (e.g., Delete, Use Seed, Use All Parameters, etc.) that can be accessed by hovering over the image. Image Workspace # When an image from the Invocation Gallery is selected, or is generated, the image will be displayed within the center of the interface. A quickbar of common image interactions are displayed along the top of the image, including: Use image in the Image to Image workflow Initialize Face Restoration on the selected file Initialize Upscaling on the selected file View File metadata and details Delete the file Acknowledgements # A huge shout-out to the core team working to make this vision a reality, including psychedelicious , Kyle0654 and blessedcoolant . hipsterusername was the team's unofficial cheerleader and added tooltips/docs.","title":"InvokeAI Web Server"},{"location":"features/WEB/#invokeai-web-server","text":"As of version 2.0.0, this distribution comes with a full-featured web server (see screenshot). To use it, launch the invoke.sh / invoke.bat script and select option (2). Alternatively, with the InvokeAI environment active, run the invokeai script by adding the --web option: invokeai --web You can then connect to the server by pointing your web browser at http://localhost:9090 . To reach the server from a different machine on your LAN, you may launch the web server with the --host argument and either the IP address of the host you are running it on, or the wildcard 0.0.0.0 . For example: invoke.sh --host 0 .0.0.0 or invokeai --web --host 0 .0.0.0","title":" InvokeAI Web Server"},{"location":"features/WEB/#quick-guided-walkthrough-of-the-webuis-features","text":"While most of the WebUI's features are intuitive, here is a guided walkthrough through its various components. The screenshot above shows the Text to Image tab of the WebUI. There are three main sections: A control panel on the left, which contains various settings for text to image generation. The most important part is the text field (currently showing strawberry sushi ) for entering the text prompt, and the camera icon directly underneath that will render the image. We'll call this the Invoke button from now on. The current image section in the middle, which shows a large format version of the image you are currently working on. A series of buttons at the top (\"image to image\", \"Use All\", \"Use Seed\", etc) lets you modify the image in various ways. A * gallery section on the left that contains a history of the images you have generated. These images are read and written to the directory specified at launch time in --outdir . In addition to these three elements, there are a series of icons for changing global settings, reporting bugs, and changing the theme on the upper right. There are also a series of icons to the left of the control panel (see highlighted area in the screenshot below) which select among a series of tabs for performing different types of operations. From top to bottom, these are: Text to Image - generate images from text Image to Image - from an uploaded starting image (drawing or photograph) generate a new one, modified by the text prompt Unified Canvas - Interactively combine multiple images, extend them with outpainting,and modify interior portions of the image with inpainting, erase portions of a starting image and have the AI fill in the erased region from a text prompt. Workflow Management (not yet implemented) - this panel will allow you to create pipelines of common operations and combine them into workflows. Training (not yet implemented) - this panel will provide an interface to textual inversion training and fine tuning. The inpainting, outpainting and postprocessing tabs are currently in development. However, limited versions of their features can already be accessed through the Text to Image and Image to Image tabs.","title":"Quick guided walkthrough of the WebUI's features"},{"location":"features/WEB/#walkthrough","text":"The following walkthrough will exercise most (but not all) of the WebUI's feature set.","title":"Walkthrough"},{"location":"features/WEB/#text-to-image","text":"Launch the WebUI using python scripts/invoke.py --web and connect to it with your browser by accessing http://localhost:9090 . If the browser and server are running on different machines on your LAN, add the option --host 0.0.0.0 to the launch command line and connect to the machine hosting the web server using its IP address or domain name. If all goes well, the WebUI should come up and you'll see a green connected message on the upper right.","title":"Text to Image"},{"location":"features/WEB/#basics","text":"Generate an image by typing strawberry sushi into the large prompt field on the upper left and then clicking on the Invoke button (the one with the Camera icon). After a short wait, you'll see a large image of sushi in the image panel, and a new thumbnail in the gallery on the right. If you need more room on the screen, you can turn the gallery off by clicking on the x to the right of \"Your Invocations\". You can turn it back on later by clicking the image icon that appears in the gallery's place. The images are written into the directory indicated by the --outdir option provided at script launch time. By default, this is outputs/img-samples under the InvokeAI directory. Generate a bunch of strawberry sushi images by increasing the number of requested images by adjusting the Images counter just below the Camera button. As each is generated, it will be added to the gallery. You can switch the active image by clicking on the gallery thumbnails. Try playing with different settings, including image width and height, the Sampler, the Steps and the CFG scale. Image Width and Height do what you'd expect. However, be aware that larger images consume more VRAM memory and take longer to generate. The Sampler controls how the AI selects the image to display. Some samplers are more \"creative\" than others and will produce a wider range of variations (see next section). Some samplers run faster than others. Steps controls how many noising/denoising/sampling steps the AI will take. The higher this value, the more refined the image will be, but the longer the image will take to generate. A typical strategy is to generate images with a low number of steps in order to select one to work on further, and then regenerate it using a higher number of steps. The CFG Scale controls how hard the AI tries to match the generated image to the input prompt. You can go as high or low as you like, but generally values greater than 20 won't improve things much, and values lower than 5 will produce unexpected images. There are complex interactions between Steps , CFG Scale and the Sampler , so experiment to find out what works for you. To regenerate a previously-generated image, select the image you want and click Use All . This loads the text prompt and other original settings into the control panel. If you then press Invoke it will regenerate the image exactly. You can also selectively modify the prompt or other settings to tweak the image. Alternatively, you may click on Use Seed to load just the image's seed, and leave other settings unchanged. To regenerate a Stable Diffusion image that was generated by another SD package, you need to know its text prompt and its Seed . Copy-paste the prompt into the prompt box, unset the Randomize Seed control in the control panel, and copy-paste the desired Seed into its text field. When you Invoke, you will get something similar to the original image. It will not be exact unless you also set the correct values for the original sampler, CFG, steps and dimensions, but it will (usually) be close.","title":"Basics"},{"location":"features/WEB/#variations-on-a-theme","text":"Let's try generating some variations. Select your favorite sushi image from the gallery to load it. Then select \"Use All\" from the list of buttons above. This will load up all the settings used to generate this image, including its unique seed. Go down to the Variations section of the Control Panel and set the button to On. Set Variation Amount to 0.2 to generate a modest number of variations on the image, and also set the Image counter to 4 . Press the invoke button. This will generate a series of related images. To obtain smaller variations, just lower the Variation Amount. You may also experiment with changing the Sampler. Some samplers generate more variability than others. k_euler_a is particularly creative, while ddim is pretty conservative. For even more variations, experiment with increasing the setting for Perlin . This adds a bit of noise to the image generation process. Note that values of Perlin noise greater than 0.15 produce poor images for several of the samplers.","title":"Variations on a theme"},{"location":"features/WEB/#facial-reconstruction-and-upscaling","text":"Stable Diffusion frequently produces mangled faces, particularly when there are multiple figures in the same scene. Stable Diffusion has particular issues with generating reallistic eyes. InvokeAI provides the ability to reconstruct faces using either the GFPGAN or CodeFormer libraries. For more information see POSTPROCESS . Invoke a prompt that generates a mangled face. A prompt that often gives this is \"portrait of a lawyer, \u00be shot\" (this is not intended as a slur against lawyers!) Once you have an image that needs some touching up, load it into the Image panel, and press the button with the face icon (highlighted in the first screenshot below). A dialog box will appear. Leave Strength at 0.8 and press *Restore Faces\". If all goes well, the eyes and other aspects of the face will be improved (see the second screenshot) The facial reconstruction Strength field adjusts how aggressively the face library will try to alter the face. It can be as high as 1.0, but be aware that this often softens the face airbrush style, losing some details. The default 0.8 is usually sufficient. \"Upscaling\" is the process of increasing the size of an image while retaining the sharpness. InvokeAI uses an external library called \"ESRGAN\" to do this. To invoke upscaling, simply select an image and press the HD button above it. You can select between 2X and 4X upscaling, and adjust the upscaling strength, which has much the same meaning as in facial reconstruction. Try running this on one of your previously-generated images. Finally, you can run facial reconstruction and/or upscaling automatically after each Invocation. Go to the Advanced Options section of the Control Panel and turn on Restore Face and/or Upscale .","title":"Facial reconstruction and upscaling"},{"location":"features/WEB/#image-to-image","text":"InvokeAI lets you take an existing image and use it as the basis for a new creation. You can use any sort of image, including a photograph, a scanned sketch, or a digital drawing, as long as it is in PNG or JPEG format. For this tutorial, we'll use files named Lincoln-and-Parrot-512.png , and Lincoln-and-Parrot-512-transparent.png . Download these images to your local machine now to continue with the walkthrough. Click on the Image to Image tab icon, which is the second icon from the top on the left-hand side of the screen: This will bring you to a screen similar to the one shown here: Drag-and-drop the Lincoln-and-Parrot image into the Image panel, or click the blank area to get an upload dialog. The image will load into an area marked Initial Image . (The WebUI will also load the most recently-generated image from the gallery into a section on the left, but this image will be replaced in the next step.) Go to the prompt box and type old sea captain with raven on shoulder and press Invoke. A derived image will appear to the right of the original one: Experiment with the different settings. The most influential one in Image to Image is Image to Image Strength located about midway down the control panel. By default it is set to 0.75, but can range from 0.0 to 0.99. The higher the value, the more of the original image the AI will replace. A value of 0 will leave the initial image completely unchanged, while 0.99 will replace it completely. However, the Sampler and CFG Scale also influence the final result. You can also generate variations in the same way as described in Text to Image. What if we only want to change certain part(s) of the image and leave the rest intact? This is called Inpainting, and a future version of the InvokeAI web server will provide an interactive painting canvas on which you can directly draw the areas you wish to Inpaint into. For now, you can achieve this effect by using an external photoeditor tool to make one or more regions of the image transparent as described in [INPAINTING.md] and uploading that. The file Lincoln-and-Parrot-512-transparent.png is a version of the earlier image in which the area around the parrot has been replaced with transparency. Click on the \"x\" in the upper right of the Initial Image and upload the transparent version. Using the same prompt \"old sea captain with raven on shoulder\" try Invoking an image. This time, only the parrot will be replaced, leaving the rest of the original image intact: Would you like to modify a previously-generated image using the Image to Image facility? Easy! While in the Image to Image panel, hover over any of the gallery images to see a little menu of icons pop up. Click the picture icon to instantly send the selected image to Image to Image as the initial image. You can do the same from the Text to Image tab by clicking on the picture icon above the central image panel. The screenshot below shows where the \"use as initial image\" icons are located.","title":"Image to Image"},{"location":"features/WEB/#unified-canvas","text":"See the Unified Canvas Guide","title":"Unified Canvas"},{"location":"features/WEB/#parting-remarks","text":"This concludes the walkthrough, but there are several more features that you can explore. Please check out the Command Line Interface documentation for further explanation of the advanced features that were not covered here. The WebUI is only rapid development. Check back regularly for updates!","title":"Parting remarks"},{"location":"features/WEB/#reference","text":"","title":"Reference"},{"location":"features/WEB/#additional-options","text":"parameter effect --web_develop Starts the web server in development mode. --web_verbose Enables verbose logging --cors [CORS ...] Additional allowed origins, comma-separated --host HOST Web server: Host or IP to listen on. Set to 0.0.0.0 to accept traffic from other devices on your network. --port PORT Web server: Port to listen on --certfile CERTFILE Web server: Path to certificate file to use for SSL. Use together with --keyfile --keyfile KEYFILE Web server: Path to private key file to use for SSL. Use together with --certfile' --gui Start InvokeAI GUI - This is the \"desktop mode\" version of the web app. It uses Flask to create a desktop app experience of the webserver.","title":"Additional Options"},{"location":"features/WEB/#web-specific-features","text":"The web experience offers an incredibly easy-to-use experience for interacting with the InvokeAI toolkit. For detailed guidance on individual features, see the Feature-specific help documents available in this directory. Note that the latest functionality available in the CLI may not always be available in the Web interface.","title":"Web Specific Features"},{"location":"features/WEB/#dark-mode-light-mode","text":"The InvokeAI interface is available in a nano-carbon black & purple Dark Mode, and a \"burn your eyes out Nosferatu\" Light Mode. These can be toggled by clicking the Sun/Moon icons at the top right of the interface.","title":"Dark Mode &amp; Light Mode"},{"location":"features/WEB/#invocation-toolbar","text":"The left side of the InvokeAI interface is available for customizing the prompt and the settings used for invoking your new image. Typing your prompt into the open text field and clicking the Invoke button will produce the image based on the settings configured in the toolbar. See below for additional documentation related to each feature: Core Prompt Settings Variations Upscaling Image to Image Inpainting Other","title":"Invocation Toolbar"},{"location":"features/WEB/#invocation-gallery","text":"The currently selected --outdir (or the default outputs folder) will display all previously generated files on load. As new invocations are generated, these will be dynamically added to the gallery, and can be previewed by selecting them. Each image also has a simple set of actions (e.g., Delete, Use Seed, Use All Parameters, etc.) that can be accessed by hovering over the image.","title":"Invocation Gallery"},{"location":"features/WEB/#image-workspace","text":"When an image from the Invocation Gallery is selected, or is generated, the image will be displayed within the center of the interface. A quickbar of common image interactions are displayed along the top of the image, including: Use image in the Image to Image workflow Initialize Face Restoration on the selected file Initialize Upscaling on the selected file View File metadata and details Delete the file","title":"Image Workspace"},{"location":"features/WEB/#acknowledgements","text":"A huge shout-out to the core team working to make this vision a reality, including psychedelicious , Kyle0654 and blessedcoolant . hipsterusername was the team's unofficial cheerleader and added tooltips/docs.","title":"Acknowledgements"},{"location":"features/WEBUIHOTKEYS/","text":"WebUI Hotkey List # App Hotkeys # Setting Hotkey Ctrl + Enter Invoke Shift + X Cancel Alt + A Focus Prompt O Toggle Options Shift + O Pin Options Z Toggle Viewer G Toggle Gallery F Maximize Workspace 1 - 5 Change Tabs ` Toggle Console General Hotkeys # Setting Hotkey P Set Prompt S Set Seed A Set Parameters Shift + R Restore Faces Shift + U Upscale I Show Info Shift + I Send To Image To Image Del Delete Image Esc Close Panels Gallery Hotkeys # Setting Hotkey Left Previous Image Right Next Image Shift + G Toggle Gallery Pin Shift + Up Increase Gallery Image Size Shift + Down Decrease Gallery Image Size Unified Canvas Hotkeys # Setting Hotkey B Select Brush E Select Eraser [ Decrease Brush Size ] Increase Brush Size Shift + [ Decrease Brush Opacity Shift + ] Increase Brush Opacity V Move Tool Shift + F Fill Bounding Box Del / Backspace Erase Bounding Box C Select Color Picker N Toggle Snap Hold Space Quick Toggle Move Q Toggle Layer Shift + C Clear Mask H Hide Mask Shift + H Show/Hide Bounding Box Shift + M Merge Visible Shift + S Save To Gallery Ctrl + C Copy To Clipboard Shift + D Download Image Ctrl + Z Undo Ctrl + Y / Ctrl + Shift + Z Redo R Reset View Left Previous Staging Image Right Next Staging Image Enter Accept Staging Image","title":"WebUI Hotkey List"},{"location":"features/WEBUIHOTKEYS/#webui-hotkey-list","text":"","title":" WebUI Hotkey List"},{"location":"features/WEBUIHOTKEYS/#app-hotkeys","text":"Setting Hotkey Ctrl + Enter Invoke Shift + X Cancel Alt + A Focus Prompt O Toggle Options Shift + O Pin Options Z Toggle Viewer G Toggle Gallery F Maximize Workspace 1 - 5 Change Tabs ` Toggle Console","title":"App Hotkeys"},{"location":"features/WEBUIHOTKEYS/#general-hotkeys","text":"Setting Hotkey P Set Prompt S Set Seed A Set Parameters Shift + R Restore Faces Shift + U Upscale I Show Info Shift + I Send To Image To Image Del Delete Image Esc Close Panels","title":"General Hotkeys"},{"location":"features/WEBUIHOTKEYS/#gallery-hotkeys","text":"Setting Hotkey Left Previous Image Right Next Image Shift + G Toggle Gallery Pin Shift + Up Increase Gallery Image Size Shift + Down Decrease Gallery Image Size","title":"Gallery Hotkeys"},{"location":"features/WEBUIHOTKEYS/#unified-canvas-hotkeys","text":"Setting Hotkey B Select Brush E Select Eraser [ Decrease Brush Size ] Increase Brush Size Shift + [ Decrease Brush Opacity Shift + ] Increase Brush Opacity V Move Tool Shift + F Fill Bounding Box Del / Backspace Erase Bounding Box C Select Color Picker N Toggle Snap Hold Space Quick Toggle Move Q Toggle Layer Shift + C Clear Mask H Hide Mask Shift + H Show/Hide Bounding Box Shift + M Merge Visible Shift + S Save To Gallery Ctrl + C Copy To Clipboard Shift + D Download Image Ctrl + Z Undo Ctrl + Y / Ctrl + Shift + Z Redo R Reset View Left Previous Staging Image Right Next Staging Image Enter Accept Staging Image","title":"Unified Canvas Hotkeys"},{"location":"help/SAMPLER_CONVERGENCE/","text":"Sampler Convergence # As features keep increasing, making the right choices for your needs can become increasingly difficult. What sampler to use? And for how many steps? Do you change the CFG value? Do you use prompt weighting? Do you allow variations? Even once you have a result, do you blend it with other images? Pass it through img2img ? With what strength? Do you use inpainting to correct small details? Outpainting to extend cropped sections? The purpose of this series of documents is to help you better understand these tools, so you can make the best out of them. Feel free to contribute with your own findings! In this document, we will talk about sampler convergence. Looking for a short version? Here's a TL;DR in 3 tables. Remember Results converge as steps ( -s ) are increased (except for K_DPM_2_A and K_EULER_A ). Often at \u2265 -s100 , but may require \u2265 -s700 ). Producing a batch of candidate images at low ( -s8 to -s30 ) step counts can save you hours of computation. K_HEUN and K_DPM_2 converge in less steps (but are slower). K_DPM_2_A and K_EULER_A incorporate a lot of creativity/variability. Sampler (3 sample avg) it/s (M1 Max 64GB, 512x512) DDIM 1.89 PLMS 1.86 K_EULER 1.86 K_LMS 1.91 K_HEUN 0.95 (slower) K_DPM_2 0.95 (slower) K_DPM_2_A 0.95 (slower) K_EULER_A 1.86 suggestions For most use cases, K_LMS , K_HEUN and K_DPM_2 are the best choices (the latter 2 run 0.5x as quick, but tend to converge 2x as quick as K_LMS ). At very low steps (\u2264 -s8 ), K_HEUN and K_DPM_2 are not recommended. Use K_LMS instead. For variability, use K_EULER_A (runs 2x as quick as K_DPM_2_A ). Sampler results # Let's start by choosing a prompt and using it with each of our 8 samplers, running it for 10, 20, 30, 40, 50 and 100 steps. Anime. \"an anime girl\" -W512 -H512 -C7.5 -S3031912972 Sampler convergence # Immediately, you can notice results tend to converge -that is, as -s (step) values increase, images look more and more similar until there comes a point where the image no longer changes. You can also notice how DDIM and PLMS eventually tend to converge to K-sampler results as steps are increased. Among K-samplers, K_HEUN and K_DPM_2 seem to require the fewest steps to converge, and even at low step counts they are good indicators of the final result. And finally, K_DPM_2_A and K_EULER_A seem to do a bit of their own thing and don't keep much similarity with the rest of the samplers. Batch generation speedup # This realization is very useful because it means you don't need to create a batch of 100 images ( -n100 ) at -s100 to choose your favorite 2 or 3 images. You can produce the same 100 images at -s10 to -s30 using a K-sampler (since they converge faster), get a rough idea of the final result, choose your 2 or 3 favorite ones, and then run -s100 on those images to polish some details. The latter technique is 3-8x as quick. Example At 60s per 100 steps. A) 60s * 100 images = 6000s (100 images at -s100 , manually picking 3 favorites) B) 6s 100 images + 60s 3 images = 780s (100 images at -s10 , manually picking 3 favorites, and running those 3 at -s100 to polish details) The result is 1 hour and 40 minutes for Variant A, vs 13 minutes for Variant B. Topic convergance # Now, these results seem interesting, but do they hold for other topics? How about nature? Food? People? Animals? Let's try! Nature. \"valley landscape wallpaper, d&d art, fantasy, painted, 4k, high detail, sharp focus, washed colors, elaborate excellent painted illustration\" -W512 -H512 -C7.5 -S1458228930 With nature, you can see how initial results are even more indicative of final result -more so than with characters/people. K_HEUN and K_DPM_2 are again the quickest indicators, almost right from the start. Results also converge faster (e.g. K_HEUN converged at -s21 ). Food. \"a hamburger with a bowl of french fries\" -W512 -H512 -C7.5 -S4053222918 Again, K_HEUN and K_DPM_2 take the fewest number of steps to be good indicators of the final result. K_DPM_2_A and K_EULER_A seem to incorporate a lot of creativity/variability, capable of producing rotten hamburgers, but also of adding lettuce to the mix. And they're the only samplers that produced an actual 'bowl of fries'! Animals. \"grown tiger, full body\" -W512 -H512 -C7.5 -S3721629802 K_HEUN and K_DPM_2 once again require the least number of steps to be indicative of the final result (around -s30 ), while other samplers are still struggling with several tails or malformed back legs. It also takes longer to converge (for comparison, K_HEUN required around 150 steps to converge). This is normal, as producing human/animal faces/bodies is one of the things the model struggles the most with. For these topics, running for more steps will often increase coherence within the composition. People. \"Ultra realistic photo, (Miranda Bloom-Kerr), young, stunning model, blue eyes, blond hair, beautiful face, intricate, highly detailed, smooth, art by artgerm and greg rutkowski and alphonse mucha, stained glass\" -W512 -H512 -C7.5 -S2131956332 . This time, we will go up to 300 steps. Observing the results, it again takes longer for all samplers to converge ( K_HEUN took around 150 steps), but we can observe good indicative results much earlier (see: K_HEUN ). Conversely, DDIM and PLMS are still undergoing moderate changes (see: lace around her neck), even at -s300 . In fact, as we can see in this other experiment, some samplers can take 700+ steps to converge when generating people. Note also the point of convergence may not be the most desirable state (e.g. I prefer an earlier version of the face, more rounded), but it will probably be the most coherent arms/hands/face attributes-wise. You can always merge different images with a photo editing tool and pass it through img2img to smoothen the composition. Sampler generation times # Once we understand the concept of sampler convergence, we must look into the performance of each sampler in terms of steps (iterations) per second, as not all samplers run at the same speed. On my M1 Max with 64GB of RAM, for a 512x512 image Sampler (3 sample average) it/s DDIM 1.89 PLMS 1.86 K_EULER 1.86 K_LMS 1.91 K_HEUN 0.95 (slower) K_DPM_2 0.95 (slower) K_DPM_2_A 0.95 (slower) K_EULER_A 1.86 Combining our results with the steps per second of each sampler, three choices come out on top: K_LMS , K_HEUN and K_DPM_2 (where the latter two run 0.5x as quick but tend to converge 2x as quick as K_LMS ). For creativity and a lot of variation between iterations, K_EULER_A can be a good choice (which runs 2x as quick as K_DPM_2_A ). Additionally, image generation at very low steps (\u2264 -s8 ) is not recommended for K_HEUN and K_DPM_2 . Use K_LMS instead. Three key points # Finally, it is relevant to mention that, in general, there are 3 important moments in the process of image formation as steps increase: The (earliest) point at which an image becomes a good indicator of the final result (useful for batch generation at low step values, to then improve the quality/coherence of the chosen images via running the same prompt and seed for more steps). The (earliest) point at which an image becomes coherent, even if different from the result if steps are increased (useful for batch generation at low step values, where quality/coherence is improved via techniques other than increasing the steps -e.g. via inpainting). The point at which an image fully converges. Hence, remember that your workflow/strategy should define your optimal number of steps, even for the same prompt and seed (for example, if you seek full convergence, you may run K_LMS for -s200 in the case of the red-haired girl, but K_LMS and -s20 -taking one tenth the time- may do as well if your workflow includes adding small details, such as the missing shoulder strap, via img2img ).","title":"Sampler Convergence"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-convergence","text":"As features keep increasing, making the right choices for your needs can become increasingly difficult. What sampler to use? And for how many steps? Do you change the CFG value? Do you use prompt weighting? Do you allow variations? Even once you have a result, do you blend it with other images? Pass it through img2img ? With what strength? Do you use inpainting to correct small details? Outpainting to extend cropped sections? The purpose of this series of documents is to help you better understand these tools, so you can make the best out of them. Feel free to contribute with your own findings! In this document, we will talk about sampler convergence. Looking for a short version? Here's a TL;DR in 3 tables. Remember Results converge as steps ( -s ) are increased (except for K_DPM_2_A and K_EULER_A ). Often at \u2265 -s100 , but may require \u2265 -s700 ). Producing a batch of candidate images at low ( -s8 to -s30 ) step counts can save you hours of computation. K_HEUN and K_DPM_2 converge in less steps (but are slower). K_DPM_2_A and K_EULER_A incorporate a lot of creativity/variability. Sampler (3 sample avg) it/s (M1 Max 64GB, 512x512) DDIM 1.89 PLMS 1.86 K_EULER 1.86 K_LMS 1.91 K_HEUN 0.95 (slower) K_DPM_2 0.95 (slower) K_DPM_2_A 0.95 (slower) K_EULER_A 1.86 suggestions For most use cases, K_LMS , K_HEUN and K_DPM_2 are the best choices (the latter 2 run 0.5x as quick, but tend to converge 2x as quick as K_LMS ). At very low steps (\u2264 -s8 ), K_HEUN and K_DPM_2 are not recommended. Use K_LMS instead. For variability, use K_EULER_A (runs 2x as quick as K_DPM_2_A ).","title":" Sampler Convergence"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-results","text":"Let's start by choosing a prompt and using it with each of our 8 samplers, running it for 10, 20, 30, 40, 50 and 100 steps. Anime. \"an anime girl\" -W512 -H512 -C7.5 -S3031912972","title":"Sampler results"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-convergence_1","text":"Immediately, you can notice results tend to converge -that is, as -s (step) values increase, images look more and more similar until there comes a point where the image no longer changes. You can also notice how DDIM and PLMS eventually tend to converge to K-sampler results as steps are increased. Among K-samplers, K_HEUN and K_DPM_2 seem to require the fewest steps to converge, and even at low step counts they are good indicators of the final result. And finally, K_DPM_2_A and K_EULER_A seem to do a bit of their own thing and don't keep much similarity with the rest of the samplers.","title":"Sampler convergence"},{"location":"help/SAMPLER_CONVERGENCE/#batch-generation-speedup","text":"This realization is very useful because it means you don't need to create a batch of 100 images ( -n100 ) at -s100 to choose your favorite 2 or 3 images. You can produce the same 100 images at -s10 to -s30 using a K-sampler (since they converge faster), get a rough idea of the final result, choose your 2 or 3 favorite ones, and then run -s100 on those images to polish some details. The latter technique is 3-8x as quick. Example At 60s per 100 steps. A) 60s * 100 images = 6000s (100 images at -s100 , manually picking 3 favorites) B) 6s 100 images + 60s 3 images = 780s (100 images at -s10 , manually picking 3 favorites, and running those 3 at -s100 to polish details) The result is 1 hour and 40 minutes for Variant A, vs 13 minutes for Variant B.","title":"Batch generation speedup"},{"location":"help/SAMPLER_CONVERGENCE/#topic-convergance","text":"Now, these results seem interesting, but do they hold for other topics? How about nature? Food? People? Animals? Let's try! Nature. \"valley landscape wallpaper, d&d art, fantasy, painted, 4k, high detail, sharp focus, washed colors, elaborate excellent painted illustration\" -W512 -H512 -C7.5 -S1458228930 With nature, you can see how initial results are even more indicative of final result -more so than with characters/people. K_HEUN and K_DPM_2 are again the quickest indicators, almost right from the start. Results also converge faster (e.g. K_HEUN converged at -s21 ). Food. \"a hamburger with a bowl of french fries\" -W512 -H512 -C7.5 -S4053222918 Again, K_HEUN and K_DPM_2 take the fewest number of steps to be good indicators of the final result. K_DPM_2_A and K_EULER_A seem to incorporate a lot of creativity/variability, capable of producing rotten hamburgers, but also of adding lettuce to the mix. And they're the only samplers that produced an actual 'bowl of fries'! Animals. \"grown tiger, full body\" -W512 -H512 -C7.5 -S3721629802 K_HEUN and K_DPM_2 once again require the least number of steps to be indicative of the final result (around -s30 ), while other samplers are still struggling with several tails or malformed back legs. It also takes longer to converge (for comparison, K_HEUN required around 150 steps to converge). This is normal, as producing human/animal faces/bodies is one of the things the model struggles the most with. For these topics, running for more steps will often increase coherence within the composition. People. \"Ultra realistic photo, (Miranda Bloom-Kerr), young, stunning model, blue eyes, blond hair, beautiful face, intricate, highly detailed, smooth, art by artgerm and greg rutkowski and alphonse mucha, stained glass\" -W512 -H512 -C7.5 -S2131956332 . This time, we will go up to 300 steps. Observing the results, it again takes longer for all samplers to converge ( K_HEUN took around 150 steps), but we can observe good indicative results much earlier (see: K_HEUN ). Conversely, DDIM and PLMS are still undergoing moderate changes (see: lace around her neck), even at -s300 . In fact, as we can see in this other experiment, some samplers can take 700+ steps to converge when generating people. Note also the point of convergence may not be the most desirable state (e.g. I prefer an earlier version of the face, more rounded), but it will probably be the most coherent arms/hands/face attributes-wise. You can always merge different images with a photo editing tool and pass it through img2img to smoothen the composition.","title":"Topic convergance"},{"location":"help/SAMPLER_CONVERGENCE/#sampler-generation-times","text":"Once we understand the concept of sampler convergence, we must look into the performance of each sampler in terms of steps (iterations) per second, as not all samplers run at the same speed. On my M1 Max with 64GB of RAM, for a 512x512 image Sampler (3 sample average) it/s DDIM 1.89 PLMS 1.86 K_EULER 1.86 K_LMS 1.91 K_HEUN 0.95 (slower) K_DPM_2 0.95 (slower) K_DPM_2_A 0.95 (slower) K_EULER_A 1.86 Combining our results with the steps per second of each sampler, three choices come out on top: K_LMS , K_HEUN and K_DPM_2 (where the latter two run 0.5x as quick but tend to converge 2x as quick as K_LMS ). For creativity and a lot of variation between iterations, K_EULER_A can be a good choice (which runs 2x as quick as K_DPM_2_A ). Additionally, image generation at very low steps (\u2264 -s8 ) is not recommended for K_HEUN and K_DPM_2 . Use K_LMS instead.","title":"Sampler generation times"},{"location":"help/SAMPLER_CONVERGENCE/#three-key-points","text":"Finally, it is relevant to mention that, in general, there are 3 important moments in the process of image formation as steps increase: The (earliest) point at which an image becomes a good indicator of the final result (useful for batch generation at low step values, to then improve the quality/coherence of the chosen images via running the same prompt and seed for more steps). The (earliest) point at which an image becomes coherent, even if different from the result if steps are increased (useful for batch generation at low step values, where quality/coherence is improved via techniques other than increasing the steps -e.g. via inpainting). The point at which an image fully converges. Hence, remember that your workflow/strategy should define your optimal number of steps, even for the same prompt and seed (for example, if you seek full convergence, you may run K_LMS for -s200 in the case of the red-haired girl, but K_LMS and -s20 -taking one tenth the time- may do as well if your workflow includes adding small details, such as the missing shoulder strap, via img2img ).","title":"Three key points"},{"location":"help/deprecated/TROUBLESHOOT/","text":"F.A.Q. # Frequently-Asked-Questions # Here are a few common installation problems and their solutions. Often these are caused by incomplete installations or crashes during the install process. During conda env create , conda hangs indefinitely # If it is because of the last PIP step (usually stuck in the Git Clone step, you can check the detailed log by this method): export PIP_LOG = \"/tmp/pip_log.txt\" touch ${ PIP_LOG } tail -f ${ PIP_LOG } & conda env create -f environment-mac.yaml --debug --verbose killall tail rm ${ PIP_LOG } SOLUTION Conda sometimes gets stuck at the last PIP step, in which several git repositories are cloned and built. Enter the stable-diffusion directory and completely remove the src directory and all its contents. The safest way to do this is to enter the stable-diffusion directory and give the command git clean -f . If this still doesn't fix the problem, try \"conda clean -all\" and then restart at the conda env create step. To further understand the problem to checking the install lot using this method: export PIP_LOG = \"/tmp/pip_log.txt\" touch ${ PIP_LOG } tail -f ${ PIP_LOG } & conda env create -f environment-mac.yaml --debug --verbose killall tail rm ${ PIP_LOG } invoke.py crashes with the complaint that it can't find ldm.simplet2i.py # Or it complains that function is being passed incorrect parameters. SOLUTION Reinstall the stable diffusion modules. Enter the stable-diffusion directory and give the command pip install -e . Missing modules # invoke.py dies, complaining of various missing modules, none of which starts with ldm . SOLUTION From within the InvokeAI directory, run conda env update This is also frequently the solution to complaints about an unknown function in a module. How can I try new features # There's a feature or bugfix in the Stable Diffusion GitHub that you want to try out. SOLUTIONS Main Branch # If the fix/feature is on the main branch, enter the stable-diffusion directory and do a git pull . Usually this will be sufficient, but if you start to see errors about missing or incorrect modules, use the command pip install -e . and/or conda env update (These commands won't break anything.) pip install -e . and/or conda env update -f environment.yaml (These commands won't break anything.) Sub Branch # If the feature/fix is on a branch (e.g. \" foo-bugfix \"), the recipe is similar, but do a git pull <name of branch> . Not Committed # If the feature/fix is in a pull request that has not yet been made part of the main branch or a feature/bugfix branch, then from the page for the desired pull request, look for the line at the top that reads \" xxxx wants to merge xx commits into lstein:main from YYYYYY \". Copy the URL in YYYY. It should have the format https://github.com/<name of contributor>/stable-diffusion/tree/<name of branch> Then go to the directory above stable-diffusion and rename the directory to \" stable-diffusion.lstein \", \" stable-diffusion.old \", or anything else. You can then git clone the branch that contains the pull request: git clone https://github.com/<name of contributor>/stable-diffusion/tree/<name of branch> You will need to go through the install procedure again, but it should be fast because all the dependencies are already loaded. CUDA out of memory # Image generation crashed with CUDA out of memory error after successful sampling. SOLUTION Try to run script with option --free_gpu_mem This will free memory before image decoding step.","title":"F.A.Q."},{"location":"help/deprecated/TROUBLESHOOT/#faq","text":"","title":" F.A.Q."},{"location":"help/deprecated/TROUBLESHOOT/#frequently-asked-questions","text":"Here are a few common installation problems and their solutions. Often these are caused by incomplete installations or crashes during the install process.","title":"Frequently-Asked-Questions"},{"location":"help/deprecated/TROUBLESHOOT/#during-conda-env-create-conda-hangs-indefinitely","text":"If it is because of the last PIP step (usually stuck in the Git Clone step, you can check the detailed log by this method): export PIP_LOG = \"/tmp/pip_log.txt\" touch ${ PIP_LOG } tail -f ${ PIP_LOG } & conda env create -f environment-mac.yaml --debug --verbose killall tail rm ${ PIP_LOG } SOLUTION Conda sometimes gets stuck at the last PIP step, in which several git repositories are cloned and built. Enter the stable-diffusion directory and completely remove the src directory and all its contents. The safest way to do this is to enter the stable-diffusion directory and give the command git clean -f . If this still doesn't fix the problem, try \"conda clean -all\" and then restart at the conda env create step. To further understand the problem to checking the install lot using this method: export PIP_LOG = \"/tmp/pip_log.txt\" touch ${ PIP_LOG } tail -f ${ PIP_LOG } & conda env create -f environment-mac.yaml --debug --verbose killall tail rm ${ PIP_LOG }","title":"During conda env create, conda hangs indefinitely"},{"location":"help/deprecated/TROUBLESHOOT/#invokepy-crashes-with-the-complaint-that-it-cant-find-ldmsimplet2ipy","text":"Or it complains that function is being passed incorrect parameters. SOLUTION Reinstall the stable diffusion modules. Enter the stable-diffusion directory and give the command pip install -e .","title":"invoke.py crashes with the complaint that it can't find ldm.simplet2i.py"},{"location":"help/deprecated/TROUBLESHOOT/#missing-modules","text":"invoke.py dies, complaining of various missing modules, none of which starts with ldm . SOLUTION From within the InvokeAI directory, run conda env update This is also frequently the solution to complaints about an unknown function in a module.","title":"Missing modules"},{"location":"help/deprecated/TROUBLESHOOT/#how-can-i-try-new-features","text":"There's a feature or bugfix in the Stable Diffusion GitHub that you want to try out. SOLUTIONS","title":"How can I try new features"},{"location":"help/deprecated/TROUBLESHOOT/#main-branch","text":"If the fix/feature is on the main branch, enter the stable-diffusion directory and do a git pull . Usually this will be sufficient, but if you start to see errors about missing or incorrect modules, use the command pip install -e . and/or conda env update (These commands won't break anything.) pip install -e . and/or conda env update -f environment.yaml (These commands won't break anything.)","title":"Main Branch"},{"location":"help/deprecated/TROUBLESHOOT/#sub-branch","text":"If the feature/fix is on a branch (e.g. \" foo-bugfix \"), the recipe is similar, but do a git pull <name of branch> .","title":"Sub Branch"},{"location":"help/deprecated/TROUBLESHOOT/#not-committed","text":"If the feature/fix is in a pull request that has not yet been made part of the main branch or a feature/bugfix branch, then from the page for the desired pull request, look for the line at the top that reads \" xxxx wants to merge xx commits into lstein:main from YYYYYY \". Copy the URL in YYYY. It should have the format https://github.com/<name of contributor>/stable-diffusion/tree/<name of branch> Then go to the directory above stable-diffusion and rename the directory to \" stable-diffusion.lstein \", \" stable-diffusion.old \", or anything else. You can then git clone the branch that contains the pull request: git clone https://github.com/<name of contributor>/stable-diffusion/tree/<name of branch> You will need to go through the install procedure again, but it should be fast because all the dependencies are already loaded.","title":"Not Committed"},{"location":"help/deprecated/TROUBLESHOOT/#cuda-out-of-memory","text":"Image generation crashed with CUDA out of memory error after successful sampling. SOLUTION Try to run script with option --free_gpu_mem This will free memory before image decoding step.","title":"CUDA out of memory"},{"location":"installation/","text":"We offer several ways to install InvokeAI, each one suited to your experience and preferences. We suggest that everyone start by reviewing the hardware and software requirements, as they are the same across each install method. Then pick the install method most suitable to your level of experience and needs. See the troubleshooting section of the automated install guide for frequently-encountered installation issues. Main Application # Automated Installer This is a script that will install all of InvokeAI's essential third party libraries and InvokeAI itself. It includes access to a \"developer console\" which will help us debug problems with you and give you to access experimental features. Manual Installation In this method you will manually run the commands needed to install InvokeAI and its dependencies. We offer two recipes: one suited to those who prefer the conda tool, and one suited to those who prefer pip and Python virtual environments. In our hands the pip install is faster and more reliable, but your mileage may vary. Note that the conda installation method is currently deprecated and will not be supported at some point in the future. This method is recommended for users who have previously used conda or pip in the past, developers, and anyone who wishes to remain on the cutting edge of future InvokeAI development and is willing to put up with occasional glitches and breakage. Docker Installation We also offer a method for creating Docker containers containing InvokeAI and its dependencies. This method is recommended for individuals with experience with Docker containers and understand the pluses and minuses of a container-based install. Quick Guides # Installing CUDA and ROCm Drivers Installing XFormers Installing PyPatchMatch Installing New Models","title":"Overview"},{"location":"installation/#main-application","text":"Automated Installer This is a script that will install all of InvokeAI's essential third party libraries and InvokeAI itself. It includes access to a \"developer console\" which will help us debug problems with you and give you to access experimental features. Manual Installation In this method you will manually run the commands needed to install InvokeAI and its dependencies. We offer two recipes: one suited to those who prefer the conda tool, and one suited to those who prefer pip and Python virtual environments. In our hands the pip install is faster and more reliable, but your mileage may vary. Note that the conda installation method is currently deprecated and will not be supported at some point in the future. This method is recommended for users who have previously used conda or pip in the past, developers, and anyone who wishes to remain on the cutting edge of future InvokeAI development and is willing to put up with occasional glitches and breakage. Docker Installation We also offer a method for creating Docker containers containing InvokeAI and its dependencies. This method is recommended for individuals with experience with Docker containers and understand the pluses and minuses of a container-based install.","title":"Main Application"},{"location":"installation/#quick-guides","text":"Installing CUDA and ROCm Drivers Installing XFormers Installing PyPatchMatch Installing New Models","title":"Quick Guides"},{"location":"installation/010_INSTALL_AUTOMATED/","text":"InvokeAI Automated Installation # Introduction # The automated installer is a Python script that automates the steps needed to install and run InvokeAI on a stock computer running recent versions of Linux, MacOS or Windows. It will leave you with a version that runs a stable version of InvokeAI with the option to upgrade to experimental versions later. Walk through # Hardware Requirements : Make sure that your system meets the hardware requirements and has the appropriate GPU drivers installed. For a system with an NVIDIA card installed, you will need to install the CUDA driver, while AMD-based cards require the ROCm driver. In most cases, if you've already used the system for gaming or other graphics-intensive tasks, the appropriate drivers will already be installed. If unsure, check the GPU Driver Guide Required Space Installation requires roughly 18G of free disk space to load the libraries and recommended model weights files. Regardless of your destination disk, your system drive ( C:\\ on Windows, / on macOS/Linux) requires at least 6GB of free disk space to download and cache python dependencies. NOTE for Linux users: if your temporary directory is mounted as a tmpfs , ensure it has sufficient space. Software Requirements : Check that your system has an up-to-date Python installed. To do this, open up a command-line window (\"Terminal\" on Linux and Macintosh, \"Command\" or \"Powershell\" on Windows) and type python --version . If Python is installed, it will print out the version number. If it is version 3.9.1 or 3.10.x , you meet requirements. What to do if you have an unsupported version Go to Python Downloads and download the appropriate installer package for your platform. We recommend Version 3.10.9 , which has been extensively tested with InvokeAI. At this time we do not recommend Python 3.11. Please select your platform in the section below for platform-specific setup requirements. Windows Linux Mac During the Python configuration process, look out for a checkbox to add Python to your PATH and select it. If the install script complains that it can't find python, then open the Python installer again and choose \"Modify\" existing installation. Installation requires an up to date version of the Microsoft Visual C libraries. Please install the 2015-2022 libraries available here: https://learn.microsoft.com/en-US/cpp/windows/latest-supported-vc-redist?view=msvc-170 Please double-click on the file WinLongPathsEnabled.reg and accept the dialog box that asks you if you wish to modify your registry. This activates long filename support on your system and will prevent mysterious errors during installation. To install an appropriate version of Python on Ubuntu 22.04 and higher, run the following: sudo apt update sudo apt install -y python3 python3-pip python3-venv sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.10 3 On Ubuntu 20.04, the process is slightly different: sudo apt update sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt install python3.10 python3-pip python3.10-venv sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.10 3 Both python and python3 commands are now pointing at Python3.10. You can still access older versions of Python by calling python2 , python3.8 , etc. Linux systems require a couple of additional graphics libraries to be installed for proper functioning of python3-opencv . Please run the following: sudo apt update && sudo apt install -y libglib2.0-0 libgl1-mesa-glx After installing Python, you may need to run the following command from the Terminal in order to install the Web certificates needed to download model data from https sites. If you see lots of CERTIFICATE ERRORS during the last part of the install, this is the problem, and you can fix it with this command: `/Applications/Python\\ 3.10/Install\\ Certificates.command` You may need to install the Xcode command line tools. These are a set of tools that are needed to run certain applications in a Terminal, including InvokeAI. This package is provided directly by Apple. To install, open a terminal window and run xcode-select --install . You will get a macOS system popup guiding you through the install. If you already have them installed, you will instead see some output in the Terminal advising you that the tools are already installed. More information can be found at FreeCode Camp Download the Installer : The InvokeAI installer is distributed as a ZIP files. Go to the latest release , and look for a file named: InvokeAI-installer-v2.X.X.zip where \"2.X.X\" is the latest released version. The file is located at the very bottom of the release page, under Assets . Unpack the installer : Unpack the zip file into a convenient directory. This will create a new directory named \"InvokeAI-Installer\". When unpacked, the directory will look like this: Launch the installer script from the desktop : If you are using a desktop GUI, double-click the installer file appropriate for your platform. It will be named install.bat on Windows systems and install.sh on Linux and Macintosh systems. Be aware that your system's file browser may suppress the display of the file extension. On Windows systems if you get an \"Untrusted Publisher\" warning. Click on \"More Info\" and then select \"Run Anyway.\" You trust us, right? [Alternative] Launch the installer script from the command line : Alternatively, from the command line, run the shell script or .bat file: C:\\Documents\\Linco> cd InvokeAI-Installer C:\\Documents\\Linco\\invokeAI> install.bat Select the location to install InvokeAI : The script will ask you to choose where to install InvokeAI. Select a directory with at least 18G of free space for a full install. InvokeAI and all its support files will be installed into a new directory named invokeai located at the location you specify. The default is to install the invokeai directory in your home directory, usually C:\\Users\\YourName\\invokeai on Windows systems, /home/YourName/invokeai on Linux systems, and /Users/YourName/invokeai on Macintoshes, where \"YourName\" is your login name. The script uses tab autocompletion to suggest directory path completions. Type part of the path (e.g. \"C:\\Users\") and press Tab repeatedly to suggest completions. Select your GPU : The installer will autodetect your platform and will request you to confirm the type of GPU your graphics card has. On Linux systems, you will have the choice of CUDA (NVidia cards), ROCm (AMD cards), or CPU (no graphics acceleration). On Windows, you'll have the choice of CUDA vs CPU, and on Macs you'll be offered CPU only. When you select CPU on M1 or M2 Macintoshes, you will get MPS-based graphics acceleration without installing additional drivers. If you are unsure what GPU you are using, you can ask the installer to guess. Watch it go! : Sit back and let the install script work. It will install the third-party libraries needed by InvokeAI and the application itself. Be aware that some of the library download and install steps take a long time. In particular, the pytorch package is quite large and often appears to get \"stuck\" at 99.9%. Have patience and the installation step will eventually resume. However, there are occasions when the library install does legitimately get stuck. If you have been waiting for more than ten minutes and nothing is happening, you can interrupt the script with ^C. You may restart it and it will pick up where it left off. Post-install Configuration : After installation completes, the installer will launch the configuration script, which will guide you through the first-time process of selecting one or more Stable Diffusion model weights files, downloading and configuring them. We provide a list of popular models that InvokeAI performs well with. However, you can add more weight files later on using the command-line client or the Web UI. See Installing Models for details. If you have already downloaded the weights file(s) for another Stable Diffusion distribution, you may skip this step (by selecting \"skip\" when prompted) and configure InvokeAI to use the previously-downloaded files. The process for this is described in Installing Models . Running InvokeAI for the first time : The script will now exit and you'll be ready to generate some images. Look for the directory invokeai installed in the location you chose at the beginning of the install session. Look for a shell script named invoke.sh (Linux/Mac) or invoke.bat (Windows). Launch the script by double-clicking it or typing its name at the command-line: C:\\Documents\\Linco> cd invokeai C:\\Documents\\Linco\\invokeAI> invoke.bat The invoke.bat ( invoke.sh ) script will give you the choice of starting (1) the command-line interface, (2) the web GUI, (3) textual inversion training, and (4) model merging. By default, the script will launch the web interface. When you do this, you'll see a series of startup messages ending with instructions to point your browser at http://localhost:9090 . Click on this link to open up a browser and start exploring InvokeAI's features. InvokeAI Options : You can launch InvokeAI with several different command-line arguments that customize its behavior. For example, you can change the location of the image output directory, or select your favorite sampler. See the Command-Line Interface for a full list of the options. To set defaults that will take effect every time you launch InvokeAI, use a text editor (e.g. Notepad) to exit the file invokeai\\invokeai.init . It contains a variety of examples that you can follow to add and modify launch options. The launcher script also offers you an option labeled \"open the developer console\". If you choose this option, you will be dropped into a command-line interface in which you can run python commands directly, access developer tools, and launch InvokeAI with customized options. Do not move or remove the invokeai directory The invokeai directory contains the invokeai application, its configuration files, the model weight files, and outputs of image generation. Once InvokeAI is installed, do not move or remove this directory.\" Troubleshooting # Package dependency conflicts # If you have previously installed InvokeAI or another Stable Diffusion package, the installer may occasionally pick up outdated libraries and either the installer or invoke will fail with complaints about library conflicts. In this case, run the invoke.sh / invoke.bat command and enter the Developer's Console by picking option (5). This will take you to a command-line prompt. Then give this command: pip install InvokeAI --force-reinstall This should fix the issues. InvokeAI runs extremely slowly on Linux or Windows systems # The most frequent cause of this problem is when the installation process installed the CPU-only version of the torch machine-learning library, rather than a version that takes advantage of GPU acceleration. To confirm this issue, look at the InvokeAI startup messages. If you see a message saying \">> Using device CPU\", then this is what happened. To fix this problem, first determine whether you have an NVidia or an AMD GPU. The former uses the CUDA driver, and the latter uses ROCm (only available on Linux). Then run the invoke.sh / invoke.bat command and enter the Developer's Console by picking option (5). This will take you to a command-line prompt. Then type the following commands: NVIDIA System AMD System pip install torch torchvision --force-reinstall --extra-index-url https://download.pytorch.org/whl/cu117 pip install xformers pip install torch torchvision --force-reinstall --extra-index-url https://download.pytorch.org/whl/rocm5.2 Corrupted configuration file # Everything seems to install ok, but invokeai complains of a corrupted configuration file and goes back into the configuration process (asking you to download models, etc), but this doesn't fix the problem. This issue is often caused by a misconfigured configuration directive in the invokeai\\invokeai.init initialization file that contains startup settings. The easiest way to fix the problem is to move the file out of the way and re-run invokeai-configure . Enter the developer's console (option 3 of the launcher script) and run this command: invokeai-configure --root=. Note the dot (.) after --root . It is part of the command. If none of these maneuvers fixes the problem then please report the problem to the InvokeAI Issues section, or visit our Discord Server for interactive assistance. Other Problems # If you run into problems during or after installation, the InvokeAI team is available to help you. Either create an Issue at our GitHub site, or make a request for help on the \"bugs-and-support\" channel of our Discord server . We are a 100% volunteer organization, but typically somebody will be available to help you within 24 hours, and often much sooner. Updating to newer versions # This distribution is changing rapidly, and we add new features regularly. Releases are announced at http://github.com/invoke-ai/InvokeAI/releases , and at https://pypi.org/project/InvokeAI/ To update to the latest released version (recommended), follow these steps: Start the invoke.sh / invoke.bat launch script from within the invokeai root directory. Choose menu item (6) \"Developer's Console\". This will launch a new command line. Type the following command: pip install InvokeAI --upgrade 4. Watch the installation run. Once it is complete, you may exit the command line by typing exit , and then start InvokeAI from the launch script as per usual. Alternatively, if you wish to get the most recent unreleased development version, perform the same steps to enter the developer's console, and then type: pip install https://github.com/invoke-ai/InvokeAI/archive/refs/heads/main.zip","title":"Installing with the Automated Installer"},{"location":"installation/010_INSTALL_AUTOMATED/#invokeai-automated-installation","text":"","title":"InvokeAI Automated Installation"},{"location":"installation/010_INSTALL_AUTOMATED/#introduction","text":"The automated installer is a Python script that automates the steps needed to install and run InvokeAI on a stock computer running recent versions of Linux, MacOS or Windows. It will leave you with a version that runs a stable version of InvokeAI with the option to upgrade to experimental versions later.","title":"Introduction"},{"location":"installation/010_INSTALL_AUTOMATED/#walk-through","text":"Hardware Requirements : Make sure that your system meets the hardware requirements and has the appropriate GPU drivers installed. For a system with an NVIDIA card installed, you will need to install the CUDA driver, while AMD-based cards require the ROCm driver. In most cases, if you've already used the system for gaming or other graphics-intensive tasks, the appropriate drivers will already be installed. If unsure, check the GPU Driver Guide Required Space Installation requires roughly 18G of free disk space to load the libraries and recommended model weights files. Regardless of your destination disk, your system drive ( C:\\ on Windows, / on macOS/Linux) requires at least 6GB of free disk space to download and cache python dependencies. NOTE for Linux users: if your temporary directory is mounted as a tmpfs , ensure it has sufficient space. Software Requirements : Check that your system has an up-to-date Python installed. To do this, open up a command-line window (\"Terminal\" on Linux and Macintosh, \"Command\" or \"Powershell\" on Windows) and type python --version . If Python is installed, it will print out the version number. If it is version 3.9.1 or 3.10.x , you meet requirements. What to do if you have an unsupported version Go to Python Downloads and download the appropriate installer package for your platform. We recommend Version 3.10.9 , which has been extensively tested with InvokeAI. At this time we do not recommend Python 3.11. Please select your platform in the section below for platform-specific setup requirements. Windows Linux Mac During the Python configuration process, look out for a checkbox to add Python to your PATH and select it. If the install script complains that it can't find python, then open the Python installer again and choose \"Modify\" existing installation. Installation requires an up to date version of the Microsoft Visual C libraries. Please install the 2015-2022 libraries available here: https://learn.microsoft.com/en-US/cpp/windows/latest-supported-vc-redist?view=msvc-170 Please double-click on the file WinLongPathsEnabled.reg and accept the dialog box that asks you if you wish to modify your registry. This activates long filename support on your system and will prevent mysterious errors during installation. To install an appropriate version of Python on Ubuntu 22.04 and higher, run the following: sudo apt update sudo apt install -y python3 python3-pip python3-venv sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.10 3 On Ubuntu 20.04, the process is slightly different: sudo apt update sudo apt install -y software-properties-common sudo add-apt-repository -y ppa:deadsnakes/ppa sudo apt install python3.10 python3-pip python3.10-venv sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.10 3 Both python and python3 commands are now pointing at Python3.10. You can still access older versions of Python by calling python2 , python3.8 , etc. Linux systems require a couple of additional graphics libraries to be installed for proper functioning of python3-opencv . Please run the following: sudo apt update && sudo apt install -y libglib2.0-0 libgl1-mesa-glx After installing Python, you may need to run the following command from the Terminal in order to install the Web certificates needed to download model data from https sites. If you see lots of CERTIFICATE ERRORS during the last part of the install, this is the problem, and you can fix it with this command: `/Applications/Python\\ 3.10/Install\\ Certificates.command` You may need to install the Xcode command line tools. These are a set of tools that are needed to run certain applications in a Terminal, including InvokeAI. This package is provided directly by Apple. To install, open a terminal window and run xcode-select --install . You will get a macOS system popup guiding you through the install. If you already have them installed, you will instead see some output in the Terminal advising you that the tools are already installed. More information can be found at FreeCode Camp Download the Installer : The InvokeAI installer is distributed as a ZIP files. Go to the latest release , and look for a file named: InvokeAI-installer-v2.X.X.zip where \"2.X.X\" is the latest released version. The file is located at the very bottom of the release page, under Assets . Unpack the installer : Unpack the zip file into a convenient directory. This will create a new directory named \"InvokeAI-Installer\". When unpacked, the directory will look like this: Launch the installer script from the desktop : If you are using a desktop GUI, double-click the installer file appropriate for your platform. It will be named install.bat on Windows systems and install.sh on Linux and Macintosh systems. Be aware that your system's file browser may suppress the display of the file extension. On Windows systems if you get an \"Untrusted Publisher\" warning. Click on \"More Info\" and then select \"Run Anyway.\" You trust us, right? [Alternative] Launch the installer script from the command line : Alternatively, from the command line, run the shell script or .bat file: C:\\Documents\\Linco> cd InvokeAI-Installer C:\\Documents\\Linco\\invokeAI> install.bat Select the location to install InvokeAI : The script will ask you to choose where to install InvokeAI. Select a directory with at least 18G of free space for a full install. InvokeAI and all its support files will be installed into a new directory named invokeai located at the location you specify. The default is to install the invokeai directory in your home directory, usually C:\\Users\\YourName\\invokeai on Windows systems, /home/YourName/invokeai on Linux systems, and /Users/YourName/invokeai on Macintoshes, where \"YourName\" is your login name. The script uses tab autocompletion to suggest directory path completions. Type part of the path (e.g. \"C:\\Users\") and press Tab repeatedly to suggest completions. Select your GPU : The installer will autodetect your platform and will request you to confirm the type of GPU your graphics card has. On Linux systems, you will have the choice of CUDA (NVidia cards), ROCm (AMD cards), or CPU (no graphics acceleration). On Windows, you'll have the choice of CUDA vs CPU, and on Macs you'll be offered CPU only. When you select CPU on M1 or M2 Macintoshes, you will get MPS-based graphics acceleration without installing additional drivers. If you are unsure what GPU you are using, you can ask the installer to guess. Watch it go! : Sit back and let the install script work. It will install the third-party libraries needed by InvokeAI and the application itself. Be aware that some of the library download and install steps take a long time. In particular, the pytorch package is quite large and often appears to get \"stuck\" at 99.9%. Have patience and the installation step will eventually resume. However, there are occasions when the library install does legitimately get stuck. If you have been waiting for more than ten minutes and nothing is happening, you can interrupt the script with ^C. You may restart it and it will pick up where it left off. Post-install Configuration : After installation completes, the installer will launch the configuration script, which will guide you through the first-time process of selecting one or more Stable Diffusion model weights files, downloading and configuring them. We provide a list of popular models that InvokeAI performs well with. However, you can add more weight files later on using the command-line client or the Web UI. See Installing Models for details. If you have already downloaded the weights file(s) for another Stable Diffusion distribution, you may skip this step (by selecting \"skip\" when prompted) and configure InvokeAI to use the previously-downloaded files. The process for this is described in Installing Models . Running InvokeAI for the first time : The script will now exit and you'll be ready to generate some images. Look for the directory invokeai installed in the location you chose at the beginning of the install session. Look for a shell script named invoke.sh (Linux/Mac) or invoke.bat (Windows). Launch the script by double-clicking it or typing its name at the command-line: C:\\Documents\\Linco> cd invokeai C:\\Documents\\Linco\\invokeAI> invoke.bat The invoke.bat ( invoke.sh ) script will give you the choice of starting (1) the command-line interface, (2) the web GUI, (3) textual inversion training, and (4) model merging. By default, the script will launch the web interface. When you do this, you'll see a series of startup messages ending with instructions to point your browser at http://localhost:9090 . Click on this link to open up a browser and start exploring InvokeAI's features. InvokeAI Options : You can launch InvokeAI with several different command-line arguments that customize its behavior. For example, you can change the location of the image output directory, or select your favorite sampler. See the Command-Line Interface for a full list of the options. To set defaults that will take effect every time you launch InvokeAI, use a text editor (e.g. Notepad) to exit the file invokeai\\invokeai.init . It contains a variety of examples that you can follow to add and modify launch options. The launcher script also offers you an option labeled \"open the developer console\". If you choose this option, you will be dropped into a command-line interface in which you can run python commands directly, access developer tools, and launch InvokeAI with customized options. Do not move or remove the invokeai directory The invokeai directory contains the invokeai application, its configuration files, the model weight files, and outputs of image generation. Once InvokeAI is installed, do not move or remove this directory.\"","title":"Walk through"},{"location":"installation/010_INSTALL_AUTOMATED/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/010_INSTALL_AUTOMATED/#package-dependency-conflicts","text":"If you have previously installed InvokeAI or another Stable Diffusion package, the installer may occasionally pick up outdated libraries and either the installer or invoke will fail with complaints about library conflicts. In this case, run the invoke.sh / invoke.bat command and enter the Developer's Console by picking option (5). This will take you to a command-line prompt. Then give this command: pip install InvokeAI --force-reinstall This should fix the issues.","title":"Package dependency conflicts"},{"location":"installation/010_INSTALL_AUTOMATED/#invokeai-runs-extremely-slowly-on-linux-or-windows-systems","text":"The most frequent cause of this problem is when the installation process installed the CPU-only version of the torch machine-learning library, rather than a version that takes advantage of GPU acceleration. To confirm this issue, look at the InvokeAI startup messages. If you see a message saying \">> Using device CPU\", then this is what happened. To fix this problem, first determine whether you have an NVidia or an AMD GPU. The former uses the CUDA driver, and the latter uses ROCm (only available on Linux). Then run the invoke.sh / invoke.bat command and enter the Developer's Console by picking option (5). This will take you to a command-line prompt. Then type the following commands: NVIDIA System AMD System pip install torch torchvision --force-reinstall --extra-index-url https://download.pytorch.org/whl/cu117 pip install xformers pip install torch torchvision --force-reinstall --extra-index-url https://download.pytorch.org/whl/rocm5.2","title":"InvokeAI runs extremely slowly on Linux or Windows systems"},{"location":"installation/010_INSTALL_AUTOMATED/#corrupted-configuration-file","text":"Everything seems to install ok, but invokeai complains of a corrupted configuration file and goes back into the configuration process (asking you to download models, etc), but this doesn't fix the problem. This issue is often caused by a misconfigured configuration directive in the invokeai\\invokeai.init initialization file that contains startup settings. The easiest way to fix the problem is to move the file out of the way and re-run invokeai-configure . Enter the developer's console (option 3 of the launcher script) and run this command: invokeai-configure --root=. Note the dot (.) after --root . It is part of the command. If none of these maneuvers fixes the problem then please report the problem to the InvokeAI Issues section, or visit our Discord Server for interactive assistance.","title":"Corrupted configuration file"},{"location":"installation/010_INSTALL_AUTOMATED/#other-problems","text":"If you run into problems during or after installation, the InvokeAI team is available to help you. Either create an Issue at our GitHub site, or make a request for help on the \"bugs-and-support\" channel of our Discord server . We are a 100% volunteer organization, but typically somebody will be available to help you within 24 hours, and often much sooner.","title":"Other Problems"},{"location":"installation/010_INSTALL_AUTOMATED/#updating-to-newer-versions","text":"This distribution is changing rapidly, and we add new features regularly. Releases are announced at http://github.com/invoke-ai/InvokeAI/releases , and at https://pypi.org/project/InvokeAI/ To update to the latest released version (recommended), follow these steps: Start the invoke.sh / invoke.bat launch script from within the invokeai root directory. Choose menu item (6) \"Developer's Console\". This will launch a new command line. Type the following command: pip install InvokeAI --upgrade 4. Watch the installation run. Once it is complete, you may exit the command line by typing exit , and then start InvokeAI from the launch script as per usual. Alternatively, if you wish to get the most recent unreleased development version, perform the same steps to enter the developer's console, and then type: pip install https://github.com/invoke-ai/InvokeAI/archive/refs/heads/main.zip","title":"Updating to newer versions"},{"location":"installation/020_INSTALL_MANUAL/","text":"Linux | macOS | Windows # This is for advanced Users python experience is mandatory Introduction # Conda As of InvokeAI v2.3.0 installation using the conda package manager is no longer being supported. It will likely still work, but we are not testing this installation method. On Windows systems, you are encouraged to install and use the PowerShell , which provides compatibility with Linux and Mac shells and nice features such as command-line completion. Prerequisites # Before you start, make sure you have the following preqrequisites installed. These are described in more detail in Automated Installation , and in many cases will already be installed (if, for example, you have used your system for gaming): Python version 3.9 or 3.10 (3.11 is not recommended). CUDA Tools For those with NVidia GPUs , you will need to install the CUDA toolkit and optionally the XFormers library . ROCm Tools For Linux users with AMD GPUs , you will need to install the ROCm toolkit . Note that InvokeAI does not support AMD GPUs on Windows systems due to lack of a Windows ROCm library. Visual C++ Libraries Windows users must install the free Visual C++ libraries from Microsoft The Xcode command line tools for Macintosh users . Instructions are available at Free Code Camp Macintosh users may also need to run the Install Certificates command if model downloads give lots of certificate errors. Run: /Applications/Python\\ 3.10/Install\\ Certificates.command Installation Walkthrough # To install InvokeAI with virtual environments and the PIP package manager, please follow these steps: Please make sure you are using Python 3.9 or 3.10. The rest of the install procedure depends on this and will not work with other versions: python -V Create a directory to contain your InvokeAI library, configuration files, and models. This is known as the \"runtime\" or \"root\" directory, and often lives in your home directory under the name invokeai . Please keep in mind the disk space requirements - you will need at least 20GB for the models and the virtual environment. From now on we will refer to this directory as INVOKEAI_ROOT . For convenience, the steps below create a shell variable of that name which contains the path to HOME/invokeai . Linux/Mac Windows (Powershell) export INVOKEAI_ROOT = ~/invokeai mkdir $INVOKEAI_ROOT Set-Variable -Name INVOKEAI_ROOT -Value $Home /invokeai mkdir $INVOKEAI_ROOT Enter the root (invokeai) directory and create a virtual Python environment within it named .venv . If the command python doesn't work, try python3 . Note that while you may create the virtual environment anywhere in the file system, we recommend that you create it within the root directory as shown here. This makes it possible for the InvokeAI applications to find the model data and configuration. If you do not choose to install the virtual environment inside the root directory, then you must set the INVOKEAI_ROOT environment variable in your shell environment, for example, by editing ~/.bashrc or ~/.zshrc files, or setting the Windows environment variable using the Advanced System Settings dialogue. Refer to your operating system documentation for details. cd $INVOKEAI_ROOT python -m venv .venv --prompt InvokeAI Activate the new environment: Linux/Mac Windows source .venv/bin/activate .venv\\Scripts\\activate If you get a permissions error at this point, run this command and try again Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser The command-line prompt should change to to show (InvokeAI) at the beginning of the prompt. Note that all the following steps should be run while inside the INVOKEAI_ROOT directory Make sure that pip is installed in your virtual environment and up to date: python -m pip install --upgrade pip Install the InvokeAI Package. The --extra-index-url option is used to select among CUDA, ROCm and CPU/MPS drivers as shown below: CUDA (NVidia) ROCm (AMD) CPU (Intel Macs & non-GPU systems) MPS (M1 and M2 Macs) pip install InvokeAI [ xformers ] --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 pip install InvokeAI --use-pep517 --extra-index-url https://download.pytorch.org/whl/rocm5.2 pip install InvokeAI --use-pep517 --extra-index-url https://download.pytorch.org/whl/cpu pip install InvokeAI --use-pep517 Deactivate and reactivate your runtime directory so that the invokeai-specific commands become available in the environment Linux/Macintosh Windows deactivate && source .venv/bin/activate deactivate .venv\\Scripts\\activate Set up the runtime directory In this step you will initialize your runtime directory with the downloaded models, model config files, directory for textual inversion embeddings, and your outputs. invokeai-configure The script invokeai-configure will interactively guide you through the process of downloading and installing the weights files needed for InvokeAI. Note that the main Stable Diffusion weights file is protected by a license agreement that you have to agree to. The script will list the steps you need to take to create an account on the site that hosts the weights files, accept the agreement, and provide an access token that allows InvokeAI to legally download and install the weights files. If you get an error message about a module not being installed, check that the invokeai environment is active and if not, repeat step 5. Tip If you have already downloaded the weights file(s) for another Stable Diffusion distribution, you may skip this step (by selecting \"skip\" when prompted) and configure InvokeAI to use the previously-downloaded files. The process for this is described in Installing Models . Run the command-line- or the web- interface: From within INVOKEAI_ROOT, activate the environment (with source .venv/bin/activate or .venv\\scripts\\activate), and then run the script invokeai . If the virtual environment you selected is NOT inside INVOKEAI_ROOT, then you must specify the path to the root directory by adding --root_dir \\path\\to\\invokeai` to the commands below: Make sure that the virtual environment is activated, which should create (.venv) in front of your prompt! CLI local Webserver Public Webserver invokeai invokeai --web invokeai --web --host 0 .0.0.0 If you choose the run the web interface, point your browser at http://localhost:9090 in order to load the GUI. Tip You can permanently set the location of the runtime directory by setting the environment variable INVOKEAI_ROOT to the path of the directory. As mentioned previously, this is highly recommended* if your virtual environment is located outside of your runtime directory. Render away! Browse the features section to learn about all the things you can do with InvokeAI. Subsequently, to relaunch the script, activate the virtual environment, and then launch invokeai command. If you forget to activate the virtual environment you will most likeley receive a command not found error. Warning Do not move the runtime directory after installation. The virtual environment will get confused if the directory is moved. Other scripts The Textual Inversion script can be launched with the command: invokeai-ti --gui Similarly, the Model Merging script can be launched with the command: invokeai-merge --gui Leave off the --gui option to run the script using command-line arguments. Pass the --help argument to get usage instructions. Developer Install # If you have an interest in how InvokeAI works, or you would like to add features or bugfixes, you are encouraged to install the source code for InvokeAI. For this to work, you will need to install the git source code management program. If it is not already installed on your system, please see the Git Installation Guide From the command line, run this command: git clone https://github.com/invoke-ai/InvokeAI.git This will create a directory named InvokeAI and populate it with the full source code from the InvokeAI repository. Activate the InvokeAI virtual environment as per step (4) of the manual installation protocol (important!) Enter the InvokeAI repository directory and run one of these commands, based on your GPU: CUDA (NVidia) ROCm (AMD) CPU (Intel Macs & non-GPU systems) MPS (M1 and M2 Macs) pip install -e . [ xformers ] --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 pip install -e . --use-pep517 --extra-index-url https://download.pytorch.org/whl/rocm5.2 pip install -e . --use-pep517 --extra-index-url https://download.pytorch.org/whl/cpu pip install -e . --use-pep517 Be sure to pass -e (for an editable install) and don't forget the dot (\".\"). It is part of the command. You can now run invokeai and its related commands. The code will be read from the repository, so that you can edit the .py source files and watch the code's behavior change. If you wish to contribute to the InvokeAI project, you are encouraged to establish a GitHub account and \"fork\" https://github.com/invoke-ai/InvokeAI into your own copy of the repository. You can then use GitHub functions to create and submit pull requests to contribute improvements to the project. Please see Contributing for hints on getting started. Unsupported Conda Install # Congratulations, you found the \"secret\" Conda installation instructions. If you really really want to use Conda with InvokeAI you can do so using this unsupported recipe: mkdir ~/invokeai conda create -n invokeai python=3.10 conda activate invokeai pip install InvokeAI[xformers] --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 invokeai-configure --root ~/invokeai invokeai --root ~/invokeai --web The pip install command shown in this recipe is for Linux/Windows systems with an NVIDIA GPU. See step (6) above for the command to use with other platforms/GPU combinations. If you don't wish to pass the --root argument to invokeai with each launch, you may set the environment variable INVOKEAI_ROOT to point to the installation directory. Note that if you run into problems with the Conda installation, the InvokeAI staff will not be able to help you out. Caveat Emptor!","title":"Installing Manually"},{"location":"installation/020_INSTALL_MANUAL/#linux-macos-windows","text":"This is for advanced Users python experience is mandatory","title":" Linux |  macOS |  Windows"},{"location":"installation/020_INSTALL_MANUAL/#introduction","text":"Conda As of InvokeAI v2.3.0 installation using the conda package manager is no longer being supported. It will likely still work, but we are not testing this installation method. On Windows systems, you are encouraged to install and use the PowerShell , which provides compatibility with Linux and Mac shells and nice features such as command-line completion.","title":"Introduction"},{"location":"installation/020_INSTALL_MANUAL/#prerequisites","text":"Before you start, make sure you have the following preqrequisites installed. These are described in more detail in Automated Installation , and in many cases will already be installed (if, for example, you have used your system for gaming): Python version 3.9 or 3.10 (3.11 is not recommended). CUDA Tools For those with NVidia GPUs , you will need to install the CUDA toolkit and optionally the XFormers library . ROCm Tools For Linux users with AMD GPUs , you will need to install the ROCm toolkit . Note that InvokeAI does not support AMD GPUs on Windows systems due to lack of a Windows ROCm library. Visual C++ Libraries Windows users must install the free Visual C++ libraries from Microsoft The Xcode command line tools for Macintosh users . Instructions are available at Free Code Camp Macintosh users may also need to run the Install Certificates command if model downloads give lots of certificate errors. Run: /Applications/Python\\ 3.10/Install\\ Certificates.command","title":"Prerequisites"},{"location":"installation/020_INSTALL_MANUAL/#installation-walkthrough","text":"To install InvokeAI with virtual environments and the PIP package manager, please follow these steps: Please make sure you are using Python 3.9 or 3.10. The rest of the install procedure depends on this and will not work with other versions: python -V Create a directory to contain your InvokeAI library, configuration files, and models. This is known as the \"runtime\" or \"root\" directory, and often lives in your home directory under the name invokeai . Please keep in mind the disk space requirements - you will need at least 20GB for the models and the virtual environment. From now on we will refer to this directory as INVOKEAI_ROOT . For convenience, the steps below create a shell variable of that name which contains the path to HOME/invokeai . Linux/Mac Windows (Powershell) export INVOKEAI_ROOT = ~/invokeai mkdir $INVOKEAI_ROOT Set-Variable -Name INVOKEAI_ROOT -Value $Home /invokeai mkdir $INVOKEAI_ROOT Enter the root (invokeai) directory and create a virtual Python environment within it named .venv . If the command python doesn't work, try python3 . Note that while you may create the virtual environment anywhere in the file system, we recommend that you create it within the root directory as shown here. This makes it possible for the InvokeAI applications to find the model data and configuration. If you do not choose to install the virtual environment inside the root directory, then you must set the INVOKEAI_ROOT environment variable in your shell environment, for example, by editing ~/.bashrc or ~/.zshrc files, or setting the Windows environment variable using the Advanced System Settings dialogue. Refer to your operating system documentation for details. cd $INVOKEAI_ROOT python -m venv .venv --prompt InvokeAI Activate the new environment: Linux/Mac Windows source .venv/bin/activate .venv\\Scripts\\activate If you get a permissions error at this point, run this command and try again Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser The command-line prompt should change to to show (InvokeAI) at the beginning of the prompt. Note that all the following steps should be run while inside the INVOKEAI_ROOT directory Make sure that pip is installed in your virtual environment and up to date: python -m pip install --upgrade pip Install the InvokeAI Package. The --extra-index-url option is used to select among CUDA, ROCm and CPU/MPS drivers as shown below: CUDA (NVidia) ROCm (AMD) CPU (Intel Macs & non-GPU systems) MPS (M1 and M2 Macs) pip install InvokeAI [ xformers ] --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 pip install InvokeAI --use-pep517 --extra-index-url https://download.pytorch.org/whl/rocm5.2 pip install InvokeAI --use-pep517 --extra-index-url https://download.pytorch.org/whl/cpu pip install InvokeAI --use-pep517 Deactivate and reactivate your runtime directory so that the invokeai-specific commands become available in the environment Linux/Macintosh Windows deactivate && source .venv/bin/activate deactivate .venv\\Scripts\\activate Set up the runtime directory In this step you will initialize your runtime directory with the downloaded models, model config files, directory for textual inversion embeddings, and your outputs. invokeai-configure The script invokeai-configure will interactively guide you through the process of downloading and installing the weights files needed for InvokeAI. Note that the main Stable Diffusion weights file is protected by a license agreement that you have to agree to. The script will list the steps you need to take to create an account on the site that hosts the weights files, accept the agreement, and provide an access token that allows InvokeAI to legally download and install the weights files. If you get an error message about a module not being installed, check that the invokeai environment is active and if not, repeat step 5. Tip If you have already downloaded the weights file(s) for another Stable Diffusion distribution, you may skip this step (by selecting \"skip\" when prompted) and configure InvokeAI to use the previously-downloaded files. The process for this is described in Installing Models . Run the command-line- or the web- interface: From within INVOKEAI_ROOT, activate the environment (with source .venv/bin/activate or .venv\\scripts\\activate), and then run the script invokeai . If the virtual environment you selected is NOT inside INVOKEAI_ROOT, then you must specify the path to the root directory by adding --root_dir \\path\\to\\invokeai` to the commands below: Make sure that the virtual environment is activated, which should create (.venv) in front of your prompt! CLI local Webserver Public Webserver invokeai invokeai --web invokeai --web --host 0 .0.0.0 If you choose the run the web interface, point your browser at http://localhost:9090 in order to load the GUI. Tip You can permanently set the location of the runtime directory by setting the environment variable INVOKEAI_ROOT to the path of the directory. As mentioned previously, this is highly recommended* if your virtual environment is located outside of your runtime directory. Render away! Browse the features section to learn about all the things you can do with InvokeAI. Subsequently, to relaunch the script, activate the virtual environment, and then launch invokeai command. If you forget to activate the virtual environment you will most likeley receive a command not found error. Warning Do not move the runtime directory after installation. The virtual environment will get confused if the directory is moved. Other scripts The Textual Inversion script can be launched with the command: invokeai-ti --gui Similarly, the Model Merging script can be launched with the command: invokeai-merge --gui Leave off the --gui option to run the script using command-line arguments. Pass the --help argument to get usage instructions.","title":"Installation Walkthrough"},{"location":"installation/020_INSTALL_MANUAL/#developer-install","text":"If you have an interest in how InvokeAI works, or you would like to add features or bugfixes, you are encouraged to install the source code for InvokeAI. For this to work, you will need to install the git source code management program. If it is not already installed on your system, please see the Git Installation Guide From the command line, run this command: git clone https://github.com/invoke-ai/InvokeAI.git This will create a directory named InvokeAI and populate it with the full source code from the InvokeAI repository. Activate the InvokeAI virtual environment as per step (4) of the manual installation protocol (important!) Enter the InvokeAI repository directory and run one of these commands, based on your GPU: CUDA (NVidia) ROCm (AMD) CPU (Intel Macs & non-GPU systems) MPS (M1 and M2 Macs) pip install -e . [ xformers ] --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 pip install -e . --use-pep517 --extra-index-url https://download.pytorch.org/whl/rocm5.2 pip install -e . --use-pep517 --extra-index-url https://download.pytorch.org/whl/cpu pip install -e . --use-pep517 Be sure to pass -e (for an editable install) and don't forget the dot (\".\"). It is part of the command. You can now run invokeai and its related commands. The code will be read from the repository, so that you can edit the .py source files and watch the code's behavior change. If you wish to contribute to the InvokeAI project, you are encouraged to establish a GitHub account and \"fork\" https://github.com/invoke-ai/InvokeAI into your own copy of the repository. You can then use GitHub functions to create and submit pull requests to contribute improvements to the project. Please see Contributing for hints on getting started.","title":"Developer Install"},{"location":"installation/020_INSTALL_MANUAL/#unsupported-conda-install","text":"Congratulations, you found the \"secret\" Conda installation instructions. If you really really want to use Conda with InvokeAI you can do so using this unsupported recipe: mkdir ~/invokeai conda create -n invokeai python=3.10 conda activate invokeai pip install InvokeAI[xformers] --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 invokeai-configure --root ~/invokeai invokeai --root ~/invokeai --web The pip install command shown in this recipe is for Linux/Windows systems with an NVIDIA GPU. See step (6) above for the command to use with other platforms/GPU combinations. If you don't wish to pass the --root argument to invokeai with each launch, you may set the environment variable INVOKEAI_ROOT to point to the installation directory. Note that if you run into problems with the Conda installation, the InvokeAI staff will not be able to help you out. Caveat Emptor!","title":"Unsupported Conda Install"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/","text":"CUDA | ROCm # In order for InvokeAI to run at full speed, you will need a graphics card with a supported GPU. InvokeAI supports NVidia cards via the CUDA driver on Windows and Linux, and AMD cards via the ROCm driver on Linux. CUDA # Linux and Windows Install # If you have used your system for other graphics-intensive tasks, such as gaming, you may very well already have the CUDA drivers installed. To confirm, open up a command-line window and type: nvidia-smi If this command produces a status report on the GPU(s) installed on your system, CUDA is installed and you have no more work to do. If instead you get \"command not found\", or similar, then the driver will need to be installed. We strongly recommend that you install the CUDA Toolkit package directly from NVIDIA. Do not try to install Ubuntu's nvidia-cuda-toolkit package. It is out of date and will cause conflicts among the NVIDIA driver and binaries. Go to CUDA Toolkit 11.7 Downloads , and use the target selection wizard to choose your operating system, hardware platform, and preferred installation method (e.g. \"local\" versus \"network\"). This will provide you with a downloadable install file or, depending on your choices, a recipe for downloading and running a install shell script. Be sure to read and follow the full installation instructions. After an install that seems successful, you can confirm by again running nvidia-smi from the command line. Linux Install with a Runtime Container # On Linux systems, an alternative to installing CUDA Toolkit directly on your system is to run an NVIDIA software container that has the CUDA libraries already in place. This is recommended if you are already familiar with containerization technologies such as Docker. For downloads and instructions, visit the NVIDIA CUDA Container Runtime Site Torch Installation # When installing torch and torchvision manually with pip , remember to provide the argument --extra-index-url https://download.pytorch.org/whl/cu117 as described in the Manual Installation Guide . ROCm # Linux Install # AMD GPUs are only supported on Linux platforms due to the lack of a Windows ROCm driver at the current time. Also be aware that support for newer AMD GPUs is spotty. Your mileage may vary. It is possible that the ROCm driver is already installed on your machine. To test, open up a terminal window and issue the following command: rocm-smi If you get a table labeled \"ROCm System Management Interface\" the driver is installed and you are done. If you get \"command not found,\" then the driver needs to be installed. Go to AMD's ROCm Downloads Guide and scroll to the Installation Methods section. Find the subsection for the install method for your preferred Linux distribution, and issue the commands given in the recipe. Annoyingly, the official AMD site does not have a recipe for the most recent version of Ubuntu, 22.04. However, this community-contributed recipe is reported to work well. After installation, please run rocm-smi a second time to confirm that the driver is present and the GPU is recognized. You may need to do a reboot in order to load the driver. Linux Install with a ROCm-docker Container # If you are comfortable with the Docker containerization system, then you can build a ROCm docker file. The source code and installation recipes are available Here Torch Installation # When installing torch and torchvision manually with pip , remember to provide the argument --extra-index-url https://download.pytorch.org/whl/rocm5.2 as described in the Manual Installation Guide . This will be done automatically for you if you use the installer script. Be aware that the torch machine learning library does not seamlessly interoperate with all AMD GPUs and you may experience garbled images, black images, or long startup delays before rendering commences. Most of these issues can be solved by Googling for workarounds. If you have a problem and find a solution, please post an Issue so that other users benefit and we can update this document.","title":"NVIDIA Cuda / AMD ROCm"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#cuda-rocm","text":"In order for InvokeAI to run at full speed, you will need a graphics card with a supported GPU. InvokeAI supports NVidia cards via the CUDA driver on Windows and Linux, and AMD cards via the ROCm driver on Linux.","title":" CUDA |  ROCm"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#cuda","text":"","title":" CUDA"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#linux-and-windows-install","text":"If you have used your system for other graphics-intensive tasks, such as gaming, you may very well already have the CUDA drivers installed. To confirm, open up a command-line window and type: nvidia-smi If this command produces a status report on the GPU(s) installed on your system, CUDA is installed and you have no more work to do. If instead you get \"command not found\", or similar, then the driver will need to be installed. We strongly recommend that you install the CUDA Toolkit package directly from NVIDIA. Do not try to install Ubuntu's nvidia-cuda-toolkit package. It is out of date and will cause conflicts among the NVIDIA driver and binaries. Go to CUDA Toolkit 11.7 Downloads , and use the target selection wizard to choose your operating system, hardware platform, and preferred installation method (e.g. \"local\" versus \"network\"). This will provide you with a downloadable install file or, depending on your choices, a recipe for downloading and running a install shell script. Be sure to read and follow the full installation instructions. After an install that seems successful, you can confirm by again running nvidia-smi from the command line.","title":"Linux and Windows Install"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#linux-install-with-a-runtime-container","text":"On Linux systems, an alternative to installing CUDA Toolkit directly on your system is to run an NVIDIA software container that has the CUDA libraries already in place. This is recommended if you are already familiar with containerization technologies such as Docker. For downloads and instructions, visit the NVIDIA CUDA Container Runtime Site","title":"Linux Install with a Runtime Container"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#torch-installation","text":"When installing torch and torchvision manually with pip , remember to provide the argument --extra-index-url https://download.pytorch.org/whl/cu117 as described in the Manual Installation Guide .","title":"Torch Installation"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#rocm","text":"","title":" ROCm"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#linux-install","text":"AMD GPUs are only supported on Linux platforms due to the lack of a Windows ROCm driver at the current time. Also be aware that support for newer AMD GPUs is spotty. Your mileage may vary. It is possible that the ROCm driver is already installed on your machine. To test, open up a terminal window and issue the following command: rocm-smi If you get a table labeled \"ROCm System Management Interface\" the driver is installed and you are done. If you get \"command not found,\" then the driver needs to be installed. Go to AMD's ROCm Downloads Guide and scroll to the Installation Methods section. Find the subsection for the install method for your preferred Linux distribution, and issue the commands given in the recipe. Annoyingly, the official AMD site does not have a recipe for the most recent version of Ubuntu, 22.04. However, this community-contributed recipe is reported to work well. After installation, please run rocm-smi a second time to confirm that the driver is present and the GPU is recognized. You may need to do a reboot in order to load the driver.","title":"Linux Install"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#linux-install-with-a-rocm-docker-container","text":"If you are comfortable with the Docker containerization system, then you can build a ROCm docker file. The source code and installation recipes are available Here","title":"Linux Install with a ROCm-docker Container"},{"location":"installation/030_INSTALL_CUDA_AND_ROCM/#torch-installation_1","text":"When installing torch and torchvision manually with pip , remember to provide the argument --extra-index-url https://download.pytorch.org/whl/rocm5.2 as described in the Manual Installation Guide . This will be done automatically for you if you use the installer script. Be aware that the torch machine learning library does not seamlessly interoperate with all AMD GPUs and you may experience garbled images, black images, or long startup delays before rendering commences. Most of these issues can be solved by Googling for workarounds. If you have a problem and find a solution, please post an Issue so that other users benefit and we can update this document.","title":"Torch Installation"},{"location":"installation/040_INSTALL_DOCKER/","text":"Docker # For end users We highly recommend to Install InvokeAI locally using these instructions For developers For container-related development tasks or for enabling easy deployment to other environments (on-premises or cloud), follow these instructions. For general use, install locally to leverage your machine's GPU. Why containers? # They provide a flexible, reliable way to build and deploy InvokeAI. You'll also use a Docker volume to store the largest model files and image outputs as a first step in decoupling storage and compute. Future enhancements can do this for other assets. See Processes under the Twelve-Factor App methodology for details on why running applications in such a stateless fashion is important. You can specify the target platform when building the image and running the container. You'll also need to specify the InvokeAI requirements file that matches the container's OS and the architecture it will run on. Developers on Apple silicon (M1/M2): You can't access your GPU cores from Docker containers and performance is reduced compared with running it directly on macOS but for development purposes it's fine. Once you're done with development tasks on your laptop you can build for the target platform and architecture and deploy to another environment with NVIDIA GPUs on-premises or in the cloud. Installation in a Linux container (desktop) # Prerequisites # Install Docker # On the Docker Desktop app , go to Preferences, Resources, Advanced. Increase the CPUs and Memory to avoid this Issue . You may need to increase Swap and Disk image size too. Get a Huggingface-Token # Besides the Docker Agent you will need an Account on huggingface.co . After you succesfully registered your account, go to huggingface.co/settings/tokens , create a token and copy it, since you will need in for the next step. Setup # Set the fork you want to use and other variables. Tip I preffer to save my env vars in the repository root in a .env (or .envrc ) file to automatically re-apply them when I come back. The build- and run- scripts contain default values for almost everything, besides the Hugging Face Token you created in the last step. Some Suggestions of variables you may want to change besides the Token: Environment-Variable Default value Description HUGGING_FACE_HUB_TOKEN No default, but required ! This is the only required variable, without it you can't download the huggingface models REPOSITORY_NAME The Basename of the Repo folder This name will used as the container repository/image name VOLUMENAME ${REPOSITORY_NAME,,}_data Name of the Docker Volume where model files will be stored ARCH arch of the build machine Can be changed if you want to build the image for another arch CONTAINER_REGISTRY ghcr.io Name of the Container Registry to use for the full tag CONTAINER_REPOSITORY $(whoami)/${REPOSITORY_NAME} Name of the Container Repository CONTAINER_FLAVOR cuda The flavor of the image to built, available options are cuda , rocm and cpu . If you choose rocm or cpu , the extra-index-url will be selected automatically, unless you set one yourself. CONTAINER_TAG ${INVOKEAI_BRANCH##*/}-${CONTAINER_FLAVOR} The Container Repository / Tag which will be used INVOKE_DOCKERFILE Dockerfile The Dockerfile which should be built, handy for development PIP_EXTRA_INDEX_URL If you want to use a custom pip-extra-index-url Build the Image # I provided a build script, which is located next to the Dockerfile in docker/build.sh . It can be executed from repository root like this: ./docker/build.sh The build Script not only builds the container, but also creates the docker volume if not existing yet. Run the Container # After the build process is done, you can run the container via the provided docker/run.sh script ./docker/run.sh When used without arguments, the container will start the webserver and provide you the link to open it. But if you want to use some other parameters you can also do so. run script example ./docker/run.sh \"banana sushi\" -Ak_lms -S42 -s10 This would generate the legendary \"banana sushi\" with Seed 42, k_lms Sampler and 10 steps. Find out more about available CLI-Parameters at features/CLI.md Running the container on your GPU # If you have an Nvidia GPU, you can enable InvokeAI to run on the GPU by running the container with an extra environment variable to enable GPU usage and have the process run much faster: GPU_FLAGS = all ./docker/run.sh This passes the --gpus all to docker and uses the GPU. If you don't have a GPU (or your host is not yet setup to use it) you will see a message like this: docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]]. You can use the full set of GPU combinations documented here: https://docs.docker.com/config/containers/resource_constraints/#gpu For example, use GPU_FLAGS=device=GPU-3a23c669-1f69-c64e-cf85-44e9b07e7a2a to choose a specific device identified by a UUID. Deprecated From here on you will find the the previous Docker-Docs, which will still provide some usefull informations. Usage (time to have fun) # Startup # If you're on a Linux container the invoke script is automatically started and the output dir set to the Docker volume you created earlier. If you're directly on macOS follow these startup instructions . With the Conda environment activated ( conda activate ldm ), run the interactive interface that combines the functionality of the original scripts txt2img and img2img : Use the more accurate but VRAM-intensive full precision math because half-precision requires autocast and won't work. By default the images are saved in outputs/img-samples/ . python3 scripts/invoke.py --full_precision You'll get the script's prompt. You can see available options or quit. invoke> -h invoke> q Text to Image # For quick (but bad) image results test with 5 steps (default 50) and 1 sample image. This will let you know that everything is set up correctly. Then increase steps to 100 or more for good (but slower) results. The prompt can be in quotes or not. invoke> The hulk fighting with sheldon cooper -s5 -n1 invoke> \"woman closeup highly detailed\" -s 150 # Reuse previous seed and apply face restoration invoke> \"woman closeup highly detailed\" --steps 150 --seed -1 -G 0 .75 You'll need to experiment to see if face restoration is making it better or worse for your specific prompt. If you're on a container the output is set to the Docker volume. You can copy it wherever you want. You can download it from the Docker Desktop app, Volumes, my-vol, data. Or you can copy it from your Mac terminal. Keep in mind docker cp can't expand *.png so you'll need to specify the image file name. On your host Mac (you can use the name of any container that mounted the volume): docker cp dummy:/data/000001.928403745.png /Users/<your-user>/Pictures Image to Image # You can also do text-guided image-to-image translation. For example, turning a sketch into a detailed drawing. strength is a value between 0.0 and 1.0 that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. 0.0 preserves image exactly, 1.0 replaces it completely. Make sure your input image size dimensions are multiples of 64 e.g. 512x512. Otherwise you'll get Error: product of dimension sizes > 2**31' . If you still get the error try a different size like 512x256. If you're on a Docker container, copy your input image into the Docker volume docker cp /Users/<your-user>/Pictures/sketch-mountains-input.jpg dummy:/data/ Try it out generating an image (or more). The invoke script needs absolute paths to find the image so don't use ~ . If you're on your Mac invoke> \"A fantasy landscape, trending on artstation\" -I /Users/<your-user>/Pictures/sketch-mountains-input.jpg --strength 0 .75 --steps 100 -n4 If you're on a Linux container on your Mac invoke> \"A fantasy landscape, trending on artstation\" -I /data/sketch-mountains-input.jpg --strength 0 .75 --steps 50 -n1 Web Interface # You can use the invoke script with a graphical web interface. Start the web server with: python3 scripts/invoke.py --full_precision --web If it's running on your Mac point your Mac web browser to http://127.0.0.1:9090 Press Control-C at the command line to stop the web server. Notes # Some text you can add at the end of the prompt to make it very pretty: cinematic photo, highly detailed, cinematic lighting, ultra-detailed, ultrarealistic, photorealism, Octane Rendering, cyberpunk lights, Hyper Detail, 8K, HD, Unreal Engine, V-Ray, full hd, cyberpunk, abstract, 3d octane render + 4k UHD + immense detail + dramatic lighting + well lit + black, purple, blue, pink, cerulean, teal, metallic colours, + fine details, ultra photoreal, photographic, concept art, cinematic composition, rule of thirds, mysterious, eerie, photorealism, breathtaking detailed, painting art deco pattern, by hsiao, ron cheng, john james audubon, bizarre compositions, exquisite detail, extremely moody lighting, painted by greg rutkowski makoto shinkai takashi takeuchi studio ghibli, akihiko yoshida The original scripts should work as well. python3 scripts/orig_scripts/txt2img.py --help python3 scripts/orig_scripts/txt2img.py --ddim_steps 100 --n_iter 1 --n_samples 1 --plms --prompt \"new born baby kitten. Hyper Detail, Octane Rendering, Unreal Engine, V-Ray\" python3 scripts/orig_scripts/txt2img.py --ddim_steps 5 --n_iter 1 --n_samples 1 --plms --prompt \"ocean\" # or --klms","title":"Installing with Docker"},{"location":"installation/040_INSTALL_DOCKER/#docker","text":"For end users We highly recommend to Install InvokeAI locally using these instructions For developers For container-related development tasks or for enabling easy deployment to other environments (on-premises or cloud), follow these instructions. For general use, install locally to leverage your machine's GPU.","title":" Docker"},{"location":"installation/040_INSTALL_DOCKER/#why-containers","text":"They provide a flexible, reliable way to build and deploy InvokeAI. You'll also use a Docker volume to store the largest model files and image outputs as a first step in decoupling storage and compute. Future enhancements can do this for other assets. See Processes under the Twelve-Factor App methodology for details on why running applications in such a stateless fashion is important. You can specify the target platform when building the image and running the container. You'll also need to specify the InvokeAI requirements file that matches the container's OS and the architecture it will run on. Developers on Apple silicon (M1/M2): You can't access your GPU cores from Docker containers and performance is reduced compared with running it directly on macOS but for development purposes it's fine. Once you're done with development tasks on your laptop you can build for the target platform and architecture and deploy to another environment with NVIDIA GPUs on-premises or in the cloud.","title":"Why containers?"},{"location":"installation/040_INSTALL_DOCKER/#installation-in-a-linux-container-desktop","text":"","title":"Installation in a Linux container (desktop)"},{"location":"installation/040_INSTALL_DOCKER/#prerequisites","text":"","title":"Prerequisites"},{"location":"installation/040_INSTALL_DOCKER/#install-docker","text":"On the Docker Desktop app , go to Preferences, Resources, Advanced. Increase the CPUs and Memory to avoid this Issue . You may need to increase Swap and Disk image size too.","title":"Install Docker"},{"location":"installation/040_INSTALL_DOCKER/#get-a-huggingface-token","text":"Besides the Docker Agent you will need an Account on huggingface.co . After you succesfully registered your account, go to huggingface.co/settings/tokens , create a token and copy it, since you will need in for the next step.","title":"Get a Huggingface-Token"},{"location":"installation/040_INSTALL_DOCKER/#setup","text":"Set the fork you want to use and other variables. Tip I preffer to save my env vars in the repository root in a .env (or .envrc ) file to automatically re-apply them when I come back. The build- and run- scripts contain default values for almost everything, besides the Hugging Face Token you created in the last step. Some Suggestions of variables you may want to change besides the Token: Environment-Variable Default value Description HUGGING_FACE_HUB_TOKEN No default, but required ! This is the only required variable, without it you can't download the huggingface models REPOSITORY_NAME The Basename of the Repo folder This name will used as the container repository/image name VOLUMENAME ${REPOSITORY_NAME,,}_data Name of the Docker Volume where model files will be stored ARCH arch of the build machine Can be changed if you want to build the image for another arch CONTAINER_REGISTRY ghcr.io Name of the Container Registry to use for the full tag CONTAINER_REPOSITORY $(whoami)/${REPOSITORY_NAME} Name of the Container Repository CONTAINER_FLAVOR cuda The flavor of the image to built, available options are cuda , rocm and cpu . If you choose rocm or cpu , the extra-index-url will be selected automatically, unless you set one yourself. CONTAINER_TAG ${INVOKEAI_BRANCH##*/}-${CONTAINER_FLAVOR} The Container Repository / Tag which will be used INVOKE_DOCKERFILE Dockerfile The Dockerfile which should be built, handy for development PIP_EXTRA_INDEX_URL If you want to use a custom pip-extra-index-url","title":"Setup"},{"location":"installation/040_INSTALL_DOCKER/#build-the-image","text":"I provided a build script, which is located next to the Dockerfile in docker/build.sh . It can be executed from repository root like this: ./docker/build.sh The build Script not only builds the container, but also creates the docker volume if not existing yet.","title":"Build the Image"},{"location":"installation/040_INSTALL_DOCKER/#run-the-container","text":"After the build process is done, you can run the container via the provided docker/run.sh script ./docker/run.sh When used without arguments, the container will start the webserver and provide you the link to open it. But if you want to use some other parameters you can also do so. run script example ./docker/run.sh \"banana sushi\" -Ak_lms -S42 -s10 This would generate the legendary \"banana sushi\" with Seed 42, k_lms Sampler and 10 steps. Find out more about available CLI-Parameters at features/CLI.md","title":"Run the Container"},{"location":"installation/040_INSTALL_DOCKER/#running-the-container-on-your-gpu","text":"If you have an Nvidia GPU, you can enable InvokeAI to run on the GPU by running the container with an extra environment variable to enable GPU usage and have the process run much faster: GPU_FLAGS = all ./docker/run.sh This passes the --gpus all to docker and uses the GPU. If you don't have a GPU (or your host is not yet setup to use it) you will see a message like this: docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]]. You can use the full set of GPU combinations documented here: https://docs.docker.com/config/containers/resource_constraints/#gpu For example, use GPU_FLAGS=device=GPU-3a23c669-1f69-c64e-cf85-44e9b07e7a2a to choose a specific device identified by a UUID. Deprecated From here on you will find the the previous Docker-Docs, which will still provide some usefull informations.","title":"Running the container on your GPU"},{"location":"installation/040_INSTALL_DOCKER/#usage-time-to-have-fun","text":"","title":"Usage (time to have fun)"},{"location":"installation/040_INSTALL_DOCKER/#startup","text":"If you're on a Linux container the invoke script is automatically started and the output dir set to the Docker volume you created earlier. If you're directly on macOS follow these startup instructions . With the Conda environment activated ( conda activate ldm ), run the interactive interface that combines the functionality of the original scripts txt2img and img2img : Use the more accurate but VRAM-intensive full precision math because half-precision requires autocast and won't work. By default the images are saved in outputs/img-samples/ . python3 scripts/invoke.py --full_precision You'll get the script's prompt. You can see available options or quit. invoke> -h invoke> q","title":"Startup"},{"location":"installation/040_INSTALL_DOCKER/#text-to-image","text":"For quick (but bad) image results test with 5 steps (default 50) and 1 sample image. This will let you know that everything is set up correctly. Then increase steps to 100 or more for good (but slower) results. The prompt can be in quotes or not. invoke> The hulk fighting with sheldon cooper -s5 -n1 invoke> \"woman closeup highly detailed\" -s 150 # Reuse previous seed and apply face restoration invoke> \"woman closeup highly detailed\" --steps 150 --seed -1 -G 0 .75 You'll need to experiment to see if face restoration is making it better or worse for your specific prompt. If you're on a container the output is set to the Docker volume. You can copy it wherever you want. You can download it from the Docker Desktop app, Volumes, my-vol, data. Or you can copy it from your Mac terminal. Keep in mind docker cp can't expand *.png so you'll need to specify the image file name. On your host Mac (you can use the name of any container that mounted the volume): docker cp dummy:/data/000001.928403745.png /Users/<your-user>/Pictures","title":"Text to Image"},{"location":"installation/040_INSTALL_DOCKER/#image-to-image","text":"You can also do text-guided image-to-image translation. For example, turning a sketch into a detailed drawing. strength is a value between 0.0 and 1.0 that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. 0.0 preserves image exactly, 1.0 replaces it completely. Make sure your input image size dimensions are multiples of 64 e.g. 512x512. Otherwise you'll get Error: product of dimension sizes > 2**31' . If you still get the error try a different size like 512x256. If you're on a Docker container, copy your input image into the Docker volume docker cp /Users/<your-user>/Pictures/sketch-mountains-input.jpg dummy:/data/ Try it out generating an image (or more). The invoke script needs absolute paths to find the image so don't use ~ . If you're on your Mac invoke> \"A fantasy landscape, trending on artstation\" -I /Users/<your-user>/Pictures/sketch-mountains-input.jpg --strength 0 .75 --steps 100 -n4 If you're on a Linux container on your Mac invoke> \"A fantasy landscape, trending on artstation\" -I /data/sketch-mountains-input.jpg --strength 0 .75 --steps 50 -n1","title":"Image to Image"},{"location":"installation/040_INSTALL_DOCKER/#web-interface","text":"You can use the invoke script with a graphical web interface. Start the web server with: python3 scripts/invoke.py --full_precision --web If it's running on your Mac point your Mac web browser to http://127.0.0.1:9090 Press Control-C at the command line to stop the web server.","title":"Web Interface"},{"location":"installation/040_INSTALL_DOCKER/#notes","text":"Some text you can add at the end of the prompt to make it very pretty: cinematic photo, highly detailed, cinematic lighting, ultra-detailed, ultrarealistic, photorealism, Octane Rendering, cyberpunk lights, Hyper Detail, 8K, HD, Unreal Engine, V-Ray, full hd, cyberpunk, abstract, 3d octane render + 4k UHD + immense detail + dramatic lighting + well lit + black, purple, blue, pink, cerulean, teal, metallic colours, + fine details, ultra photoreal, photographic, concept art, cinematic composition, rule of thirds, mysterious, eerie, photorealism, breathtaking detailed, painting art deco pattern, by hsiao, ron cheng, john james audubon, bizarre compositions, exquisite detail, extremely moody lighting, painted by greg rutkowski makoto shinkai takashi takeuchi studio ghibli, akihiko yoshida The original scripts should work as well. python3 scripts/orig_scripts/txt2img.py --help python3 scripts/orig_scripts/txt2img.py --ddim_steps 100 --n_iter 1 --n_samples 1 --plms --prompt \"new born baby kitten. Hyper Detail, Octane Rendering, Unreal Engine, V-Ray\" python3 scripts/orig_scripts/txt2img.py --ddim_steps 5 --n_iter 1 --n_samples 1 --plms --prompt \"ocean\" # or --klms","title":"Notes"},{"location":"installation/050_INSTALLING_MODELS/","text":"Installing Models # Checkpoint and Diffusers Models # The model checkpoint files ('*.ckpt') are the Stable Diffusion \"secret sauce\". They are the product of training the AI on millions of captioned images gathered from multiple sources. Originally there was only a single Stable Diffusion weights file, which many people named model.ckpt . Now there are dozens or more that have been fine tuned to provide particulary styles, genres, or other features. In addition, there are several new formats that improve on the original checkpoint format: a .safetensors format which prevents malware from masquerading as a model, and diffusers models, the most recent innovation. InvokeAI supports all three formats but strongly prefers the diffusers format. These are distributed as directories containing multiple subfolders, each of which contains a different aspect of the model. The advantage of this is that the models load from disk really fast. Another advantage is that diffusers models are supported by a large and active set of open source developers working at and with HuggingFace organization, and improvements in both rendering quality and performance are being made at a rapid pace. Among other features is the ability to download and install a diffusers model just by providing its HuggingFace repository ID. While InvokeAI will continue to support .ckpt and .safetensors models for the near future, these are deprecated and support will likely be withdrawn at some point in the not-too-distant future. This manual will guide you through installing and configuring model weight files and converting legacy .ckpt and .safetensors files into performant diffusers models. Base Models # InvokeAI comes with support for a good set of starter models. You'll find them listed in the master models file configs/INITIAL_MODELS.yaml in the InvokeAI root directory. The subset that are currently installed are found in configs/models.yaml . The current list is: Model HuggingFace Repo ID Description URL stable-diffusion-1.5 runwayml/stable-diffusion-v1-5 Most recent version of base Stable Diffusion model https://huggingface.co/runwayml/stable-diffusion-v1-5 stable-diffusion-1.4 runwayml/stable-diffusion-v1-4 Previous version of base Stable Diffusion model https://huggingface.co/runwayml/stable-diffusion-v1-4 inpainting-1.5 runwayml/stable-diffusion-inpainting Stable diffusion 1.5 optimized for inpainting https://huggingface.co/runwayml/stable-diffusion-inpainting stable-diffusion-2.1-base stabilityai/stable-diffusion-2-1-base Stable Diffusion version 2.1 trained on 512 pixel images https://huggingface.co/stabilityai/stable-diffusion-2-1-base stable-diffusion-2.1-768 stabilityai/stable-diffusion-2-1 Stable Diffusion version 2.1 trained on 768 pixel images https://huggingface.co/stabilityai/stable-diffusion-2-1 dreamlike-diffusion-1.0 dreamlike-art/dreamlike-diffusion-1.0 An SD 1.5 model finetuned on high quality art https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0 dreamlike-photoreal-2.0 dreamlike-art/dreamlike-photoreal-2.0 A photorealistic model trained on 768 pixel images https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0 openjourney-4.0 prompthero/openjourney An SD 1.5 model finetuned on Midjourney images prompt with \"mdjrny-v4 style\" https://huggingface.co/prompthero/openjourney nitro-diffusion-1.0 nitrosocke/Nitro-Diffusion An SD 1.5 model finetuned on three styles, prompt with \"archer style\", \"arcane style\" or \"modern disney style\" https://huggingface.co/nitrosocke/Nitro-Diffusion trinart-2.0 naclbit/trinart_stable_diffusion_v2 An SD 1.5 model finetuned with ~40,000 assorted high resolution manga/anime-style pictures https://huggingface.co/naclbit/trinart_stable_diffusion_v2 trinart-characters-2_0 naclbit/trinart_derrida_characters_v2_stable_diffusion An SD 1.5 model finetuned with 19.2M manga/anime-style pictures https://huggingface.co/naclbit/trinart_derrida_characters_v2_stable_diffusion Note that these files are covered by an \"Ethical AI\" license which forbids certain uses. When you initially download them, you are asked to accept the license terms. Community-Contributed Models # There are too many to list here and more are being contributed every day. HuggingFace is a great resource for diffusers models, and is also the home of a fast-growing repository of embedding (\".bin\") models that add subjects and/or styles to your images. The latter are automatically installed on the fly when you include the text <concept-name> in your prompt. See Concepts Library for more information. Another popular site for community-contributed models is CIVITAI . This extensive site currently supports only .safetensors and .ckpt models, but they can be easily loaded into InvokeAI and/or converted into optimized diffusers models. Be aware that CIVITAI hosts many models that generate NSFW content. Installation # There are multiple ways to install and manage models: The invokeai-configure script which will download and install them for you. The command-line tool (CLI) has commands that allows you to import, configure and modify models files. The web interface (WebUI) has a GUI for importing and managing models. Installation via invokeai-configure # From the invoke launcher, choose option (6) \"re-run the configure script to download new models.\" This will launch the same script that prompted you to select models at install time. You can use this to add models that you skipped the first time around. It is all right to specify a model that was previously downloaded; the script will just confirm that the files are complete. Installation via the CLI # You can install a new model, including any of the community-supported ones, via the command-line client's !import_model command. Installing .ckpt and .safetensors models # If the model is already downloaded to your local disk, use !import_model /path/to/file.ckpt to load it. For example: invoke> !import_model C:/Users/fred/Downloads/martians.safetensors Forward Slashes On Windows systems, use forward slashes rather than backslashes in your file paths. If you do use backslashes, you must double them like this: C:\\\\Users\\\\fred\\\\Downloads\\\\martians.safetensors Alternatively you can directly import the file using its URL: invoke> !import_model https://example.org/sd_models/martians.safetensors For this to work, the URL must not be password-protected. Otherwise you will receive a 404 error. When you import a legacy model, the CLI will ask you a few questions about the model, including what size image it was trained on (usually 512x512), what name and description you wish to use for it, what configuration file to use for it (usually the default v1-inference.yaml ), whether you'd like to make this model the default at startup time, and whether you would like to install a custom VAE (variable autoencoder) file for the model. For recent models, the answer to the VAE question is usually \"no,\" but it won't hurt to answer \"yes\". Installing diffusers models # You can install a diffusers model from the HuggingFace site using !import_model and the HuggingFace repo_id for the model: invoke> !import_model andite/anything-v4.0 Alternatively, you can download the model to disk and import it from there. The model may be distributed as a ZIP file, or as a Git repository: invoke> !import_model C:/Users/fred/Downloads/andite--anything-v4.0 The CLI supports file path autocompletion Type a bit of the path name and hit Tab in order to get a choice of possible completions. On Windows, you can drag model files onto the command-line Once you have typed in !import_model , you can drag the model file or directory onto the command-line to insert the model path. This way, you don't need to type it or copy/paste. However, you will need to reverse or double backslashes as noted above. Before installing, the CLI will ask you for a short name and description for the model, whether to make this the default model that is loaded at InvokeAI startup time, and whether to replace its VAE. Generally the answer to the latter question is \"no\". Converting legacy models into diffusers # The CLI !convert_model will convert a .safetensors or .ckpt models file into diffusers and install it.This will enable the model to load and run faster without loss of image quality. The usage is identical to !import_model . You may point the command to either a downloaded model file on disk, or to a (non-password protected) URL: invoke> !convert_model C:/Users/fred/Downloads/martians.safetensors After a successful conversion, the CLI will offer you the option of deleting the original .ckpt or .safetensors file. Optimizing a previously-installed model # Lastly, if you have previously installed a .ckpt or .safetensors file and wish to convert it into a diffusers model, you can do this without re-downloading and converting the original file using the !optimize_model command. Simply pass the short name of an existing installed model: invoke> !optimize_model martians-v1.0 The model will be converted into diffusers format and replace the previously installed version. You will again be offered the opportunity to delete the original .ckpt or .safetensors file. Related CLI Commands # There are a whole series of additional model management commands in the CLI that you can read about in Command-Line Interface . These include: !models - List all installed models !switch <model name> - Switch to the indicated model !edit_model <model name> - Edit the indicated model to change its name, description or other properties !del_model <model name> - Delete the indicated model Manually editing configs/models.yaml # If you are comfortable with a text editor then you may simply edit models.yaml directly. You will need to download the desired .ckpt/.safetensors file and place it somewhere on your machine's filesystem. Alternatively, for a diffusers model, record the repo_id or download the whole model directory. Then using a text editor (e.g. the Windows Notepad application), open the file configs/models.yaml , and add a new stanza that follows this model: A legacy model # A legacy .ckpt or .safetensors entry will look like this: arabian-nights-1.0 : description : A great fine-tune in Arabian Nights style weights : ./path/to/arabian-nights-1.0.ckpt config : ./configs/stable-diffusion/v1-inference.yaml format : ckpt width : 512 height : 512 default : false Note that format is ckpt for both .ckpt and .safetensors files. A diffusers model # A stanza for a diffusers model will look like this for a HuggingFace model with a repository ID: arabian-nights-1.1 : description : An even better fine-tune of the Arabian Nights repo_id : captahab/arabian-nights-1.1 format : diffusers default : true And for a downloaded directory: arabian-nights-1.1 : description : An even better fine-tune of the Arabian Nights path : /path/to/captahab-arabian-nights-1.1 format : diffusers default : true There is additional syntax for indicating an external VAE to use with this model. See INITIAL_MODELS.yaml and models.yaml for examples. After you save the modified models.yaml file relaunch invokeai . The new model will now be available for your use. Installation via the WebUI # To access the WebUI Model Manager, click on the button that looks like a cute in the upper right side of the browser screen. This will bring up a dialogue that lists the models you have already installed, and allows you to load, delete or edit them: To add a new model, click on + Add New and select to either a checkpoint/safetensors model, or a diffusers model: In this example, we chose Add Diffusers . As shown in the figure below, a new dialogue prompts you to enter the name to use for the model, its description, and either the location of the diffusers model on disk, or its Repo ID on the HuggingFace web site. If you choose to enter a path to disk, the system will autocomplete for you as you type: Press Add Model at the bottom of the dialogue (scrolled out of site in the figure), and the model will be downloaded, imported, and registered in models.yaml . The Add Checkpoint/Safetensor Model option is similar, except that in this case you can choose to scan an entire folder for checkpoint/safetensors files to import. Simply type in the path of the directory and press the \"Search\" icon. This will display the .ckpt and .safetensors found inside the directory and its subfolders, and allow you to choose which ones to import: Model Management Startup Options # The invoke launcher and the invokeai script accept a series of command-line arguments that modify InvokeAI's behavior when loading models. These can be provided on the command line, or added to the InvokeAI root directory's invokeai.init initialization file. The arguments are: --model <model name> -- Start up with the indicated model loaded --ckpt_convert -- When a checkpoint/safetensors model is loaded, convert it into a diffusers model in memory. This does not permanently save the converted model to disk. --autoconvert <path/to/directory> -- Scan the indicated directory path for new checkpoint/safetensors files, convert them into diffusers models, and import them into InvokeAI. Here is an example of providing an argument on the command line using the invoke.sh launch script: invoke.sh --autoconvert /home/fred/stable-diffusion-checkpoints And here is what the same argument looks like in invokeai.init : --outdir=\"/home/fred/invokeai/outputs --no-nsfw_checker --autoconvert /home/fred/stable-diffusion-checkpoints","title":"Installing Models"},{"location":"installation/050_INSTALLING_MODELS/#installing-models","text":"","title":" Installing Models"},{"location":"installation/050_INSTALLING_MODELS/#checkpoint-and-diffusers-models","text":"The model checkpoint files ('*.ckpt') are the Stable Diffusion \"secret sauce\". They are the product of training the AI on millions of captioned images gathered from multiple sources. Originally there was only a single Stable Diffusion weights file, which many people named model.ckpt . Now there are dozens or more that have been fine tuned to provide particulary styles, genres, or other features. In addition, there are several new formats that improve on the original checkpoint format: a .safetensors format which prevents malware from masquerading as a model, and diffusers models, the most recent innovation. InvokeAI supports all three formats but strongly prefers the diffusers format. These are distributed as directories containing multiple subfolders, each of which contains a different aspect of the model. The advantage of this is that the models load from disk really fast. Another advantage is that diffusers models are supported by a large and active set of open source developers working at and with HuggingFace organization, and improvements in both rendering quality and performance are being made at a rapid pace. Among other features is the ability to download and install a diffusers model just by providing its HuggingFace repository ID. While InvokeAI will continue to support .ckpt and .safetensors models for the near future, these are deprecated and support will likely be withdrawn at some point in the not-too-distant future. This manual will guide you through installing and configuring model weight files and converting legacy .ckpt and .safetensors files into performant diffusers models.","title":"Checkpoint and Diffusers Models"},{"location":"installation/050_INSTALLING_MODELS/#base-models","text":"InvokeAI comes with support for a good set of starter models. You'll find them listed in the master models file configs/INITIAL_MODELS.yaml in the InvokeAI root directory. The subset that are currently installed are found in configs/models.yaml . The current list is: Model HuggingFace Repo ID Description URL stable-diffusion-1.5 runwayml/stable-diffusion-v1-5 Most recent version of base Stable Diffusion model https://huggingface.co/runwayml/stable-diffusion-v1-5 stable-diffusion-1.4 runwayml/stable-diffusion-v1-4 Previous version of base Stable Diffusion model https://huggingface.co/runwayml/stable-diffusion-v1-4 inpainting-1.5 runwayml/stable-diffusion-inpainting Stable diffusion 1.5 optimized for inpainting https://huggingface.co/runwayml/stable-diffusion-inpainting stable-diffusion-2.1-base stabilityai/stable-diffusion-2-1-base Stable Diffusion version 2.1 trained on 512 pixel images https://huggingface.co/stabilityai/stable-diffusion-2-1-base stable-diffusion-2.1-768 stabilityai/stable-diffusion-2-1 Stable Diffusion version 2.1 trained on 768 pixel images https://huggingface.co/stabilityai/stable-diffusion-2-1 dreamlike-diffusion-1.0 dreamlike-art/dreamlike-diffusion-1.0 An SD 1.5 model finetuned on high quality art https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0 dreamlike-photoreal-2.0 dreamlike-art/dreamlike-photoreal-2.0 A photorealistic model trained on 768 pixel images https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0 openjourney-4.0 prompthero/openjourney An SD 1.5 model finetuned on Midjourney images prompt with \"mdjrny-v4 style\" https://huggingface.co/prompthero/openjourney nitro-diffusion-1.0 nitrosocke/Nitro-Diffusion An SD 1.5 model finetuned on three styles, prompt with \"archer style\", \"arcane style\" or \"modern disney style\" https://huggingface.co/nitrosocke/Nitro-Diffusion trinart-2.0 naclbit/trinart_stable_diffusion_v2 An SD 1.5 model finetuned with ~40,000 assorted high resolution manga/anime-style pictures https://huggingface.co/naclbit/trinart_stable_diffusion_v2 trinart-characters-2_0 naclbit/trinart_derrida_characters_v2_stable_diffusion An SD 1.5 model finetuned with 19.2M manga/anime-style pictures https://huggingface.co/naclbit/trinart_derrida_characters_v2_stable_diffusion Note that these files are covered by an \"Ethical AI\" license which forbids certain uses. When you initially download them, you are asked to accept the license terms.","title":"Base Models"},{"location":"installation/050_INSTALLING_MODELS/#community-contributed-models","text":"There are too many to list here and more are being contributed every day. HuggingFace is a great resource for diffusers models, and is also the home of a fast-growing repository of embedding (\".bin\") models that add subjects and/or styles to your images. The latter are automatically installed on the fly when you include the text <concept-name> in your prompt. See Concepts Library for more information. Another popular site for community-contributed models is CIVITAI . This extensive site currently supports only .safetensors and .ckpt models, but they can be easily loaded into InvokeAI and/or converted into optimized diffusers models. Be aware that CIVITAI hosts many models that generate NSFW content.","title":"Community-Contributed Models"},{"location":"installation/050_INSTALLING_MODELS/#installation","text":"There are multiple ways to install and manage models: The invokeai-configure script which will download and install them for you. The command-line tool (CLI) has commands that allows you to import, configure and modify models files. The web interface (WebUI) has a GUI for importing and managing models.","title":"Installation"},{"location":"installation/050_INSTALLING_MODELS/#installation-via-invokeai-configure","text":"From the invoke launcher, choose option (6) \"re-run the configure script to download new models.\" This will launch the same script that prompted you to select models at install time. You can use this to add models that you skipped the first time around. It is all right to specify a model that was previously downloaded; the script will just confirm that the files are complete.","title":"Installation via invokeai-configure"},{"location":"installation/050_INSTALLING_MODELS/#installation-via-the-cli","text":"You can install a new model, including any of the community-supported ones, via the command-line client's !import_model command.","title":"Installation via the CLI"},{"location":"installation/050_INSTALLING_MODELS/#installing-ckpt-and-safetensors-models","text":"If the model is already downloaded to your local disk, use !import_model /path/to/file.ckpt to load it. For example: invoke> !import_model C:/Users/fred/Downloads/martians.safetensors Forward Slashes On Windows systems, use forward slashes rather than backslashes in your file paths. If you do use backslashes, you must double them like this: C:\\\\Users\\\\fred\\\\Downloads\\\\martians.safetensors Alternatively you can directly import the file using its URL: invoke> !import_model https://example.org/sd_models/martians.safetensors For this to work, the URL must not be password-protected. Otherwise you will receive a 404 error. When you import a legacy model, the CLI will ask you a few questions about the model, including what size image it was trained on (usually 512x512), what name and description you wish to use for it, what configuration file to use for it (usually the default v1-inference.yaml ), whether you'd like to make this model the default at startup time, and whether you would like to install a custom VAE (variable autoencoder) file for the model. For recent models, the answer to the VAE question is usually \"no,\" but it won't hurt to answer \"yes\".","title":"Installing .ckpt and .safetensors models"},{"location":"installation/050_INSTALLING_MODELS/#installing-diffusers-models","text":"You can install a diffusers model from the HuggingFace site using !import_model and the HuggingFace repo_id for the model: invoke> !import_model andite/anything-v4.0 Alternatively, you can download the model to disk and import it from there. The model may be distributed as a ZIP file, or as a Git repository: invoke> !import_model C:/Users/fred/Downloads/andite--anything-v4.0 The CLI supports file path autocompletion Type a bit of the path name and hit Tab in order to get a choice of possible completions. On Windows, you can drag model files onto the command-line Once you have typed in !import_model , you can drag the model file or directory onto the command-line to insert the model path. This way, you don't need to type it or copy/paste. However, you will need to reverse or double backslashes as noted above. Before installing, the CLI will ask you for a short name and description for the model, whether to make this the default model that is loaded at InvokeAI startup time, and whether to replace its VAE. Generally the answer to the latter question is \"no\".","title":"Installing diffusers models"},{"location":"installation/050_INSTALLING_MODELS/#converting-legacy-models-into-diffusers","text":"The CLI !convert_model will convert a .safetensors or .ckpt models file into diffusers and install it.This will enable the model to load and run faster without loss of image quality. The usage is identical to !import_model . You may point the command to either a downloaded model file on disk, or to a (non-password protected) URL: invoke> !convert_model C:/Users/fred/Downloads/martians.safetensors After a successful conversion, the CLI will offer you the option of deleting the original .ckpt or .safetensors file.","title":"Converting legacy models into diffusers"},{"location":"installation/050_INSTALLING_MODELS/#optimizing-a-previously-installed-model","text":"Lastly, if you have previously installed a .ckpt or .safetensors file and wish to convert it into a diffusers model, you can do this without re-downloading and converting the original file using the !optimize_model command. Simply pass the short name of an existing installed model: invoke> !optimize_model martians-v1.0 The model will be converted into diffusers format and replace the previously installed version. You will again be offered the opportunity to delete the original .ckpt or .safetensors file.","title":"Optimizing a previously-installed model"},{"location":"installation/050_INSTALLING_MODELS/#related-cli-commands","text":"There are a whole series of additional model management commands in the CLI that you can read about in Command-Line Interface . These include: !models - List all installed models !switch <model name> - Switch to the indicated model !edit_model <model name> - Edit the indicated model to change its name, description or other properties !del_model <model name> - Delete the indicated model","title":"Related CLI Commands"},{"location":"installation/050_INSTALLING_MODELS/#manually-editing-configsmodelsyaml","text":"If you are comfortable with a text editor then you may simply edit models.yaml directly. You will need to download the desired .ckpt/.safetensors file and place it somewhere on your machine's filesystem. Alternatively, for a diffusers model, record the repo_id or download the whole model directory. Then using a text editor (e.g. the Windows Notepad application), open the file configs/models.yaml , and add a new stanza that follows this model:","title":"Manually editing configs/models.yaml"},{"location":"installation/050_INSTALLING_MODELS/#a-legacy-model","text":"A legacy .ckpt or .safetensors entry will look like this: arabian-nights-1.0 : description : A great fine-tune in Arabian Nights style weights : ./path/to/arabian-nights-1.0.ckpt config : ./configs/stable-diffusion/v1-inference.yaml format : ckpt width : 512 height : 512 default : false Note that format is ckpt for both .ckpt and .safetensors files.","title":"A legacy model"},{"location":"installation/050_INSTALLING_MODELS/#a-diffusers-model","text":"A stanza for a diffusers model will look like this for a HuggingFace model with a repository ID: arabian-nights-1.1 : description : An even better fine-tune of the Arabian Nights repo_id : captahab/arabian-nights-1.1 format : diffusers default : true And for a downloaded directory: arabian-nights-1.1 : description : An even better fine-tune of the Arabian Nights path : /path/to/captahab-arabian-nights-1.1 format : diffusers default : true There is additional syntax for indicating an external VAE to use with this model. See INITIAL_MODELS.yaml and models.yaml for examples. After you save the modified models.yaml file relaunch invokeai . The new model will now be available for your use.","title":"A diffusers model"},{"location":"installation/050_INSTALLING_MODELS/#installation-via-the-webui","text":"To access the WebUI Model Manager, click on the button that looks like a cute in the upper right side of the browser screen. This will bring up a dialogue that lists the models you have already installed, and allows you to load, delete or edit them: To add a new model, click on + Add New and select to either a checkpoint/safetensors model, or a diffusers model: In this example, we chose Add Diffusers . As shown in the figure below, a new dialogue prompts you to enter the name to use for the model, its description, and either the location of the diffusers model on disk, or its Repo ID on the HuggingFace web site. If you choose to enter a path to disk, the system will autocomplete for you as you type: Press Add Model at the bottom of the dialogue (scrolled out of site in the figure), and the model will be downloaded, imported, and registered in models.yaml . The Add Checkpoint/Safetensor Model option is similar, except that in this case you can choose to scan an entire folder for checkpoint/safetensors files to import. Simply type in the path of the directory and press the \"Search\" icon. This will display the .ckpt and .safetensors found inside the directory and its subfolders, and allow you to choose which ones to import:","title":"Installation via the WebUI"},{"location":"installation/050_INSTALLING_MODELS/#model-management-startup-options","text":"The invoke launcher and the invokeai script accept a series of command-line arguments that modify InvokeAI's behavior when loading models. These can be provided on the command line, or added to the InvokeAI root directory's invokeai.init initialization file. The arguments are: --model <model name> -- Start up with the indicated model loaded --ckpt_convert -- When a checkpoint/safetensors model is loaded, convert it into a diffusers model in memory. This does not permanently save the converted model to disk. --autoconvert <path/to/directory> -- Scan the indicated directory path for new checkpoint/safetensors files, convert them into diffusers models, and import them into InvokeAI. Here is an example of providing an argument on the command line using the invoke.sh launch script: invoke.sh --autoconvert /home/fred/stable-diffusion-checkpoints And here is what the same argument looks like in invokeai.init : --outdir=\"/home/fred/invokeai/outputs --no-nsfw_checker --autoconvert /home/fred/stable-diffusion-checkpoints","title":"Model Management Startup Options"},{"location":"installation/060_INSTALL_PATCHMATCH/","text":"Installing PyPatchMatch # pypatchmatch is a Python module for inpainting images. It is not needed to run InvokeAI, but it greatly improves the quality of inpainting and outpainting and is recommended. Unfortunately, it is a C++ optimized module and installation can be somewhat challenging. This guide leads you through the steps. Windows # You're in luck! On Windows platforms PyPatchMatch will install automatically on Windows systems with no extra intervention. Macintosh # You need to have opencv installed so that pypatchmatch can be built: brew install opencv The next time you start invoke , after sucesfully installing opencv, pypatchmatch will be built. Linux # Prior to installing PyPatchMatch, you need to take the following steps: Debian Based Distros # Install the build-essential tools: sudo apt update sudo apt install build-essential Install opencv : sudo apt install python3-opencv libopencv-dev Activate the environment you use for invokeai, either with conda or with a virtual environment. Install pypatchmatch: pip install pypatchmatch Confirm that pypatchmatch is installed. At the command-line prompt enter python , and then at the >>> line type from patchmatch import patch_match : It should look like the follwing: Python 3.9.5 ( default , Nov 23 2021 , 15 : 27 : 38 ) [ GCC 9.3.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> from patchmatch import patch_match Compiling and loading c extensions from \"/home/lstein/Projects/InvokeAI/.invokeai-env/src/pypatchmatch/patchmatch\" . rm - rf build / obj libpatchmatch . so mkdir : created directory 'build/obj' mkdir : created directory 'build/obj/csrc/' [ dep ] csrc / masked_image . cpp ... [ dep ] csrc / nnf . cpp ... [ dep ] csrc / inpaint . cpp ... [ dep ] csrc / pyinterface . cpp ... [ CC ] csrc / pyinterface . cpp ... [ CC ] csrc / inpaint . cpp ... [ CC ] csrc / nnf . cpp ... [ CC ] csrc / masked_image . cpp ... [ link ] libpatchmatch . so ... Arch Based Distros # Install the base-devel package: sudo pacman -Syu sudo pacman -S --needed base-devel Install opencv : sudo pacman -S opencv or for CUDA support sudo pacman -S opencv-cuda Fix the naming of the opencv package configuration file: cd /usr/lib/pkgconfig/ ln -sf opencv4.pc opencv.pc Next, Follow Steps 4-6 from the Debian Section above If you see no errors, then you're ready to go!","title":"Installing PyPatchMatch"},{"location":"installation/060_INSTALL_PATCHMATCH/#installing-pypatchmatch","text":"pypatchmatch is a Python module for inpainting images. It is not needed to run InvokeAI, but it greatly improves the quality of inpainting and outpainting and is recommended. Unfortunately, it is a C++ optimized module and installation can be somewhat challenging. This guide leads you through the steps.","title":" Installing PyPatchMatch"},{"location":"installation/060_INSTALL_PATCHMATCH/#windows","text":"You're in luck! On Windows platforms PyPatchMatch will install automatically on Windows systems with no extra intervention.","title":"Windows"},{"location":"installation/060_INSTALL_PATCHMATCH/#macintosh","text":"You need to have opencv installed so that pypatchmatch can be built: brew install opencv The next time you start invoke , after sucesfully installing opencv, pypatchmatch will be built.","title":"Macintosh"},{"location":"installation/060_INSTALL_PATCHMATCH/#linux","text":"Prior to installing PyPatchMatch, you need to take the following steps:","title":"Linux"},{"location":"installation/060_INSTALL_PATCHMATCH/#debian-based-distros","text":"Install the build-essential tools: sudo apt update sudo apt install build-essential Install opencv : sudo apt install python3-opencv libopencv-dev Activate the environment you use for invokeai, either with conda or with a virtual environment. Install pypatchmatch: pip install pypatchmatch Confirm that pypatchmatch is installed. At the command-line prompt enter python , and then at the >>> line type from patchmatch import patch_match : It should look like the follwing: Python 3.9.5 ( default , Nov 23 2021 , 15 : 27 : 38 ) [ GCC 9.3.0 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> from patchmatch import patch_match Compiling and loading c extensions from \"/home/lstein/Projects/InvokeAI/.invokeai-env/src/pypatchmatch/patchmatch\" . rm - rf build / obj libpatchmatch . so mkdir : created directory 'build/obj' mkdir : created directory 'build/obj/csrc/' [ dep ] csrc / masked_image . cpp ... [ dep ] csrc / nnf . cpp ... [ dep ] csrc / inpaint . cpp ... [ dep ] csrc / pyinterface . cpp ... [ CC ] csrc / pyinterface . cpp ... [ CC ] csrc / inpaint . cpp ... [ CC ] csrc / nnf . cpp ... [ CC ] csrc / masked_image . cpp ... [ link ] libpatchmatch . so ...","title":"Debian Based Distros"},{"location":"installation/060_INSTALL_PATCHMATCH/#arch-based-distros","text":"Install the base-devel package: sudo pacman -Syu sudo pacman -S --needed base-devel Install opencv : sudo pacman -S opencv or for CUDA support sudo pacman -S opencv-cuda Fix the naming of the opencv package configuration file: cd /usr/lib/pkgconfig/ ln -sf opencv4.pc opencv.pc Next, Follow Steps 4-6 from the Debian Section above If you see no errors, then you're ready to go!","title":"Arch Based Distros"},{"location":"installation/070_INSTALL_XFORMERS/","text":"Installing xformers # xFormers is toolbox that integrates with the pyTorch and CUDA libraries to provide accelerated performance and reduced memory consumption for applications using the transformers machine learning architecture. After installing xFormers, InvokeAI users who have CUDA GPUs will see a noticeable decrease in GPU memory consumption and an increase in speed. xFormers can be installed into a working InvokeAI installation without any code changes or other updates. This document explains how to install xFormers. Pip Install # For both Windows and Linux, you can install xformers in just a couple of steps from the command line. If you are used to launching invoke.sh or invoke.bat to start InvokeAI, then run the launcher and select the \"developer's console\" to get to the command line. If you run invoke.py directly from the command line, then just be sure to activate it's virtual environment. Then run the following three commands: pip install xformers == 0 .0.16rc425 pip install triton python -m xformers.info output The first command installs xformers , the second installs the triton training accelerator, and the third prints out the xformers installation status. If all goes well, you'll see a report like the following: xFormers 0 .0.16rc425 memory_efficient_attention.cutlassF: available memory_efficient_attention.cutlassB: available memory_efficient_attention.flshattF: available memory_efficient_attention.flshattB: available memory_efficient_attention.smallkF: available memory_efficient_attention.smallkB: available memory_efficient_attention.tritonflashattF: available memory_efficient_attention.tritonflashattB: available swiglu.fused.p.cpp: available is_triton_available: True is_functorch_available: False pytorch.version: 1 .13.1+cu117 pytorch.cuda: available gpu.compute_capability: 8 .6 gpu.name: NVIDIA RTX A2000 12GB build.info: available build.cuda_version: 1107 build.python_version: 3 .10.9 build.torch_version: 1 .13.1+cu117 build.env.TORCH_CUDA_ARCH_LIST: 5 .0+PTX 6 .0 6 .1 7 .0 7 .5 8 .0 8 .6 build.env.XFORMERS_BUILD_TYPE: Release build.env.XFORMERS_ENABLE_DEBUG_ASSERTIONS: None build.env.NVCC_FLAGS: None build.env.XFORMERS_PACKAGE_FROM: wheel-v0.0.16rc425 source.privacy: open source Source Builds # xformers is currently under active development and at some point you may wish to build it from sourcce to get the latest features and bugfixes. Source Build on Linux # Note that xFormers only works with true NVIDIA GPUs and will not work properly with the ROCm driver for AMD acceleration. xFormers is not currently available as a pip binary wheel and must be installed from source. These instructions were written for a system running Ubuntu 22.04, but other Linux distributions should be able to adapt this recipe. 1. Install CUDA Toolkit 11.7 # You will need the CUDA developer's toolkit in order to compile and install xFormers. Do not try to install Ubuntu's nvidia-cuda-toolkit package. It is out of date and will cause conflicts among the NVIDIA driver and binaries. Instead install the CUDA Toolkit package provided by NVIDIA itself. Go to CUDA Toolkit 11.7 Downloads and use the target selection wizard to choose your platform and Linux distribution. Select an installer type of \"runfile (local)\" at the last step. This will provide you with a recipe for downloading and running a install shell script that will install the toolkit and drivers. For example, the install script recipe for Ubuntu 22.04 running on a x86_64 system is: wget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda_11.7.0_515.43.04_linux.run sudo sh cuda_11.7.0_515.43.04_linux.run Rather than cut-and-paste this example, We recommend that you walk through the toolkit wizard in order to get the most up to date installer for your system. 2. Confirm/Install pyTorch 1.13 with CUDA 11.7 support # If you are using InvokeAI 2.3 or higher, these will already be installed. If not, you can check whether you have the needed libraries using a quick command. Activate the invokeai virtual environment, either by entering the \"developer's console\", or manually with a command similar to source ~/invokeai/.venv/bin/activate (depending on where your invokeai directory is. Then run the command: python -c 'exec(\"import torch\\nprint(torch.__version__)\")' If it prints 1.13.1+cu117 you're good. If not, you can install the most up to date libraries with this command: pip install --upgrade --force-reinstall torch torchvision 3. Install the triton module # This module isn't necessary for xFormers image inference optimization, but avoids a startup warning. pip install triton 4. Install source code build prerequisites # To build xFormers from source, you will need the build-essentials package. If you don't have it installed already, run: sudo apt install build-essential 5. Build xFormers # There is no pip wheel package for xFormers at this time (January 2023). Although there is a conda package, InvokeAI no longer officially supports conda installations and you're on your own if you wish to try this route. Following the recipe provided at the xFormers GitHub page , and with the InvokeAI virtual environment active (see step 1) run the following commands: pip install ninja export TORCH_CUDA_ARCH_LIST = \"6.0;6.1;6.2;7.0;7.2;7.5;8.0;8.6\" pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg = xformers The TORCH_CUDA_ARCH_LIST is a list of GPU architectures to compile xFormer support for. You can speed up compilation by selecting the architecture specific for your system. You'll find the list of GPUs and their architectures at NVIDIA's GPU Compute Capability table. If the compile and install completes successfully, you can check that xFormers is installed with this command: python -m xformers.info If suiccessful, the top of the listing should indicate \"available\" for each of the memory_efficient_attention modules, as shown here: memory_efficient_attention.cutlassF: available memory_efficient_attention.cutlassB: available memory_efficient_attention.flshattF: available memory_efficient_attention.flshattB: available memory_efficient_attention.smallkF: available memory_efficient_attention.smallkB: available memory_efficient_attention.tritonflashattF: available memory_efficient_attention.tritonflashattB: available [ ... ] You can now launch InvokeAI and enjoy the benefits of xFormers. Windows # To come \u00a9 Copyright 2023 Lincoln Stein and the InvokeAI Development Team","title":"Installing xFormers"},{"location":"installation/070_INSTALL_XFORMERS/#installing-xformers","text":"xFormers is toolbox that integrates with the pyTorch and CUDA libraries to provide accelerated performance and reduced memory consumption for applications using the transformers machine learning architecture. After installing xFormers, InvokeAI users who have CUDA GPUs will see a noticeable decrease in GPU memory consumption and an increase in speed. xFormers can be installed into a working InvokeAI installation without any code changes or other updates. This document explains how to install xFormers.","title":" Installing xformers"},{"location":"installation/070_INSTALL_XFORMERS/#pip-install","text":"For both Windows and Linux, you can install xformers in just a couple of steps from the command line. If you are used to launching invoke.sh or invoke.bat to start InvokeAI, then run the launcher and select the \"developer's console\" to get to the command line. If you run invoke.py directly from the command line, then just be sure to activate it's virtual environment. Then run the following three commands: pip install xformers == 0 .0.16rc425 pip install triton python -m xformers.info output The first command installs xformers , the second installs the triton training accelerator, and the third prints out the xformers installation status. If all goes well, you'll see a report like the following: xFormers 0 .0.16rc425 memory_efficient_attention.cutlassF: available memory_efficient_attention.cutlassB: available memory_efficient_attention.flshattF: available memory_efficient_attention.flshattB: available memory_efficient_attention.smallkF: available memory_efficient_attention.smallkB: available memory_efficient_attention.tritonflashattF: available memory_efficient_attention.tritonflashattB: available swiglu.fused.p.cpp: available is_triton_available: True is_functorch_available: False pytorch.version: 1 .13.1+cu117 pytorch.cuda: available gpu.compute_capability: 8 .6 gpu.name: NVIDIA RTX A2000 12GB build.info: available build.cuda_version: 1107 build.python_version: 3 .10.9 build.torch_version: 1 .13.1+cu117 build.env.TORCH_CUDA_ARCH_LIST: 5 .0+PTX 6 .0 6 .1 7 .0 7 .5 8 .0 8 .6 build.env.XFORMERS_BUILD_TYPE: Release build.env.XFORMERS_ENABLE_DEBUG_ASSERTIONS: None build.env.NVCC_FLAGS: None build.env.XFORMERS_PACKAGE_FROM: wheel-v0.0.16rc425 source.privacy: open source","title":"Pip Install"},{"location":"installation/070_INSTALL_XFORMERS/#source-builds","text":"xformers is currently under active development and at some point you may wish to build it from sourcce to get the latest features and bugfixes.","title":"Source Builds"},{"location":"installation/070_INSTALL_XFORMERS/#source-build-on-linux","text":"Note that xFormers only works with true NVIDIA GPUs and will not work properly with the ROCm driver for AMD acceleration. xFormers is not currently available as a pip binary wheel and must be installed from source. These instructions were written for a system running Ubuntu 22.04, but other Linux distributions should be able to adapt this recipe.","title":"Source Build on Linux"},{"location":"installation/070_INSTALL_XFORMERS/#1-install-cuda-toolkit-117","text":"You will need the CUDA developer's toolkit in order to compile and install xFormers. Do not try to install Ubuntu's nvidia-cuda-toolkit package. It is out of date and will cause conflicts among the NVIDIA driver and binaries. Instead install the CUDA Toolkit package provided by NVIDIA itself. Go to CUDA Toolkit 11.7 Downloads and use the target selection wizard to choose your platform and Linux distribution. Select an installer type of \"runfile (local)\" at the last step. This will provide you with a recipe for downloading and running a install shell script that will install the toolkit and drivers. For example, the install script recipe for Ubuntu 22.04 running on a x86_64 system is: wget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda_11.7.0_515.43.04_linux.run sudo sh cuda_11.7.0_515.43.04_linux.run Rather than cut-and-paste this example, We recommend that you walk through the toolkit wizard in order to get the most up to date installer for your system.","title":"1. Install CUDA Toolkit 11.7"},{"location":"installation/070_INSTALL_XFORMERS/#2-confirminstall-pytorch-113-with-cuda-117-support","text":"If you are using InvokeAI 2.3 or higher, these will already be installed. If not, you can check whether you have the needed libraries using a quick command. Activate the invokeai virtual environment, either by entering the \"developer's console\", or manually with a command similar to source ~/invokeai/.venv/bin/activate (depending on where your invokeai directory is. Then run the command: python -c 'exec(\"import torch\\nprint(torch.__version__)\")' If it prints 1.13.1+cu117 you're good. If not, you can install the most up to date libraries with this command: pip install --upgrade --force-reinstall torch torchvision","title":"2. Confirm/Install pyTorch 1.13 with CUDA 11.7 support"},{"location":"installation/070_INSTALL_XFORMERS/#3-install-the-triton-module","text":"This module isn't necessary for xFormers image inference optimization, but avoids a startup warning. pip install triton","title":"3. Install the triton module"},{"location":"installation/070_INSTALL_XFORMERS/#4-install-source-code-build-prerequisites","text":"To build xFormers from source, you will need the build-essentials package. If you don't have it installed already, run: sudo apt install build-essential","title":"4. Install source code build prerequisites"},{"location":"installation/070_INSTALL_XFORMERS/#5-build-xformers","text":"There is no pip wheel package for xFormers at this time (January 2023). Although there is a conda package, InvokeAI no longer officially supports conda installations and you're on your own if you wish to try this route. Following the recipe provided at the xFormers GitHub page , and with the InvokeAI virtual environment active (see step 1) run the following commands: pip install ninja export TORCH_CUDA_ARCH_LIST = \"6.0;6.1;6.2;7.0;7.2;7.5;8.0;8.6\" pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg = xformers The TORCH_CUDA_ARCH_LIST is a list of GPU architectures to compile xFormer support for. You can speed up compilation by selecting the architecture specific for your system. You'll find the list of GPUs and their architectures at NVIDIA's GPU Compute Capability table. If the compile and install completes successfully, you can check that xFormers is installed with this command: python -m xformers.info If suiccessful, the top of the listing should indicate \"available\" for each of the memory_efficient_attention modules, as shown here: memory_efficient_attention.cutlassF: available memory_efficient_attention.cutlassB: available memory_efficient_attention.flshattF: available memory_efficient_attention.flshattB: available memory_efficient_attention.smallkF: available memory_efficient_attention.smallkB: available memory_efficient_attention.tritonflashattF: available memory_efficient_attention.tritonflashattB: available [ ... ] You can now launch InvokeAI and enjoy the benefits of xFormers.","title":"5. Build xFormers"},{"location":"installation/070_INSTALL_XFORMERS/#windows","text":"To come \u00a9 Copyright 2023 Lincoln Stein and the InvokeAI Development Team","title":"Windows"},{"location":"installation/Developers_documentation/BUILDING_BINARY_INSTALLERS/","text":"How to build \"binary\" installers (InvokeAI-mac/windows/linux_on_*.zip) # 1. Ensure installers/requirements.in is correct # and up to date on the branch to be installed. 2. Run pip-compile on each platform. # On each target platform, in the branch that is to be installed, and inside the InvokeAI git root folder, run the following commands: conda activate invokeai # or however you activate python pip install pip-tools pip-compile --allow-unsafe --generate-hashes --output-file=binary_installer/<reqsfile>.txt binary_installer/requirements.in where <reqsfile>.txt is whichever of py3.10-darwin-arm64-mps-reqs.txt py3.10-darwin-x86_64-reqs.txt py3.10-linux-x86_64-cuda-reqs.txt py3.10-windows-x86_64-cuda-reqs.txt matches the current OS and architecture. There is no way to cross-compile these. They must be done on a system matching the target OS and arch. 3. Set github repository and branch # Once all reqs files have been collected and committed to the branch to be installed , edit binary_installer/install.sh.in and binary_installer/install.bat.in so that RELEASE_URL and RELEASE_SOURCEBALL point to the github repo and branch that is to be installed. For example, to install main branch of InvokeAI , they should be set as follows: install.sh.in : RELEASE_URL=https://github.com/invoke-ai/InvokeAI RELEASE_SOURCEBALL=/archive/refs/heads/main.tar.gz install.bat.in : set RELEASE_URL=https://github.com/invoke-ai/InvokeAI set RELEASE_SOURCEBALL=/archive/refs/heads/main.tar.gz Or, to install damians-cool-feature branch of damian0815 , set them as follows: install.sh.in : RELEASE_URL=https://github.com/damian0815/InvokeAI RELEASE_SOURCEBALL=/archive/refs/heads/damians-cool-feature.tar.gz install.bat.in : set RELEASE_URL=https://github.com/damian0815/InvokeAI set RELEASE_SOURCEBALL=/archive/refs/heads/damians-cool-feature.tar.gz The branch and repo specified here must contain the correct reqs files. The installer zip files do not contain requirements files, they are pulled from the specified branch during the installation process. 4. Create zip files. # cd into the installers/ folder and run ./create_installers.sh . This will create InvokeAI-mac_on_<branch>.zip , InvokeAI-windows_on_<branch>.zip and InvokeAI-linux_on_<branch>.zip . These files can be distributed to end users. These zips will continue to function as installers for all future pushes to those branches, as long as necessary changes to requirements.in are propagated in a timely manner to the py3.10-*-reqs.txt files using pip-compile as outlined in step 2 . To actually install, users should unzip the appropriate zip file into an empty folder and run install.sh on macOS/Linux or install.bat on Windows.","title":"build binary installers"},{"location":"installation/Developers_documentation/BUILDING_BINARY_INSTALLERS/#how-to-build-binary-installers-invokeai-macwindowslinux_on_zip","text":"","title":" How to build \"binary\" installers (InvokeAI-mac/windows/linux_on_*.zip)"},{"location":"installation/Developers_documentation/BUILDING_BINARY_INSTALLERS/#1-ensure-installersrequirementsin-is-correct","text":"and up to date on the branch to be installed.","title":"1. Ensure installers/requirements.in is correct"},{"location":"installation/Developers_documentation/BUILDING_BINARY_INSTALLERS/#2-run-pip-compile-on-each-platform","text":"On each target platform, in the branch that is to be installed, and inside the InvokeAI git root folder, run the following commands: conda activate invokeai # or however you activate python pip install pip-tools pip-compile --allow-unsafe --generate-hashes --output-file=binary_installer/<reqsfile>.txt binary_installer/requirements.in where <reqsfile>.txt is whichever of py3.10-darwin-arm64-mps-reqs.txt py3.10-darwin-x86_64-reqs.txt py3.10-linux-x86_64-cuda-reqs.txt py3.10-windows-x86_64-cuda-reqs.txt matches the current OS and architecture. There is no way to cross-compile these. They must be done on a system matching the target OS and arch.","title":" 2. Run pip-compile on each platform."},{"location":"installation/Developers_documentation/BUILDING_BINARY_INSTALLERS/#3-set-github-repository-and-branch","text":"Once all reqs files have been collected and committed to the branch to be installed , edit binary_installer/install.sh.in and binary_installer/install.bat.in so that RELEASE_URL and RELEASE_SOURCEBALL point to the github repo and branch that is to be installed. For example, to install main branch of InvokeAI , they should be set as follows: install.sh.in : RELEASE_URL=https://github.com/invoke-ai/InvokeAI RELEASE_SOURCEBALL=/archive/refs/heads/main.tar.gz install.bat.in : set RELEASE_URL=https://github.com/invoke-ai/InvokeAI set RELEASE_SOURCEBALL=/archive/refs/heads/main.tar.gz Or, to install damians-cool-feature branch of damian0815 , set them as follows: install.sh.in : RELEASE_URL=https://github.com/damian0815/InvokeAI RELEASE_SOURCEBALL=/archive/refs/heads/damians-cool-feature.tar.gz install.bat.in : set RELEASE_URL=https://github.com/damian0815/InvokeAI set RELEASE_SOURCEBALL=/archive/refs/heads/damians-cool-feature.tar.gz The branch and repo specified here must contain the correct reqs files. The installer zip files do not contain requirements files, they are pulled from the specified branch during the installation process.","title":" 3. Set github repository and branch"},{"location":"installation/Developers_documentation/BUILDING_BINARY_INSTALLERS/#4-create-zip-files","text":"cd into the installers/ folder and run ./create_installers.sh . This will create InvokeAI-mac_on_<branch>.zip , InvokeAI-windows_on_<branch>.zip and InvokeAI-linux_on_<branch>.zip . These files can be distributed to end users. These zips will continue to function as installers for all future pushes to those branches, as long as necessary changes to requirements.in are propagated in a timely manner to the py3.10-*-reqs.txt files using pip-compile as outlined in step 2 . To actually install, users should unzip the appropriate zip file into an empty folder and run install.sh on macOS/Linux or install.bat on Windows.","title":"4. Create zip files."},{"location":"installation/deprecated_documentation/INSTALL_BINARY/","text":"The InvokeAI binary installer is a shell script that will install InvokeAI onto a stock computer running recent versions of Linux, MacOSX or Windows. It will leave you with a version that runs a stable version of InvokeAI. When a new version of InvokeAI is released, you will download and reinstall the new version. If you wish to tinker with unreleased versions of InvokeAI that introduce potentially unstable new features, you should consider using the source installer or one of the manual install methods. Important Caveats - This script does not support AMD GPUs. For Linux AMD support, please use the manual or source code installer methods. This script has difficulty on some Macintosh machines that have previously been used for Python development due to conflicting development tools versions. Mac developers may wish to try the source code installer or one of the manual methods instead. Todo Before you begin, make sure that you meet the hardware requirements and has the appropriate GPU drivers installed. In particular, if you are a Linux user with an AMD GPU installed, you may need to install the ROCm-driver . Installation requires roughly 18G of free disk space to load the libraries and recommended model weights files. Steps to Install # Download the latest release of InvokeAI's installer for your platform. Look for a file named InvokeAI-binary-<your platform>.zip Place the downloaded package someplace where you have plenty of HDD space, and have full permissions (i.e. ~/ on Lin/Mac; your home folder on Windows) Extract the 'InvokeAI' folder from the downloaded package Open the extracted 'InvokeAI' folder Double-click 'install.bat' (Windows), or 'install.sh' (Lin/Mac) (or run from a terminal) Follow the prompts After installation, please run the 'invoke.bat' file (on Windows) or 'invoke.sh' file (on Linux/Mac) to start InvokeAI. Troubleshooting # If you run into problems during or after installation, the InvokeAI team is available to help you. Either create an Issue at our GitHub site, or make a request for help on the \"bugs-and-support\" channel of our Discord server . We are a 100% volunteer organization, but typically somebody will be available to help you within 24 hours, and often much sooner.","title":"InvokeAI Binary Installer"},{"location":"installation/deprecated_documentation/INSTALL_BINARY/#steps-to-install","text":"Download the latest release of InvokeAI's installer for your platform. Look for a file named InvokeAI-binary-<your platform>.zip Place the downloaded package someplace where you have plenty of HDD space, and have full permissions (i.e. ~/ on Lin/Mac; your home folder on Windows) Extract the 'InvokeAI' folder from the downloaded package Open the extracted 'InvokeAI' folder Double-click 'install.bat' (Windows), or 'install.sh' (Lin/Mac) (or run from a terminal) Follow the prompts After installation, please run the 'invoke.bat' file (on Windows) or 'invoke.sh' file (on Linux/Mac) to start InvokeAI.","title":"Steps to Install"},{"location":"installation/deprecated_documentation/INSTALL_BINARY/#troubleshooting","text":"If you run into problems during or after installation, the InvokeAI team is available to help you. Either create an Issue at our GitHub site, or make a request for help on the \"bugs-and-support\" channel of our Discord server . We are a 100% volunteer organization, but typically somebody will be available to help you within 24 hours, and often much sooner.","title":"Troubleshooting"},{"location":"installation/deprecated_documentation/INSTALL_JUPYTER/","text":"Introduction # We have a Jupyter notebook with cell-by-cell installation steps. It will download the code in this repo as one of the steps, so instead of cloning this repo, simply download the notebook from the link above and load it up in VSCode (with the appropriate extensions installed)/Jupyter/JupyterLab and start running the cells one-by-one. you will need NVIDIA drivers, Python 3.10, and Git installed beforehand Running Online On Google Colabotary # Running Locally (Cloning) # Install the Jupyter Notebook python library (one-time): pip install jupyter Clone the InvokeAI repository: git clone https://github.com/invoke-ai/InvokeAI.git cd invoke-ai Create a virtual environment using conda: conda create -n invoke jupyter Activate the environment and start the Jupyter notebook: conda activate invoke jupyter notebook","title":"Running InvokeAI on Google Colab using a Jupyter Notebook"},{"location":"installation/deprecated_documentation/INSTALL_JUPYTER/#introduction","text":"We have a Jupyter notebook with cell-by-cell installation steps. It will download the code in this repo as one of the steps, so instead of cloning this repo, simply download the notebook from the link above and load it up in VSCode (with the appropriate extensions installed)/Jupyter/JupyterLab and start running the cells one-by-one. you will need NVIDIA drivers, Python 3.10, and Git installed beforehand","title":"Introduction"},{"location":"installation/deprecated_documentation/INSTALL_JUPYTER/#running-online-on-google-colabotary","text":"","title":"Running Online On Google Colabotary"},{"location":"installation/deprecated_documentation/INSTALL_JUPYTER/#running-locally-cloning","text":"Install the Jupyter Notebook python library (one-time): pip install jupyter Clone the InvokeAI repository: git clone https://github.com/invoke-ai/InvokeAI.git cd invoke-ai Create a virtual environment using conda: conda create -n invoke jupyter Activate the environment and start the Jupyter notebook: conda activate invoke jupyter notebook","title":"Running Locally (Cloning)"},{"location":"installation/deprecated_documentation/INSTALL_LINUX/","text":"Linux # Installation # You will need to install the following prerequisites if they are not already available. Use your operating system's preferred installer. Python (version 3.8.5 recommended; higher may work) git Install the Python Anaconda environment manager. ~$ wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh ~$ chmod +x Anaconda3-2022.05-Linux-x86_64.sh ~$ ./Anaconda3-2022.05-Linux-x86_64.sh After installing anaconda, you should log out of your system and log back in. If the installation worked, your command prompt will be prefixed by the name of the current anaconda environment - (base) . Copy the InvokeAI source code from GitHub: ( base ) ~$ git clone https://github.com/invoke-ai/InvokeAI.git This will create InvokeAI folder where you will follow the rest of the steps. Enter the newly-created InvokeAI folder. From this step forward make sure that you are working in the InvokeAI directory! ( base ) ~$ cd InvokeAI ( base ) ~/InvokeAI$ Use anaconda to copy necessary python packages, create a new python environment named invokeai and then activate the environment. For systems with a CUDA (Nvidia) card: ( base ) rm -rf src # (this is a precaution in case there is already a src directory) ( base ) ~/InvokeAI$ conda env create -f environment-cuda.yml ( base ) ~/InvokeAI$ conda activate invokeai ( invokeai ) ~/InvokeAI$ For systems with an AMD card (using ROCm driver): ( base ) rm -rf src # (this is a precaution in case there is already a src directory) ( base ) ~/InvokeAI$ conda env create -f environment-AMD.yml ( base ) ~/InvokeAI$ conda activate invokeai ( invokeai ) ~/InvokeAI$ After these steps, your command prompt will be prefixed by (invokeai) as shown above. Load the big stable diffusion weights files and a couple of smaller machine-learning models: ( invokeai ) ~/InvokeAI$ python3 scripts/configure_invokeai.py Note This script will lead you through the process of creating an account on Hugging Face, accepting the terms and conditions of the Stable Diffusion model license, and obtaining an access token for downloading. It will then download and install the weights files for you. Please look here for a manual process for doing the same thing. Start generating images! Run InvokeAI! IMPORTANT Make sure that the conda environment is activated, which should create (invokeai) in front of your prompt! CLI local Webserver Public Webserver python scripts/invoke.py python scripts/invoke.py --web python scripts/invoke.py --web --host 0 .0.0.0 To use an alternative model you may invoke the !switch command in the CLI, or pass --model <model_name> during invoke.py launch for either the CLI or the Web UI. See Command Line Client . The model names are defined in configs/models.yaml . Subsequently, to relaunch the script, be sure to run \"conda activate invokeai\" (step 5, second command), enter the InvokeAI directory, and then launch the invoke script (step 8). If you forget to activate the 'invokeai' environment, the script will fail with multiple ModuleNotFound errors. Updating to newer versions of the script # This distribution is changing rapidly. If you used the git clone method (step 5) to download the InvokeAI directory, then to update to the latest and greatest version, launch the Anaconda window, enter InvokeAI and type: ( invokeai ) ~/InvokeAI$ git pull ( invokeai ) ~/InvokeAI$ rm -rf src # prevents conda freezing errors ( invokeai ) ~/InvokeAI$ conda env update -f environment.yml This will bring your local copy into sync with the remote one.","title":"Manual Installation, Linux"},{"location":"installation/deprecated_documentation/INSTALL_LINUX/#linux","text":"","title":" Linux"},{"location":"installation/deprecated_documentation/INSTALL_LINUX/#installation","text":"You will need to install the following prerequisites if they are not already available. Use your operating system's preferred installer. Python (version 3.8.5 recommended; higher may work) git Install the Python Anaconda environment manager. ~$ wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh ~$ chmod +x Anaconda3-2022.05-Linux-x86_64.sh ~$ ./Anaconda3-2022.05-Linux-x86_64.sh After installing anaconda, you should log out of your system and log back in. If the installation worked, your command prompt will be prefixed by the name of the current anaconda environment - (base) . Copy the InvokeAI source code from GitHub: ( base ) ~$ git clone https://github.com/invoke-ai/InvokeAI.git This will create InvokeAI folder where you will follow the rest of the steps. Enter the newly-created InvokeAI folder. From this step forward make sure that you are working in the InvokeAI directory! ( base ) ~$ cd InvokeAI ( base ) ~/InvokeAI$ Use anaconda to copy necessary python packages, create a new python environment named invokeai and then activate the environment. For systems with a CUDA (Nvidia) card: ( base ) rm -rf src # (this is a precaution in case there is already a src directory) ( base ) ~/InvokeAI$ conda env create -f environment-cuda.yml ( base ) ~/InvokeAI$ conda activate invokeai ( invokeai ) ~/InvokeAI$ For systems with an AMD card (using ROCm driver): ( base ) rm -rf src # (this is a precaution in case there is already a src directory) ( base ) ~/InvokeAI$ conda env create -f environment-AMD.yml ( base ) ~/InvokeAI$ conda activate invokeai ( invokeai ) ~/InvokeAI$ After these steps, your command prompt will be prefixed by (invokeai) as shown above. Load the big stable diffusion weights files and a couple of smaller machine-learning models: ( invokeai ) ~/InvokeAI$ python3 scripts/configure_invokeai.py Note This script will lead you through the process of creating an account on Hugging Face, accepting the terms and conditions of the Stable Diffusion model license, and obtaining an access token for downloading. It will then download and install the weights files for you. Please look here for a manual process for doing the same thing. Start generating images! Run InvokeAI! IMPORTANT Make sure that the conda environment is activated, which should create (invokeai) in front of your prompt! CLI local Webserver Public Webserver python scripts/invoke.py python scripts/invoke.py --web python scripts/invoke.py --web --host 0 .0.0.0 To use an alternative model you may invoke the !switch command in the CLI, or pass --model <model_name> during invoke.py launch for either the CLI or the Web UI. See Command Line Client . The model names are defined in configs/models.yaml . Subsequently, to relaunch the script, be sure to run \"conda activate invokeai\" (step 5, second command), enter the InvokeAI directory, and then launch the invoke script (step 8). If you forget to activate the 'invokeai' environment, the script will fail with multiple ModuleNotFound errors.","title":"Installation"},{"location":"installation/deprecated_documentation/INSTALL_LINUX/#updating-to-newer-versions-of-the-script","text":"This distribution is changing rapidly. If you used the git clone method (step 5) to download the InvokeAI directory, then to update to the latest and greatest version, launch the Anaconda window, enter InvokeAI and type: ( invokeai ) ~/InvokeAI$ git pull ( invokeai ) ~/InvokeAI$ rm -rf src # prevents conda freezing errors ( invokeai ) ~/InvokeAI$ conda env update -f environment.yml This will bring your local copy into sync with the remote one.","title":"Updating to newer versions of the script"},{"location":"installation/deprecated_documentation/INSTALL_MAC/","text":"macOS # Invoke AI runs quite well on M1 Macs and we have a number of M1 users in the community. While the repo does run on Intel Macs, we only have a couple reports. If you have an Intel Mac and run into issues, please create an issue on Github and we will do our best to help. Requirements # macOS 12.3 Monterey or later About 10GB of storage (and 10GB of data if your internet connection has data caps) Any M1 Macs or an Intel Macs with 4GB+ of VRAM (ideally more) Installation # Homebrew First you will install the \"brew\" package manager. Skip this if brew is already installed. install brew (and Xcode command line tools) /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" Conda Installation Now there are two different ways to set up the Python (miniconda) environment: Standalone with pyenv If you don't know what we are talking about, choose Standalone. If you are familiar with python environments, choose \"with pyenv\" Standalone with pyenv Install cmake, protobuf, and rust brew install cmake protobuf rust Clone the InvokeAI repository # Clone the Invoke AI repo git clone https://github.com/invoke-ai/InvokeAI.git cd InvokeAI Choose the appropriate architecture for your system and install miniconda: M1 arm64 Intel x86_64 Install miniconda for M1 arm64 curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh \\ -o Miniconda3-latest-MacOSX-arm64.sh /bin/bash Miniconda3-latest-MacOSX-arm64.sh Install miniconda for Intel curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh \\ -o Miniconda3-latest-MacOSX-x86_64.sh /bin/bash Miniconda3-latest-MacOSX-x86_64.sh brew install pyenv-virtualenv pyenv install anaconda3-2022.05 pyenv virtualenv anaconda3-2022.05 eval \" $( pyenv init - ) \" pyenv activate anaconda3-2022.05 Clone the Invoke AI repo git clone https://github.com/invoke-ai/InvokeAI.git cd InvokeAI Create the environment & install packages M1 Mac Intel x86_64 Mac PIP_EXISTS_ACTION = w CONDA_SUBDIR = osx-arm64 conda env create -f environment-mac.yml PIP_EXISTS_ACTION = w CONDA_SUBDIR = osx-64 conda env create -f environment-mac.yml # Activate the environment (you need to do this every time you want to run SD) conda activate invokeai Info export PIP_EXISTS_ACTION=w is a precaution to fix conda env create -f environment-mac.yml never finishing in some situations. So it isn't required but won't hurt. Download the model weight files The configure_invokeai.py script downloads and installs the model weight files for you. It will lead you through the process of getting a Hugging Face account, accepting the Stable Diffusion model weight license agreement, and creating a download token: # This will take some time, depending on the speed of your internet connection # and will consume about 10GB of space python scripts/configure_invokeai.py Run InvokeAI! IMPORTANT Make sure that the conda environment is activated, which should create (invokeai) in front of your prompt! CLI local Webserver Public Webserver python scripts/invoke.py python scripts/invoke.py --web python scripts/invoke.py --web --host 0 .0.0.0 To use an alternative model you may invoke the !switch command in the CLI, or pass --model <model_name> during invoke.py launch for either the CLI or the Web UI. See Command Line Client . The model names are defined in configs/models.yaml . Common problems # After you followed all the instructions and try to run invoke.py, you might get several errors. Here's the errors I've seen and found solutions for. Is it slow? # Be sure to specify 1 sample and 1 iteration. python ./scripts/orig_scripts/txt2img.py \\ --prompt \"ocean\" \\ --ddim_steps 5 \\ --n_samples 1 \\ --n_iter 1 Doesn't work anymore? # PyTorch nightly includes support for MPS. Because of this, this setup is inherently unstable. One morning I woke up and it no longer worked no matter what I did until I switched to miniforge. However, I have another Mac that works just fine with Anaconda. If you can't get it to work, please search a little first because many of the errors will get posted and solved. If you can't find a solution please create an issue . One debugging step is to update to the latest version of PyTorch nightly. conda install \\ pytorch \\ torchvision \\ -c pytorch-nightly \\ -n invokeai If it takes forever to run conda env create -f environment-mac.yml , try this: git clean -f conda clean \\ --yes \\ --all Or you could try to completley reset Anaconda: conda update \\ --force-reinstall \\ -y \\ -n base \\ -c defaults conda \"No module named cv2\", torch, 'invokeai', 'transformers', 'taming', etc # There are several causes of these errors: Did you remember to conda activate invokeai ? If your terminal prompt begins with \"(invokeai)\" then you activated it. If it begins with \"(base)\" or something else you haven't. You might've run ./scripts/configure_invokeai.py or ./scripts/invoke.py instead of python ./scripts/configure_invokeai.py or python ./scripts/invoke.py . The cause of this error is long so it's below. if it says you're missing taming you need to rebuild your virtual environment. conda deactivate conda env remove -n invokeai conda env create -f environment-mac.yml If you have activated the invokeai virtual environment and tried rebuilding it, maybe the problem could be that I have something installed that you don't and you'll just need to manually install it. Make sure you activate the virtual environment so it installs there instead of globally. conda activate invokeai pip install <package name> You might also need to install Rust (I mention this again below). How many snakes are living in your computer? # You might have multiple Python installations on your system, in which case it's important to be explicit and consistent about which one to use for a given project. This is because virtual environments are coupled to the Python that created it (and all the associated 'system-level' modules). When you run python or python3 , your shell searches the colon-delimited locations in the PATH environment variable ( echo $PATH to see that list) in that order - first match wins. You can ask for the location of the first python3 found in your PATH with the which command like this: % which python3 /usr/bin/python3 Anything in /usr/bin is part of the OS . However, /usr/bin/python3 is not actually python3, but rather a stub that offers to install Xcode (which includes python 3). If you have Xcode installed already, /usr/bin/python3 will execute /Library/Developer/CommandLineTools/usr/bin/python3 or /Applications/Xcode.app/Contents/Developer/usr/bin/python3 (depending on which Xcode you've selected with xcode-select ). Note that /usr/bin/python is an entirely different python - specifically, python 2. Note: starting in macOS 12.3, /usr/bin/python no longer exists. % which python3 /opt/homebrew/bin/python3 If you installed python3 with Homebrew and you've modified your path to search for Homebrew binaries before system ones, you'll see the above path. % which python /opt/anaconda3/bin/python If you have Anaconda installed, you will see the above path. There is a /opt/anaconda3/bin/python3 also. We expect that /opt/anaconda3/bin/python and /opt/anaconda3/bin/python3 should actually be the same python , which you can verify by comparing the output of python3 -V and python -V . ( invokeai ) % which python /Users/name/miniforge3/envs/invokeai/bin/python The above is what you'll see if you have miniforge and correctly activated the invokeai environment, while usingd the standalone setup instructions above. If you otherwise installed via pyenv, you will get this result: ( anaconda3-2022.05 ) % which python /Users/name/.pyenv/shims/python It's all a mess and you should know how to modify the path environment variable if you want to fix it. Here's a brief hint of the most common ways you can modify it (don't really have the time to explain it all here). ~/.zshrc ~/.bash_profile ~/.bashrc /etc/paths.d /etc/path Which one you use will depend on what you have installed, except putting a file in /etc/paths.d - which also is the way I prefer to do. Finally, to answer the question posed by this section's title, it may help to list all of the python / python3 things found in $PATH instead of just the first hit. To do so, add the -a switch to which : % which -a python3 ... This will show a list of all binaries which are actually available in your PATH. Debugging? # Tired of waiting for your renders to finish before you can see if it works? Reduce the steps! The image quality will be horrible but at least you'll get quick feedback. python ./scripts/txt2img.py \\ --prompt \"ocean\" \\ --ddim_steps 5 \\ --n_samples 1 \\ --n_iter 1 OSError: Can't load tokenizer for 'openai/clip-vit-large-patch14' # python scripts/configure_invokeai.py \"The operator [name] is not current implemented for the MPS device.\" (sic) # example error ... NotImplementedError: The operator 'aten::_index_put_impl_' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable ` PYTORCH_ENABLE_MPS_FALLBACK = 1 ` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS. The InvokeAI version includes this fix in environment-mac.yml . \"Could not build wheels for tokenizers\" # I have not seen this error because I had Rust installed on my computer before I started playing with Stable Diffusion. The fix is to install Rust. curl \\ --proto '=https' \\ --tlsv1.2 \\ -sSf https://sh.rustup.rs | sh How come --seed doesn't work? # Information Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds. PyTorch docs Second, we might have a fix that at least gets a consistent seed sort of. We're still working on it. libiomp5.dylib error? # OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized. You are likely using an Intel package by mistake. Be sure to run conda with the environment variable CONDA_SUBDIR=osx-arm64 , like so: CONDA_SUBDIR=osx-arm64 conda install ... This error happens with Anaconda on Macs when the Intel-only mkl is pulled in by a dependency. nomkl is a metapackage designed to prevent this, by making it impossible to install mkl , but if your environment is already broken it may not work. Do not use os.environ['KMP_DUPLICATE_LIB_OK']='True' or equivalents as this masks the underlying issue of using Intel packages. Not enough memory # This seems to be a common problem and is probably the underlying problem for a lot of symptoms (listed below). The fix is to lower your image size or to add model.half() right after the model is loaded. I should probably test it out. I've read that the reason this fixes problems is because it converts the model from 32-bit to 16-bit and that leaves more RAM for other things. I have no idea how that would affect the quality of the images though. See this issue . \"Error: product of dimension sizes > 2**31'\" # This error happens with img2img, which I haven't played with too much yet. But I know it's because your image is too big or the resolution isn't a multiple of 32x32. Because the stable-diffusion model was trained on images that were 512 x 512, it's always best to use that output size (which is the default). However, if you're using that size and you get the above error, try 256 x 256 or 512 x 256 or something as the source image. BTW, 2**31-1 = 2,147,483,647 , which is also 32-bit signed LONG_MAX in C. I just got Rickrolled! Do I have a virus? # You don't have a virus. It's part of the project. Here's Rick and here's the code that swaps him in. It's a NSFW filter, which IMO, doesn't work very good (and we call this \"computer vision\", sheesh). My images come out black # We might have this fixed, we are still testing. There's a similar issue on CUDA GPU's where the images come out green. Maybe it's the same issue? Someone in that issue says to use \"--precision full\", but this fork actually disables that flag. I don't know why, someone else provided that code and I don't know what it does. Maybe the model.half() suggestion above would fix this issue too. I should probably test it. \"view size is not compatible with input tensor's size and stride\" # File \"/opt/anaconda3/envs/invokeai/lib/python3.10/site-packages/torch/nn/functional.py\" , line 2511 , in layer_norm return torch.layer_norm ( input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled ) RuntimeError: view size is not compatible with input tensor ' s size and stride ( at least one dimension spans across two contiguous subspaces ) . Use .reshape ( ... ) instead. Update to the latest version of invoke-ai/InvokeAI. We were patching pytorch but we found a file in stable-diffusion that we could change instead. This is a 32-bit vs 16-bit problem. The processor must support the Intel bla bla bla # What? Intel? On an Apple Silicon? Intel MKL FATAL ERROR: This system does not meet the minimum requirements for use of the Intel ( R ) Math Kernel Library. The processor must support the Intel ( R ) Supplemental Streaming SIMD Extensions 3 ( Intel ( R ) SSSE3 ) instructions. The processor must support the Intel ( R ) Streaming SIMD Extensions 4 .2 ( Intel ( R ) SSE4.2 ) instructions. The processor must support the Intel ( R ) Advanced Vector Extensions ( Intel ( R ) AVX ) instructions. This is due to the Intel mkl package getting picked up when you try to install something that depends on it-- Rosetta can translate some Intel instructions but not the specialized ones here. To avoid this, make sure to use the environment variable CONDA_SUBDIR=osx-arm64 , which restricts the Conda environment to only use ARM packages, and use nomkl as described above. input types 'tensor<2x1280xf32>' and 'tensor<*xf16>' are not broadcast compatible # May appear when just starting to generate, e.g.: invoke> clouds Generating: 0 % | | 0 /1 [ 00 :00<?, ?it/s ] /Users/ [ ... ] /dev/stable-diffusion/ldm/modules/embedding_manager.py:152: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. ( Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662016319283/work/aten/src/ATen/mps/MPSFallback.mm:11. ) placeholder_idx = torch.where ( loc ( \"mps_add\" ( \"(mpsFileLoc): /AppleInternal/Library/BuildRoots/20d6c351-ee94-11ec-bcaf-7247572f23b4/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\" :219:0 )) : error: input types 'tensor<2x1280xf32>' and 'tensor<*xf16>' are not broadcast compatible LLVM ERROR: Failed to infer result type ( s ) . Abort trap: 6 /Users/ [ ... ] /opt/anaconda3/envs/invokeai/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown warnings.warn ( 'resource_tracker: There appear to be %d '","title":"Manual Installation, macOS"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#macos","text":"Invoke AI runs quite well on M1 Macs and we have a number of M1 users in the community. While the repo does run on Intel Macs, we only have a couple reports. If you have an Intel Mac and run into issues, please create an issue on Github and we will do our best to help.","title":" macOS"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#requirements","text":"macOS 12.3 Monterey or later About 10GB of storage (and 10GB of data if your internet connection has data caps) Any M1 Macs or an Intel Macs with 4GB+ of VRAM (ideally more)","title":"Requirements"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#installation","text":"Homebrew First you will install the \"brew\" package manager. Skip this if brew is already installed. install brew (and Xcode command line tools) /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" Conda Installation Now there are two different ways to set up the Python (miniconda) environment: Standalone with pyenv If you don't know what we are talking about, choose Standalone. If you are familiar with python environments, choose \"with pyenv\" Standalone with pyenv Install cmake, protobuf, and rust brew install cmake protobuf rust Clone the InvokeAI repository # Clone the Invoke AI repo git clone https://github.com/invoke-ai/InvokeAI.git cd InvokeAI Choose the appropriate architecture for your system and install miniconda: M1 arm64 Intel x86_64 Install miniconda for M1 arm64 curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh \\ -o Miniconda3-latest-MacOSX-arm64.sh /bin/bash Miniconda3-latest-MacOSX-arm64.sh Install miniconda for Intel curl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh \\ -o Miniconda3-latest-MacOSX-x86_64.sh /bin/bash Miniconda3-latest-MacOSX-x86_64.sh brew install pyenv-virtualenv pyenv install anaconda3-2022.05 pyenv virtualenv anaconda3-2022.05 eval \" $( pyenv init - ) \" pyenv activate anaconda3-2022.05 Clone the Invoke AI repo git clone https://github.com/invoke-ai/InvokeAI.git cd InvokeAI Create the environment & install packages M1 Mac Intel x86_64 Mac PIP_EXISTS_ACTION = w CONDA_SUBDIR = osx-arm64 conda env create -f environment-mac.yml PIP_EXISTS_ACTION = w CONDA_SUBDIR = osx-64 conda env create -f environment-mac.yml # Activate the environment (you need to do this every time you want to run SD) conda activate invokeai Info export PIP_EXISTS_ACTION=w is a precaution to fix conda env create -f environment-mac.yml never finishing in some situations. So it isn't required but won't hurt. Download the model weight files The configure_invokeai.py script downloads and installs the model weight files for you. It will lead you through the process of getting a Hugging Face account, accepting the Stable Diffusion model weight license agreement, and creating a download token: # This will take some time, depending on the speed of your internet connection # and will consume about 10GB of space python scripts/configure_invokeai.py Run InvokeAI! IMPORTANT Make sure that the conda environment is activated, which should create (invokeai) in front of your prompt! CLI local Webserver Public Webserver python scripts/invoke.py python scripts/invoke.py --web python scripts/invoke.py --web --host 0 .0.0.0 To use an alternative model you may invoke the !switch command in the CLI, or pass --model <model_name> during invoke.py launch for either the CLI or the Web UI. See Command Line Client . The model names are defined in configs/models.yaml .","title":"Installation"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#common-problems","text":"After you followed all the instructions and try to run invoke.py, you might get several errors. Here's the errors I've seen and found solutions for.","title":"Common problems"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#is-it-slow","text":"Be sure to specify 1 sample and 1 iteration. python ./scripts/orig_scripts/txt2img.py \\ --prompt \"ocean\" \\ --ddim_steps 5 \\ --n_samples 1 \\ --n_iter 1","title":"Is it slow?"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#doesnt-work-anymore","text":"PyTorch nightly includes support for MPS. Because of this, this setup is inherently unstable. One morning I woke up and it no longer worked no matter what I did until I switched to miniforge. However, I have another Mac that works just fine with Anaconda. If you can't get it to work, please search a little first because many of the errors will get posted and solved. If you can't find a solution please create an issue . One debugging step is to update to the latest version of PyTorch nightly. conda install \\ pytorch \\ torchvision \\ -c pytorch-nightly \\ -n invokeai If it takes forever to run conda env create -f environment-mac.yml , try this: git clean -f conda clean \\ --yes \\ --all Or you could try to completley reset Anaconda: conda update \\ --force-reinstall \\ -y \\ -n base \\ -c defaults conda","title":"Doesn't work anymore?"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#no-module-named-cv2-torch-invokeai-transformers-taming-etc","text":"There are several causes of these errors: Did you remember to conda activate invokeai ? If your terminal prompt begins with \"(invokeai)\" then you activated it. If it begins with \"(base)\" or something else you haven't. You might've run ./scripts/configure_invokeai.py or ./scripts/invoke.py instead of python ./scripts/configure_invokeai.py or python ./scripts/invoke.py . The cause of this error is long so it's below. if it says you're missing taming you need to rebuild your virtual environment. conda deactivate conda env remove -n invokeai conda env create -f environment-mac.yml If you have activated the invokeai virtual environment and tried rebuilding it, maybe the problem could be that I have something installed that you don't and you'll just need to manually install it. Make sure you activate the virtual environment so it installs there instead of globally. conda activate invokeai pip install <package name> You might also need to install Rust (I mention this again below).","title":"\"No module named cv2\", torch, 'invokeai', 'transformers', 'taming', etc"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#how-many-snakes-are-living-in-your-computer","text":"You might have multiple Python installations on your system, in which case it's important to be explicit and consistent about which one to use for a given project. This is because virtual environments are coupled to the Python that created it (and all the associated 'system-level' modules). When you run python or python3 , your shell searches the colon-delimited locations in the PATH environment variable ( echo $PATH to see that list) in that order - first match wins. You can ask for the location of the first python3 found in your PATH with the which command like this: % which python3 /usr/bin/python3 Anything in /usr/bin is part of the OS . However, /usr/bin/python3 is not actually python3, but rather a stub that offers to install Xcode (which includes python 3). If you have Xcode installed already, /usr/bin/python3 will execute /Library/Developer/CommandLineTools/usr/bin/python3 or /Applications/Xcode.app/Contents/Developer/usr/bin/python3 (depending on which Xcode you've selected with xcode-select ). Note that /usr/bin/python is an entirely different python - specifically, python 2. Note: starting in macOS 12.3, /usr/bin/python no longer exists. % which python3 /opt/homebrew/bin/python3 If you installed python3 with Homebrew and you've modified your path to search for Homebrew binaries before system ones, you'll see the above path. % which python /opt/anaconda3/bin/python If you have Anaconda installed, you will see the above path. There is a /opt/anaconda3/bin/python3 also. We expect that /opt/anaconda3/bin/python and /opt/anaconda3/bin/python3 should actually be the same python , which you can verify by comparing the output of python3 -V and python -V . ( invokeai ) % which python /Users/name/miniforge3/envs/invokeai/bin/python The above is what you'll see if you have miniforge and correctly activated the invokeai environment, while usingd the standalone setup instructions above. If you otherwise installed via pyenv, you will get this result: ( anaconda3-2022.05 ) % which python /Users/name/.pyenv/shims/python It's all a mess and you should know how to modify the path environment variable if you want to fix it. Here's a brief hint of the most common ways you can modify it (don't really have the time to explain it all here). ~/.zshrc ~/.bash_profile ~/.bashrc /etc/paths.d /etc/path Which one you use will depend on what you have installed, except putting a file in /etc/paths.d - which also is the way I prefer to do. Finally, to answer the question posed by this section's title, it may help to list all of the python / python3 things found in $PATH instead of just the first hit. To do so, add the -a switch to which : % which -a python3 ... This will show a list of all binaries which are actually available in your PATH.","title":"How many snakes are living in your computer?"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#debugging","text":"Tired of waiting for your renders to finish before you can see if it works? Reduce the steps! The image quality will be horrible but at least you'll get quick feedback. python ./scripts/txt2img.py \\ --prompt \"ocean\" \\ --ddim_steps 5 \\ --n_samples 1 \\ --n_iter 1","title":"Debugging?"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#oserror-cant-load-tokenizer-for-openaiclip-vit-large-patch14","text":"python scripts/configure_invokeai.py","title":"OSError: Can't load tokenizer for 'openai/clip-vit-large-patch14'"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#the-operator-name-is-not-current-implemented-for-the-mps-device-sic","text":"example error ... NotImplementedError: The operator 'aten::_index_put_impl_' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable ` PYTORCH_ENABLE_MPS_FALLBACK = 1 ` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS. The InvokeAI version includes this fix in environment-mac.yml .","title":"\"The operator [name] is not current implemented for the MPS device.\" (sic)"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#could-not-build-wheels-for-tokenizers","text":"I have not seen this error because I had Rust installed on my computer before I started playing with Stable Diffusion. The fix is to install Rust. curl \\ --proto '=https' \\ --tlsv1.2 \\ -sSf https://sh.rustup.rs | sh","title":"\"Could not build wheels for tokenizers\""},{"location":"installation/deprecated_documentation/INSTALL_MAC/#how-come-seed-doesnt-work","text":"Information Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds. PyTorch docs Second, we might have a fix that at least gets a consistent seed sort of. We're still working on it.","title":"How come --seed doesn't work?"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#libiomp5dylib-error","text":"OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized. You are likely using an Intel package by mistake. Be sure to run conda with the environment variable CONDA_SUBDIR=osx-arm64 , like so: CONDA_SUBDIR=osx-arm64 conda install ... This error happens with Anaconda on Macs when the Intel-only mkl is pulled in by a dependency. nomkl is a metapackage designed to prevent this, by making it impossible to install mkl , but if your environment is already broken it may not work. Do not use os.environ['KMP_DUPLICATE_LIB_OK']='True' or equivalents as this masks the underlying issue of using Intel packages.","title":"libiomp5.dylib error?"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#not-enough-memory","text":"This seems to be a common problem and is probably the underlying problem for a lot of symptoms (listed below). The fix is to lower your image size or to add model.half() right after the model is loaded. I should probably test it out. I've read that the reason this fixes problems is because it converts the model from 32-bit to 16-bit and that leaves more RAM for other things. I have no idea how that would affect the quality of the images though. See this issue .","title":"Not enough memory"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#error-product-of-dimension-sizes-231","text":"This error happens with img2img, which I haven't played with too much yet. But I know it's because your image is too big or the resolution isn't a multiple of 32x32. Because the stable-diffusion model was trained on images that were 512 x 512, it's always best to use that output size (which is the default). However, if you're using that size and you get the above error, try 256 x 256 or 512 x 256 or something as the source image. BTW, 2**31-1 = 2,147,483,647 , which is also 32-bit signed LONG_MAX in C.","title":"\"Error: product of dimension sizes &gt; 2**31'\""},{"location":"installation/deprecated_documentation/INSTALL_MAC/#i-just-got-rickrolled-do-i-have-a-virus","text":"You don't have a virus. It's part of the project. Here's Rick and here's the code that swaps him in. It's a NSFW filter, which IMO, doesn't work very good (and we call this \"computer vision\", sheesh).","title":"I just got Rickrolled! Do I have a virus?"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#my-images-come-out-black","text":"We might have this fixed, we are still testing. There's a similar issue on CUDA GPU's where the images come out green. Maybe it's the same issue? Someone in that issue says to use \"--precision full\", but this fork actually disables that flag. I don't know why, someone else provided that code and I don't know what it does. Maybe the model.half() suggestion above would fix this issue too. I should probably test it.","title":"My images come out black"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#view-size-is-not-compatible-with-input-tensors-size-and-stride","text":"File \"/opt/anaconda3/envs/invokeai/lib/python3.10/site-packages/torch/nn/functional.py\" , line 2511 , in layer_norm return torch.layer_norm ( input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled ) RuntimeError: view size is not compatible with input tensor ' s size and stride ( at least one dimension spans across two contiguous subspaces ) . Use .reshape ( ... ) instead. Update to the latest version of invoke-ai/InvokeAI. We were patching pytorch but we found a file in stable-diffusion that we could change instead. This is a 32-bit vs 16-bit problem.","title":"\"view size is not compatible with input tensor's size and stride\""},{"location":"installation/deprecated_documentation/INSTALL_MAC/#the-processor-must-support-the-intel-bla-bla-bla","text":"What? Intel? On an Apple Silicon? Intel MKL FATAL ERROR: This system does not meet the minimum requirements for use of the Intel ( R ) Math Kernel Library. The processor must support the Intel ( R ) Supplemental Streaming SIMD Extensions 3 ( Intel ( R ) SSSE3 ) instructions. The processor must support the Intel ( R ) Streaming SIMD Extensions 4 .2 ( Intel ( R ) SSE4.2 ) instructions. The processor must support the Intel ( R ) Advanced Vector Extensions ( Intel ( R ) AVX ) instructions. This is due to the Intel mkl package getting picked up when you try to install something that depends on it-- Rosetta can translate some Intel instructions but not the specialized ones here. To avoid this, make sure to use the environment variable CONDA_SUBDIR=osx-arm64 , which restricts the Conda environment to only use ARM packages, and use nomkl as described above.","title":"The processor must support the Intel bla bla bla"},{"location":"installation/deprecated_documentation/INSTALL_MAC/#input-types-tensor2x1280xf32-and-tensorxf16-are-not-broadcast-compatible","text":"May appear when just starting to generate, e.g.: invoke> clouds Generating: 0 % | | 0 /1 [ 00 :00<?, ?it/s ] /Users/ [ ... ] /dev/stable-diffusion/ldm/modules/embedding_manager.py:152: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. ( Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1662016319283/work/aten/src/ATen/mps/MPSFallback.mm:11. ) placeholder_idx = torch.where ( loc ( \"mps_add\" ( \"(mpsFileLoc): /AppleInternal/Library/BuildRoots/20d6c351-ee94-11ec-bcaf-7247572f23b4/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\" :219:0 )) : error: input types 'tensor<2x1280xf32>' and 'tensor<*xf16>' are not broadcast compatible LLVM ERROR: Failed to infer result type ( s ) . Abort trap: 6 /Users/ [ ... ] /opt/anaconda3/envs/invokeai/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown warnings.warn ( 'resource_tracker: There appear to be %d '","title":"input types 'tensor&lt;2x1280xf32&gt;' and 'tensor&lt;*xf16&gt;' are not broadcast compatible"},{"location":"installation/deprecated_documentation/INSTALL_PCP/","text":"THIS NEEDS TO BE FLESHED OUT # Introduction # Walkthrough # Updating to newer versions # Updating the stable version # Updating to the development version # Troubleshooting #","title":"Installing InvokeAI with the Pre-Compiled PIP Installer"},{"location":"installation/deprecated_documentation/INSTALL_PCP/#this-needs-to-be-fleshed-out","text":"","title":"THIS NEEDS TO BE FLESHED OUT"},{"location":"installation/deprecated_documentation/INSTALL_PCP/#introduction","text":"","title":"Introduction"},{"location":"installation/deprecated_documentation/INSTALL_PCP/#walkthrough","text":"","title":"Walkthrough"},{"location":"installation/deprecated_documentation/INSTALL_PCP/#updating-to-newer-versions","text":"","title":"Updating to newer versions"},{"location":"installation/deprecated_documentation/INSTALL_PCP/#updating-the-stable-version","text":"","title":"Updating the stable version"},{"location":"installation/deprecated_documentation/INSTALL_PCP/#updating-to-the-development-version","text":"","title":"Updating to the development version"},{"location":"installation/deprecated_documentation/INSTALL_PCP/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/","text":"The InvokeAI Source Installer # Introduction # The source installer is a shell script that attempts to automate every step needed to install and run InvokeAI on a stock computer running recent versions of Linux, MacOS or Windows. It will leave you with a version that runs a stable version of InvokeAI with the option to upgrade to experimental versions later. Before you begin, make sure that you meet the hardware requirements and has the appropriate GPU drivers installed. In particular, if you are a Linux user with an AMD GPU installed, you may need to install the ROCm driver . Installation requires roughly 18G of free disk space to load the libraries and recommended model weights files. Walk through # Though there are multiple steps, there really is only one click involved to kick off the process. The source installer is distributed in ZIP files. Go to the latest release , and look for a series of files named: invokeAI-src-installer-2.2.3-mac.zip invokeAI-src-installer-2.2.3-windows.zip invokeAI-src-installer-2.2.3-linux.zip Download the one that is appropriate for your operating system. Unpack the zip file into a directory that has at least 18G of free space. Do not unpack into a directory that has an earlier version of InvokeAI. This will create a new directory named \"InvokeAI\". This example shows how this would look using the unzip command-line tool, but you may use any graphical or command-line Zip extractor: C:\\Documents\\Linco> unzip invokeAI-windows.zip Archive: C: \\Linco\\Downloads\\invokeAI-linux.zip creating: invokeAI\\ inflating: invokeAI\\install.bat inflating: invokeAI\\readme.txt If you are a macOS user, you may need to install the Xcode command line tools. These are a set of tools that are needed to run certain applications in a Terminal, including InvokeAI. This package is provided directly by Apple. To install, open a terminal window and run xcode-select --install . You will get a macOS system popup guiding you through the install. If you already have them installed, you will instead see some output in the Terminal advising you that the tools are already installed. More information can be found here: https://www.freecodecamp.org/news/install-xcode-command-line-tools/ If you are using a desktop GUI, double-click the installer file. It will be named install.bat on Windows systems and install.sh on Linux and Macintosh systems. Alternatively, from the command line, run the shell script or .bat file: C:\\Documents\\Linco> cd invokeAI C:\\Documents\\Linco\\invokeAI> install.bat Sit back and let the install script work. It will install various binary requirements including Conda, Git and Python, then download the current InvokeAI code and install it along with its dependencies. Be aware that some of the library download and install steps take a long time. In particular, the pytorch package is quite large and often appears to get \"stuck\" at 99.9%. Similarly, the pip installing requirements step may appear to hang. Have patience and the installation step will eventually resume. However, there are occasions when the library install does legitimately get stuck. If you have been waiting for more than ten minutes and nothing is happening, you can interrupt the script with ^C. You may restart it and it will pick up where it left off. After installation completes, the installer will launch a script called configure_invokeai.py , which will guide you through the first-time process of selecting one or more Stable Diffusion model weights files, downloading and configuring them. Note that the main Stable Diffusion weights file is protected by a license agreement that you must agree to in order to use. The script will list the steps you need to take to create an account on the official site that hosts the weights files, accept the agreement, and provide an access token that allows InvokeAI to legally download and install the weights files. If you have already downloaded the weights file(s) for another Stable Diffusion distribution, you may skip this step (by selecting \"skip\" when prompted) and configure InvokeAI to use the previously-downloaded files. The process for this is described in Installing Models . The script will now exit and you'll be ready to generate some images. The invokeAI directory will contain numerous files. Look for a shell script named invoke.sh (Linux/Mac) or invoke.bat (Windows). Launch the script by double-clicking it or typing its name at the command-line: C:\\Documents\\Linco> cd invokeAI C:\\Documents\\Linco\\invokeAI> invoke.bat The invoke.bat ( invoke.sh ) script will give you the choice of starting (1) the command-line interface, or (2) the web GUI. If you start the latter, you can load the user interface by pointing your browser at http://localhost:9090 . The invoke script also offers you a third option labeled \"open the developer console\". If you choose this option, you will be dropped into a command-line interface in which you can run python commands directly, access developer tools, and launch InvokeAI with customized options. To do the latter, you would launch the script scripts/invoke.py as shown in this example: python scripts/invoke.py --web --max_load_models=3 \\ --model=waifu-1.3 --steps=30 --outdir=C:/Documents/AIPhotos These options are described in detail in the Command-Line Interface documentation. Troubleshooting # Package dependency conflicts If you have previously installed InvokeAI or another Stable Diffusion package, the installer may occasionally pick up outdated libraries and either the installer or invoke will fail with complaints out library conflicts. There are two steps you can take to clear this problem. Both of these are done from within the \"developer's console\", which you can get to by launching invoke.sh (or invoke.bat ) and selecting launch option 3: # Remove the previous invokeai environment completely. From within the developer's console, give the command conda env remove -n invokeai . This will delete previous files installed by invoke . Then exit from the developer's console and launch the script update.sh (or update.bat ). This will download the most recent InvokeAI (including bug fixes) and reinstall the environment. You should then be able to run invoke.sh / invoke.bat . If this doesn't work, you can try cleaning your system's conda cache. This is slightly more extreme, but won't interfere with any other python-based programs installed on your computer. From the developer's console, run the command conda clean -a and answer \"yes\" to all prompts. After this is done, run update.sh and try again as before. \"Corrupted configuration file.\"_ Everything seems to install ok, but invoke complains of a corrupted configuration file and goes calls configure_invokeai.py to fix, but this doesn't fix the problem. This issue is often caused by a misconfigured configuration directive in the .invokeai initialization file that contains startup settings. This can be corrected by fixing the offending line. First find .invokeai . It is a small text file located in your home directory, ~/.invokeai on Mac and Linux systems, and C:\\Users\\*your name*\\.invokeai on Windows systems. Open it with a text editor (e.g. Notepad on Windows, TextEdit on Macs, or nano on Linux) and look for the lines starting with --root and --outdir . An example is here: --root=\"/home/lstein/invokeai\" --outdir=\"/home/lstein/invokeai/outputs\" There should not be whitespace before or after the directory paths, and the paths should not end with slashes: --root=\"/home/lstein/invokeai \" # wrong! no whitespace here --root=\"/home\\lstein\\invokeai\\\" # wrong! shouldn't end in a slash Fix the problem with your text editor and save as a plain text file. This should clear the issue. If none of these maneuvers fixes the problem then please report the problem to the InvokeAI Issues section, or visit our Discord Server for interactive assistance. Updating to newer versions # This section describes how to update InvokeAI to new versions of the software. Updating the stable version # This distribution is changing rapidly, and we add new features on a daily basis. To update to the latest released version (recommended), run the update.sh (Linux/Mac) or update.bat (Windows) scripts. This will fetch the latest release and re-run the configure_invokeai script to download any updated models files that may be needed. You can also use this to add additional models that you did not select at installation time. You can now close the developer console and run invoke as before. If you get complaints about missing models, then you may need to do the additional step of running configure_invokeai.py . This happens relatively infrequently. To do this, simply open up the developer's console again and type python scripts/configure_invokeai.py . Troubleshooting # If you run into problems during or after installation, the InvokeAI team is available to help you. Either create an Issue at our GitHub site, or make a request for help on the \"bugs-and-support\" channel of our Discord server . We are a 100% volunteer organization, but typically somebody will be available to help you within 24 hours, and often much sooner.","title":"Source Installer"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/#the-invokeai-source-installer","text":"","title":"The InvokeAI Source Installer"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/#introduction","text":"The source installer is a shell script that attempts to automate every step needed to install and run InvokeAI on a stock computer running recent versions of Linux, MacOS or Windows. It will leave you with a version that runs a stable version of InvokeAI with the option to upgrade to experimental versions later. Before you begin, make sure that you meet the hardware requirements and has the appropriate GPU drivers installed. In particular, if you are a Linux user with an AMD GPU installed, you may need to install the ROCm driver . Installation requires roughly 18G of free disk space to load the libraries and recommended model weights files.","title":"Introduction"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/#walk-through","text":"Though there are multiple steps, there really is only one click involved to kick off the process. The source installer is distributed in ZIP files. Go to the latest release , and look for a series of files named: invokeAI-src-installer-2.2.3-mac.zip invokeAI-src-installer-2.2.3-windows.zip invokeAI-src-installer-2.2.3-linux.zip Download the one that is appropriate for your operating system. Unpack the zip file into a directory that has at least 18G of free space. Do not unpack into a directory that has an earlier version of InvokeAI. This will create a new directory named \"InvokeAI\". This example shows how this would look using the unzip command-line tool, but you may use any graphical or command-line Zip extractor: C:\\Documents\\Linco> unzip invokeAI-windows.zip Archive: C: \\Linco\\Downloads\\invokeAI-linux.zip creating: invokeAI\\ inflating: invokeAI\\install.bat inflating: invokeAI\\readme.txt If you are a macOS user, you may need to install the Xcode command line tools. These are a set of tools that are needed to run certain applications in a Terminal, including InvokeAI. This package is provided directly by Apple. To install, open a terminal window and run xcode-select --install . You will get a macOS system popup guiding you through the install. If you already have them installed, you will instead see some output in the Terminal advising you that the tools are already installed. More information can be found here: https://www.freecodecamp.org/news/install-xcode-command-line-tools/ If you are using a desktop GUI, double-click the installer file. It will be named install.bat on Windows systems and install.sh on Linux and Macintosh systems. Alternatively, from the command line, run the shell script or .bat file: C:\\Documents\\Linco> cd invokeAI C:\\Documents\\Linco\\invokeAI> install.bat Sit back and let the install script work. It will install various binary requirements including Conda, Git and Python, then download the current InvokeAI code and install it along with its dependencies. Be aware that some of the library download and install steps take a long time. In particular, the pytorch package is quite large and often appears to get \"stuck\" at 99.9%. Similarly, the pip installing requirements step may appear to hang. Have patience and the installation step will eventually resume. However, there are occasions when the library install does legitimately get stuck. If you have been waiting for more than ten minutes and nothing is happening, you can interrupt the script with ^C. You may restart it and it will pick up where it left off. After installation completes, the installer will launch a script called configure_invokeai.py , which will guide you through the first-time process of selecting one or more Stable Diffusion model weights files, downloading and configuring them. Note that the main Stable Diffusion weights file is protected by a license agreement that you must agree to in order to use. The script will list the steps you need to take to create an account on the official site that hosts the weights files, accept the agreement, and provide an access token that allows InvokeAI to legally download and install the weights files. If you have already downloaded the weights file(s) for another Stable Diffusion distribution, you may skip this step (by selecting \"skip\" when prompted) and configure InvokeAI to use the previously-downloaded files. The process for this is described in Installing Models . The script will now exit and you'll be ready to generate some images. The invokeAI directory will contain numerous files. Look for a shell script named invoke.sh (Linux/Mac) or invoke.bat (Windows). Launch the script by double-clicking it or typing its name at the command-line: C:\\Documents\\Linco> cd invokeAI C:\\Documents\\Linco\\invokeAI> invoke.bat The invoke.bat ( invoke.sh ) script will give you the choice of starting (1) the command-line interface, or (2) the web GUI. If you start the latter, you can load the user interface by pointing your browser at http://localhost:9090 . The invoke script also offers you a third option labeled \"open the developer console\". If you choose this option, you will be dropped into a command-line interface in which you can run python commands directly, access developer tools, and launch InvokeAI with customized options. To do the latter, you would launch the script scripts/invoke.py as shown in this example: python scripts/invoke.py --web --max_load_models=3 \\ --model=waifu-1.3 --steps=30 --outdir=C:/Documents/AIPhotos These options are described in detail in the Command-Line Interface documentation.","title":"Walk through"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/#troubleshooting","text":"Package dependency conflicts If you have previously installed InvokeAI or another Stable Diffusion package, the installer may occasionally pick up outdated libraries and either the installer or invoke will fail with complaints out library conflicts. There are two steps you can take to clear this problem. Both of these are done from within the \"developer's console\", which you can get to by launching invoke.sh (or invoke.bat ) and selecting launch option","title":"Troubleshooting"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/#3","text":"Remove the previous invokeai environment completely. From within the developer's console, give the command conda env remove -n invokeai . This will delete previous files installed by invoke . Then exit from the developer's console and launch the script update.sh (or update.bat ). This will download the most recent InvokeAI (including bug fixes) and reinstall the environment. You should then be able to run invoke.sh / invoke.bat . If this doesn't work, you can try cleaning your system's conda cache. This is slightly more extreme, but won't interfere with any other python-based programs installed on your computer. From the developer's console, run the command conda clean -a and answer \"yes\" to all prompts. After this is done, run update.sh and try again as before. \"Corrupted configuration file.\"_ Everything seems to install ok, but invoke complains of a corrupted configuration file and goes calls configure_invokeai.py to fix, but this doesn't fix the problem. This issue is often caused by a misconfigured configuration directive in the .invokeai initialization file that contains startup settings. This can be corrected by fixing the offending line. First find .invokeai . It is a small text file located in your home directory, ~/.invokeai on Mac and Linux systems, and C:\\Users\\*your name*\\.invokeai on Windows systems. Open it with a text editor (e.g. Notepad on Windows, TextEdit on Macs, or nano on Linux) and look for the lines starting with --root and --outdir . An example is here: --root=\"/home/lstein/invokeai\" --outdir=\"/home/lstein/invokeai/outputs\" There should not be whitespace before or after the directory paths, and the paths should not end with slashes: --root=\"/home/lstein/invokeai \" # wrong! no whitespace here --root=\"/home\\lstein\\invokeai\\\" # wrong! shouldn't end in a slash Fix the problem with your text editor and save as a plain text file. This should clear the issue. If none of these maneuvers fixes the problem then please report the problem to the InvokeAI Issues section, or visit our Discord Server for interactive assistance.","title":"3:"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/#updating-to-newer-versions","text":"This section describes how to update InvokeAI to new versions of the software.","title":"Updating to newer versions"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/#updating-the-stable-version","text":"This distribution is changing rapidly, and we add new features on a daily basis. To update to the latest released version (recommended), run the update.sh (Linux/Mac) or update.bat (Windows) scripts. This will fetch the latest release and re-run the configure_invokeai script to download any updated models files that may be needed. You can also use this to add additional models that you did not select at installation time. You can now close the developer console and run invoke as before. If you get complaints about missing models, then you may need to do the additional step of running configure_invokeai.py . This happens relatively infrequently. To do this, simply open up the developer's console again and type python scripts/configure_invokeai.py .","title":"Updating the stable version"},{"location":"installation/deprecated_documentation/INSTALL_SOURCE/#troubleshooting_1","text":"If you run into problems during or after installation, the InvokeAI team is available to help you. Either create an Issue at our GitHub site, or make a request for help on the \"bugs-and-support\" channel of our Discord server . We are a 100% volunteer organization, but typically somebody will be available to help you within 24 hours, and often much sooner.","title":"Troubleshooting"},{"location":"installation/deprecated_documentation/INSTALL_WINDOWS/","text":"Windows # Notebook install (semi-automated) # We have a Jupyter notebook with cell-by-cell installation steps. It will download the code in this repo as one of the steps, so instead of cloning this repo, simply download the notebook from the link above and load it up in VSCode (with the appropriate extensions installed)/Jupyter/JupyterLab and start running the cells one-by-one. Note that you will need NVIDIA drivers, Python 3.10, and Git installed beforehand. Manual Install with Conda # Install Anaconda3 (miniconda3 version) from here Install Git from here Launch Anaconda from the Windows Start menu. This will bring up a command window. Type all the remaining commands in this window. Run the command: git clone https://github.com/invoke-ai/InvokeAI.git This will create stable-diffusion folder where you will follow the rest of the steps. Enter the newly-created InvokeAI folder. From this step forward make sure that you are working in the InvokeAI directory! cd InvokeAI Run the following commands: For systems with a CUDA (Nvidia) card: rmdir src # (this is a precaution in case there is already a src directory) conda env create -f environment-cuda.yml conda activate invokeai ( invokeai ) > For systems with an AMD card (using ROCm driver): rmdir src # (this is a precaution in case there is already a src directory) conda env create -f environment-AMD.yml conda activate invokeai ( invokeai ) > This will install all python requirements and activate the \"invokeai\" environment which sets PATH and other environment variables properly. Load the big stable diffusion weights files and a couple of smaller machine-learning models: python scripts/configure_invokeai.py Note This script will lead you through the process of creating an account on Hugging Face, accepting the terms and conditions of the Stable Diffusion model license, and obtaining an access token for downloading. It will then download and install the weights files for you. Please look here for a manual process for doing the same thing. Start generating images! IMPORTANT Make sure that the conda environment is activated, which should create (invokeai) in front of your prompt! CLI local Webserver Public Webserver python scripts/invoke.py python scripts/invoke.py --web python scripts/invoke.py --web --host 0 .0.0.0 To use an alternative model you may invoke the !switch command in the CLI, or pass --model <model_name> during invoke.py launch for either the CLI or the Web UI. See Command Line Client . The model names are defined in configs/models.yaml . Subsequently, to relaunch the script, first activate the Anaconda command window (step 3),enter the InvokeAI directory (step 5, cd \\path\\to\\InvokeAI ), run conda activate invokeai (step 6b), and then launch the invoke script (step 9). Tildebyte has written an alternative \"Easy peasy Windows install\" which uses the Windows Powershell and pew. If you are having trouble with Anaconda on Windows, give this a try (or try it first!) This distribution is changing rapidly. If you used the git clone method (step 5) to download the stable-diffusion directory, then to update to the latest and greatest version, launch the Anaconda window, enter stable-diffusion , and type: git pull conda env update This will bring your local copy into sync with the remote one.","title":"Manual Installation, Windows"},{"location":"installation/deprecated_documentation/INSTALL_WINDOWS/#windows","text":"","title":" Windows"},{"location":"installation/deprecated_documentation/INSTALL_WINDOWS/#notebook-install-semi-automated","text":"We have a Jupyter notebook with cell-by-cell installation steps. It will download the code in this repo as one of the steps, so instead of cloning this repo, simply download the notebook from the link above and load it up in VSCode (with the appropriate extensions installed)/Jupyter/JupyterLab and start running the cells one-by-one. Note that you will need NVIDIA drivers, Python 3.10, and Git installed beforehand.","title":"Notebook install (semi-automated)"},{"location":"installation/deprecated_documentation/INSTALL_WINDOWS/#manual-install-with-conda","text":"Install Anaconda3 (miniconda3 version) from here Install Git from here Launch Anaconda from the Windows Start menu. This will bring up a command window. Type all the remaining commands in this window. Run the command: git clone https://github.com/invoke-ai/InvokeAI.git This will create stable-diffusion folder where you will follow the rest of the steps. Enter the newly-created InvokeAI folder. From this step forward make sure that you are working in the InvokeAI directory! cd InvokeAI Run the following commands: For systems with a CUDA (Nvidia) card: rmdir src # (this is a precaution in case there is already a src directory) conda env create -f environment-cuda.yml conda activate invokeai ( invokeai ) > For systems with an AMD card (using ROCm driver): rmdir src # (this is a precaution in case there is already a src directory) conda env create -f environment-AMD.yml conda activate invokeai ( invokeai ) > This will install all python requirements and activate the \"invokeai\" environment which sets PATH and other environment variables properly. Load the big stable diffusion weights files and a couple of smaller machine-learning models: python scripts/configure_invokeai.py Note This script will lead you through the process of creating an account on Hugging Face, accepting the terms and conditions of the Stable Diffusion model license, and obtaining an access token for downloading. It will then download and install the weights files for you. Please look here for a manual process for doing the same thing. Start generating images! IMPORTANT Make sure that the conda environment is activated, which should create (invokeai) in front of your prompt! CLI local Webserver Public Webserver python scripts/invoke.py python scripts/invoke.py --web python scripts/invoke.py --web --host 0 .0.0.0 To use an alternative model you may invoke the !switch command in the CLI, or pass --model <model_name> during invoke.py launch for either the CLI or the Web UI. See Command Line Client . The model names are defined in configs/models.yaml . Subsequently, to relaunch the script, first activate the Anaconda command window (step 3),enter the InvokeAI directory (step 5, cd \\path\\to\\InvokeAI ), run conda activate invokeai (step 6b), and then launch the invoke script (step 9). Tildebyte has written an alternative \"Easy peasy Windows install\" which uses the Windows Powershell and pew. If you are having trouble with Anaconda on Windows, give this a try (or try it first!) This distribution is changing rapidly. If you used the git clone method (step 5) to download the stable-diffusion directory, then to update to the latest and greatest version, launch the Anaconda window, enter stable-diffusion , and type: git pull conda env update This will bring your local copy into sync with the remote one.","title":"Manual Install with Conda"},{"location":"other/CONTRIBUTORS/","text":"Contributors # The list of all the amazing people who have contributed to the various features that you get to experience in this fork. We thank them for all of their time and hard work. Original Author # Lincoln D. Stein Current core team # @lstein (Lincoln Stein) - Co-maintainer @blessedcoolant - Co-maintainer @hipsterusername (Kent Keirsey) - Product Manager @psychedelicious - Web Team Leader @Kyle0654 (Kyle Schouviller) - Node Architect and General Backend Wizard @damian0815 - Attention Systems and Gameplay Engineer @mauwii (Matthias Wild) - Continuous integration and product maintenance engineer @Netsvetaev (Artur Netsvetaev) - UI/UX Developer @tildebyte - General gadfly and resident (self-appointed) know-it-all @keturn - Lead for Diffusers port @ebr (Eugene Brodsky) - Cloud/DevOps/Sofware engineer; your friendly neighbourhood cluster-autoscaler @jpphoto (Jonathan Pollack) - Inference and rendering engine optimization @genomancer (Gregg Helt) - Model training and merging Contributions by # Sean McLellan Kevin Gibbons Tesseract Cat blessedcoolant David Ford yunsaki James Reynolds David Wager Jason Toffaletti tildebyte Cragin Godley BlueAmulet Benjamin Warner Cora Johnson-Roberson veprogames JigenD Niek van der Maas Henry van Megen H\u00e5vard Gulldahl greentext2 Simon Vans-Colina Gabriel Rotbart Eric Khun Brent Ozar nderscore Mikhail Tishin Tom Elovi Spruce spezialspezial Yosuke Shinya Andy Pilate Muhammad Usama Arturo Mendivil Paul Sajna Samuel Husso nicolai256 Mihai Any Winter Doggettx Matthias Wild Kyle Schouviller rabidcopy Dominic Letz Dmitry T. Kent Keirsey psychedelicious damian0815 Eugene Brodsky Original CompVis Authors # Robin Rombach Patrick von Platen ablattmann Patrick Esser owenvincent apolinario Charles Packer If you have contributed and don't see your name on the list of contributors, please let one of the collaborators know about the omission, or feel free to make a pull request.","title":"Contributors"},{"location":"other/CONTRIBUTORS/#contributors","text":"The list of all the amazing people who have contributed to the various features that you get to experience in this fork. We thank them for all of their time and hard work.","title":" Contributors"},{"location":"other/CONTRIBUTORS/#original-author","text":"Lincoln D. Stein","title":"Original Author"},{"location":"other/CONTRIBUTORS/#current-core-team","text":"@lstein (Lincoln Stein) - Co-maintainer @blessedcoolant - Co-maintainer @hipsterusername (Kent Keirsey) - Product Manager @psychedelicious - Web Team Leader @Kyle0654 (Kyle Schouviller) - Node Architect and General Backend Wizard @damian0815 - Attention Systems and Gameplay Engineer @mauwii (Matthias Wild) - Continuous integration and product maintenance engineer @Netsvetaev (Artur Netsvetaev) - UI/UX Developer @tildebyte - General gadfly and resident (self-appointed) know-it-all @keturn - Lead for Diffusers port @ebr (Eugene Brodsky) - Cloud/DevOps/Sofware engineer; your friendly neighbourhood cluster-autoscaler @jpphoto (Jonathan Pollack) - Inference and rendering engine optimization @genomancer (Gregg Helt) - Model training and merging","title":"Current core team"},{"location":"other/CONTRIBUTORS/#contributions-by","text":"Sean McLellan Kevin Gibbons Tesseract Cat blessedcoolant David Ford yunsaki James Reynolds David Wager Jason Toffaletti tildebyte Cragin Godley BlueAmulet Benjamin Warner Cora Johnson-Roberson veprogames JigenD Niek van der Maas Henry van Megen H\u00e5vard Gulldahl greentext2 Simon Vans-Colina Gabriel Rotbart Eric Khun Brent Ozar nderscore Mikhail Tishin Tom Elovi Spruce spezialspezial Yosuke Shinya Andy Pilate Muhammad Usama Arturo Mendivil Paul Sajna Samuel Husso nicolai256 Mihai Any Winter Doggettx Matthias Wild Kyle Schouviller rabidcopy Dominic Letz Dmitry T. Kent Keirsey psychedelicious damian0815 Eugene Brodsky","title":"Contributions by"},{"location":"other/CONTRIBUTORS/#original-compvis-authors","text":"Robin Rombach Patrick von Platen ablattmann Patrick Esser owenvincent apolinario Charles Packer If you have contributed and don't see your name on the list of contributors, please let one of the collaborators know about the omission, or feel free to make a pull request.","title":"Original CompVis Authors"},{"location":"other/README-CompViz/","text":"README from CompViz/stable-diffusion # Stable Diffusion was made possible thanks to a collaboration with Stability AI and Runway and builds upon our previous work: High-Resolution Image Synthesis with Latent Diffusion Models Robin Rombach *, Andreas Blattmann *, Dominik Lorenz \\, Patrick Esser , Bj\u00f6rn Ommer CVPR '22 Oral # which is available on GitHub . PDF at arXiv . Please also visit our Project page . Stable Diffusion is a latent text-to-image diffusion model. Thanks to a generous compute donation from Stability AI and support from LAION , we were able to train a Latent Diffusion Model on 512x512 images from a subset of the LAION-5B database. Similar to Google's Imagen , this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See this section below and the model card . Requirements # A suitable conda environment named ldm can be created and activated with: conda env create conda activate ldm Note that the first line may be abbreviated conda env create , since conda will look for environment.yml by default. You can also update an existing latent diffusion environment by running conda install pytorch torchvision -c pytorch pip install transformers == 4 .19.2 pip install -e . Stable Diffusion v1 # Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images. *Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding model card . Research into the safe deployment of general text-to-image models is an ongoing effort. To prevent misuse and harm, we currently provide access to the checkpoints only for academic research purposes upon request . This is an experiment in safe and community-driven publication of a capable and general text-to-image model. We are working on a public release with a more permissive license that also incorporates ethical considerations.* Request access to Stable Diffusion v1 checkpoints for academic research Weights # We currently provide three checkpoints, sd-v1-1.ckpt , sd-v1-2.ckpt and sd-v1-3.ckpt , which were trained as follows, sd-v1-1.ckpt : 237k steps at resolution 256x256 on laion2B-en . 194k steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024 ). sd-v1-2.ckpt : Resumed from sd-v1-1.ckpt . 515k steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en, filtered to images with an original size >= 512x512 , estimated aesthetics score > 5.0 , and an estimated watermark probability < 0.5 . The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an improved aesthetics estimator ). sd-v1-3.ckpt : Resumed from sd-v1-2.ckpt . 195k steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10\\% dropping of the text-conditioning to improve classifier-free guidance sampling . Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints: Text-to-Image with Stable Diffusion # Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder. Sampling Script # After obtaining the weights , link them mkdir -p models/ldm/stable-diffusion-v1/ ln -s <path/to/model.ckpt> models/ldm/stable-diffusion-v1/model.ckpt and sample with python scripts/txt2img.py --prompt \"a photograph of an astronaut riding a horse\" --plms By default, this uses a guidance scale of --scale 7.5 , Katherine Crowson's implementation of the PLMS sampler, and renders images of size 512x512 (which it was trained on) in 50 steps. All supported arguments are listed below (type python scripts/txt2img.py --help ). usage: txt2img.py [-h] [--prompt [PROMPT]] [--outdir [OUTDIR]] [--skip_grid] [--skip_save] [--ddim_steps DDIM_STEPS] [--plms] [--laion400m] [--fixed_code] [--ddim_eta DDIM_ETA] [--n_iter N_ITER] [--H H] [--W W] [--C C] [--f F] [--n_samples N_SAMPLES] [--n_rows N_ROWS] [--scale SCALE] [--from-file FROM_FILE] [--config CONFIG] [--ckpt CKPT] [--seed SEED] [--precision {full,autocast}] optional arguments: -h, --help show this help message and exit --prompt [PROMPT] the prompt to render --outdir [OUTDIR] dir to write results to --skip_grid do not save a grid, only individual samples. Helpful when evaluating lots of samples --skip_save do not save individual samples. For speed measurements. --ddim_steps DDIM_STEPS number of ddim sampling steps --plms use plms sampling --laion400m uses the LAION400M model --fixed_code if enabled, uses the same starting code across samples --ddim_eta DDIM_ETA ddim eta (eta=0.0 corresponds to deterministic sampling --n_iter N_ITER sample this often --H H image height, in pixel space --W W image width, in pixel space --C C latent channels --f F downsampling factor --n_samples N_SAMPLES how many samples to produce for each given prompt. A.k.a. batch size (note that the seeds for each image in the batch will be unavailable) --n_rows N_ROWS rows in the grid (default: n_samples) --scale SCALE unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty)) --from-file FROM_FILE if specified, load prompts from this file --config CONFIG path to config which constructs model --ckpt CKPT path to checkpoint of model --seed SEED the seed (for reproducible sampling) --precision {full,autocast} evaluate at this precision Note: The inference config for all v1 versions is designed to be used with EMA-only checkpoints. For this reason use_ema=False is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights. If you want to examine the effect of EMA vs no EMA, we provide \"full\" checkpoints which contain both types of weights. For these, use_ema=False will load and use the non-EMA weights. Diffusers Integration # Another way to download and sample Stable Diffusion is by using the diffusers library # make sure you're logged in with `huggingface-cli login` from torch import autocast from diffusers import StableDiffusionPipeline , LMSDiscreteScheduler pipe = StableDiffusionPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-3-diffusers\" , use_auth_token = True ) prompt = \"a photo of an astronaut riding a horse on mars\" with autocast ( \"cuda\" ): image = pipe ( prompt )[ \"sample\" ][ 0 ] image . save ( \"astronaut_rides_horse.png\" ) Image Modification with Stable Diffusion # By using a diffusion-denoising mechanism as first proposed by SDEdit , the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion. The following describes an example where a rough sketch made in Pinta is converted into a detailed artwork. python scripts/img2img.py --prompt \"A fantasy landscape, trending on artstation\" --init-img <path-to-img.jpg> --strength 0.8 Here, strength is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. See the following example. Input Outputs This procedure can, for example, also be used to upscale samples from the base model. Comments # Our codebase for the diffusion models builds heavily on OpenAI's ADM codebase and https://github.com/lucidrains/denoising-diffusion-pytorch . Thanks for open-sourcing! The implementation of the transformer encoder is from x-transformers by lucidrains . BibTeX # @misc{rombach2021highresolution, title={High-Resolution Image Synthesis with Latent Diffusion Models}, author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj\u00f6rn Ommer}, year={2021}, eprint={2112.10752}, archivePrefix={arXiv}, primaryClass={cs.CV} }","title":"CompViz-Readme"},{"location":"other/README-CompViz/#readme-from-compvizstable-diffusion","text":"Stable Diffusion was made possible thanks to a collaboration with Stability AI and Runway and builds upon our previous work: High-Resolution Image Synthesis with Latent Diffusion Models Robin Rombach *, Andreas Blattmann *, Dominik Lorenz \\, Patrick Esser , Bj\u00f6rn Ommer","title":"README from CompViz/stable-diffusion"},{"location":"other/README-CompViz/#cvpr-22-oral","text":"which is available on GitHub . PDF at arXiv . Please also visit our Project page . Stable Diffusion is a latent text-to-image diffusion model. Thanks to a generous compute donation from Stability AI and support from LAION , we were able to train a Latent Diffusion Model on 512x512 images from a subset of the LAION-5B database. Similar to Google's Imagen , this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM. See this section below and the model card .","title":"CVPR '22 Oral"},{"location":"other/README-CompViz/#requirements","text":"A suitable conda environment named ldm can be created and activated with: conda env create conda activate ldm Note that the first line may be abbreviated conda env create , since conda will look for environment.yml by default. You can also update an existing latent diffusion environment by running conda install pytorch torchvision -c pytorch pip install transformers == 4 .19.2 pip install -e .","title":"Requirements"},{"location":"other/README-CompViz/#stable-diffusion-v1","text":"Stable Diffusion v1 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 860M UNet and CLIP ViT-L/14 text encoder for the diffusion model. The model was pretrained on 256x256 images and then finetuned on 512x512 images. *Note: Stable Diffusion v1 is a general text-to-image diffusion model and therefore mirrors biases and (mis-)conceptions that are present in its training data. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding model card . Research into the safe deployment of general text-to-image models is an ongoing effort. To prevent misuse and harm, we currently provide access to the checkpoints only for academic research purposes upon request . This is an experiment in safe and community-driven publication of a capable and general text-to-image model. We are working on a public release with a more permissive license that also incorporates ethical considerations.* Request access to Stable Diffusion v1 checkpoints for academic research","title":"Stable Diffusion v1"},{"location":"other/README-CompViz/#weights","text":"We currently provide three checkpoints, sd-v1-1.ckpt , sd-v1-2.ckpt and sd-v1-3.ckpt , which were trained as follows, sd-v1-1.ckpt : 237k steps at resolution 256x256 on laion2B-en . 194k steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024 ). sd-v1-2.ckpt : Resumed from sd-v1-1.ckpt . 515k steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en, filtered to images with an original size >= 512x512 , estimated aesthetics score > 5.0 , and an estimated watermark probability < 0.5 . The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an improved aesthetics estimator ). sd-v1-3.ckpt : Resumed from sd-v1-2.ckpt . 195k steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10\\% dropping of the text-conditioning to improve classifier-free guidance sampling . Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling steps show the relative improvements of the checkpoints:","title":"Weights"},{"location":"other/README-CompViz/#text-to-image-with-stable-diffusion","text":"Stable Diffusion is a latent diffusion model conditioned on the (non-pooled) text embeddings of a CLIP ViT-L/14 text encoder.","title":"Text-to-Image with Stable Diffusion"},{"location":"other/README-CompViz/#sampling-script","text":"After obtaining the weights , link them mkdir -p models/ldm/stable-diffusion-v1/ ln -s <path/to/model.ckpt> models/ldm/stable-diffusion-v1/model.ckpt and sample with python scripts/txt2img.py --prompt \"a photograph of an astronaut riding a horse\" --plms By default, this uses a guidance scale of --scale 7.5 , Katherine Crowson's implementation of the PLMS sampler, and renders images of size 512x512 (which it was trained on) in 50 steps. All supported arguments are listed below (type python scripts/txt2img.py --help ). usage: txt2img.py [-h] [--prompt [PROMPT]] [--outdir [OUTDIR]] [--skip_grid] [--skip_save] [--ddim_steps DDIM_STEPS] [--plms] [--laion400m] [--fixed_code] [--ddim_eta DDIM_ETA] [--n_iter N_ITER] [--H H] [--W W] [--C C] [--f F] [--n_samples N_SAMPLES] [--n_rows N_ROWS] [--scale SCALE] [--from-file FROM_FILE] [--config CONFIG] [--ckpt CKPT] [--seed SEED] [--precision {full,autocast}] optional arguments: -h, --help show this help message and exit --prompt [PROMPT] the prompt to render --outdir [OUTDIR] dir to write results to --skip_grid do not save a grid, only individual samples. Helpful when evaluating lots of samples --skip_save do not save individual samples. For speed measurements. --ddim_steps DDIM_STEPS number of ddim sampling steps --plms use plms sampling --laion400m uses the LAION400M model --fixed_code if enabled, uses the same starting code across samples --ddim_eta DDIM_ETA ddim eta (eta=0.0 corresponds to deterministic sampling --n_iter N_ITER sample this often --H H image height, in pixel space --W W image width, in pixel space --C C latent channels --f F downsampling factor --n_samples N_SAMPLES how many samples to produce for each given prompt. A.k.a. batch size (note that the seeds for each image in the batch will be unavailable) --n_rows N_ROWS rows in the grid (default: n_samples) --scale SCALE unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty)) --from-file FROM_FILE if specified, load prompts from this file --config CONFIG path to config which constructs model --ckpt CKPT path to checkpoint of model --seed SEED the seed (for reproducible sampling) --precision {full,autocast} evaluate at this precision Note: The inference config for all v1 versions is designed to be used with EMA-only checkpoints. For this reason use_ema=False is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights. If you want to examine the effect of EMA vs no EMA, we provide \"full\" checkpoints which contain both types of weights. For these, use_ema=False will load and use the non-EMA weights.","title":"Sampling Script"},{"location":"other/README-CompViz/#diffusers-integration","text":"Another way to download and sample Stable Diffusion is by using the diffusers library # make sure you're logged in with `huggingface-cli login` from torch import autocast from diffusers import StableDiffusionPipeline , LMSDiscreteScheduler pipe = StableDiffusionPipeline . from_pretrained ( \"CompVis/stable-diffusion-v1-3-diffusers\" , use_auth_token = True ) prompt = \"a photo of an astronaut riding a horse on mars\" with autocast ( \"cuda\" ): image = pipe ( prompt )[ \"sample\" ][ 0 ] image . save ( \"astronaut_rides_horse.png\" )","title":"Diffusers Integration"},{"location":"other/README-CompViz/#image-modification-with-stable-diffusion","text":"By using a diffusion-denoising mechanism as first proposed by SDEdit , the model can be used for different tasks such as text-guided image-to-image translation and upscaling. Similar to the txt2img sampling script, we provide a script to perform image modification with Stable Diffusion. The following describes an example where a rough sketch made in Pinta is converted into a detailed artwork. python scripts/img2img.py --prompt \"A fantasy landscape, trending on artstation\" --init-img <path-to-img.jpg> --strength 0.8 Here, strength is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input. See the following example. Input Outputs This procedure can, for example, also be used to upscale samples from the base model.","title":"Image Modification with Stable Diffusion"},{"location":"other/README-CompViz/#comments","text":"Our codebase for the diffusion models builds heavily on OpenAI's ADM codebase and https://github.com/lucidrains/denoising-diffusion-pytorch . Thanks for open-sourcing! The implementation of the transformer encoder is from x-transformers by lucidrains .","title":"Comments"},{"location":"other/README-CompViz/#bibtex","text":"@misc{rombach2021highresolution, title={High-Resolution Image Synthesis with Latent Diffusion Models}, author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj\u00f6rn Ommer}, year={2021}, eprint={2112.10752}, archivePrefix={arXiv}, primaryClass={cs.CV} }","title":"BibTeX"}]}